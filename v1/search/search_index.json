{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":""},{"location":"#introduction","title":"Introduction","text":"<p>What is Agentic RAG?</p> <p>Agentic RAG is a method that combines the strengths of retrieval-augmented-generation with autonomous agents.</p> <p>With vanilla RAG, Vectara receives a user query, retrieves the most relevant facts from your data, and uses an LLM to generate the most accurate response based on those facts. (Unfamiliar with RAG? Check out this page to learn more!)</p> <p>Agentic RAG leverages an LLM to \"manage\" the process of answering the user query via reasoning, planning, and a provided set of \"tools\". Since a \"manager\" LLM-powered agent is in charge, it is smart enough to analyze the user query and properly call tools to obtain a comprehensive response to a complex user query.</p> <p>For example:</p> <ul> <li>The agent can rephrase the user query to fit a certain style, role,     or persona.</li> <li>The agent can break the query down into multiple (simpler)     sub-queries and call the RAG query tool for each sub-query, then     combine the responses to come up with a comprehensive response.</li> <li>The agent can identify filtering criteria in the user query and use     them to filter the results from the RAG query tool.</li> </ul> <p>The main tool used in vectara-agentic is the <code>Vectara RAG query tool</code>, which queries a Vectara corpus and returns the most relevant response. By using a RAG-based agent, you mitigate some of the issues with pure LLMs, particularly hallucinations and explainability.</p> <p>Another important tool that can be used to query a Vectara corpus is the <code>Vectara search tool</code>, which queries a Vectara corpus for the most relevant search results and documents that match a query.</p> <p>Additional tools give your application superpowers to retrieve up-to-date information, access enterprise specific data via APIs, make SQL queries to a database, or even perform actions such as creating a calendar event or sending an email.</p> <p>Let's demonstrate the advantage of Agentic RAG via a simple example.</p> <p>Imagine that you have ingested into Vectara all your Google Drive files, JIRA tickets, and product documentation. You build an Agentic RAG application using these tools:</p> <ol> <li>A JIRA RAG query tool</li> <li>A Google Drive RAG query tool</li> <li>A product docs RAG query tool</li> <li>A tool that can issue SQL queries against an internal database     containing customer support data</li> </ol> <p>Consider the query: \"What is the top issue reported by customers in the last 3 months? Who is working to solve it?\"</p> <p>A standard RAG pipeline would try to match this entire query to the most relevant facts in your data, and generate a response. It may fail to distinguish the query as two separate questions, and given the complexity, may fail to produce a good response.</p> <p>An Agentic RAG assistant would recognize the complexity of the user query, and decide to act in two steps. First it will form a query with its SQL tool to identify the top issue reported by customers in the last 3 months, and then it will call the JIRA tool to identify who is working on that issue from the first query.</p> <p>What is vectara-agentic?</p> <p>Vectara-agentic is a Python package that allows you to quickly and easily build Agentic RAG applications, powered by Vectara. It is based on LlamaIndex and provides a simple API to define tools, including a quick way to generate Vectara RAG tools.</p> <p>It also includes some pre-built tools that you can use out-of-the-box for various purposes (such as legal or finance), and provides access to a wide range of LLMs through integrations with OpenAI, Anthropic, Gemini, Together.AI, Cohere, GROQ, and Fireworks AI.</p>"},{"location":"#agent-architecture","title":"Agent Architecture","text":"<p>{.align-center}</p> <p>Vectara-agentic follows a typical agentic RAG architecture. It consists of the following components:</p> <ul> <li>One or more RAG tools for making queries to corpora in Vectara.</li> <li>A set of additional tools that the agent can use to retrieve     information, process data, or perform actions.</li> <li>A central LLM, or agent (based on <code>ReAct</code>, <code>OpenAI</code>, <code>LATS</code>, or     <code>LLMCompiler</code> agent type) that manages the process of interpreting     the user query, creating and executing a plan to collect information     needed to respond to that query, and crafting a final response.</li> </ul>"},{"location":"#basic-example","title":"Basic Example","text":"<p>The most basic application you can make with vectara-agentic is an agent with a single RAG tool that can pull information from a Vectara corpus.</p> <p>Let's see how this is implemented in code:</p> <pre><code>from vectara_agentic.agent import Agent\nfrom vectara_agentic.tools import VectaraToolFactory\nfrom pydantic import Field, BaseModel\n\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\napi_key = str(os.environ['VECTARA_API_KEY'])\ncustomer_id = str(os.environ['VECTARA_CUSTOMER_ID'])\ncorpus_id = str(os.environ['VECTARA_CORPUS_ID'])\n\n\nvec_factory = VectaraToolFactory(\n  vectara_api_key = api_key, \n  vectara_customer_id = customer_id, \n  vectara_corpus_id = corpus_id\n)\n\nclass QueryPetPolicyArgs(BaseModel):\n    query: str = Field(..., description=\"The user query.\")\n\nask_pet_policy_tool = vec_factory.create_rag_tool (\n    tool_name = \"ask_pet_policy\",\n    tool_description = \"Responds to questions about Vectara's pet policy.\",\n    tool_args_schema = QueryPetPolicyArgs,\n    summary_num_results = 10,\n    n_sentences_before = 3,\n    n_sentences_after = 3,\n    mmr_diversity_bias = 0.1,\n    include_citations = False\n)\n\nagent = Agent(\n    tools = [ask_pet_policy_tool],\n    topic = \"Vectara Pet Policy\"\n)\n\nagent.chat(\"What is Vectara's pet policy?\")\n</code></pre> <p>When we run this code, we get the following response:</p> <p>Vectara's pet policy does not allow common household pets like cats and dogs on their campuses. Instead, they welcome a select group of exotic creatures that reflect their innovative spirit and core values. Additionally, birds are not only permitted but encouraged in their workspace as part of their unique approach.</p> <p>In the above code, we defined a single RAG tool (ask_pet_policy_tool) for our <code>Agent</code> class, and then created an AI assistant with this tool.</p> <p>The above code demonstrates the typical flow for instantiating your <code>Agent</code> object when you are defining more than one tool, only in this case it only used a single tool. Since making a simple assistant like this with just one RAG tool is a common need, we have provided a single function that does all of this at once called <code>from_corpus()</code>.</p> <p>Here's how you can create a simple assistant that uses a single RAG tool for asking questions about Medicare:</p> <pre><code>agent = Agent.from_corpus(\n  vectara_customer_id=customer_id,\n  vectara_corpus_id=corpus_id,\n  vectara_api_key=api_key,\n  data_description=\"medical plan benefits and pricing\",\n  assistant_specialty=\"Medicare\",\n  tool_name=\"ask_medicare\",\n)\n</code></pre>"},{"location":"#try-it-yourself","title":"Try it Yourself","text":"<p>To run this code yourself, add the following environment variables to your console or a .env file:</p> <p>Vectara Corpus:</p> <p><code>VECTARA_CUSTOMER_ID</code>: The customer id for your Vectara account. If you don't have an account, simply create one to get started.</p> <p><code>VECTARA_CORPUS_ID</code>: The corpus id for the corpus that contains the Vectara pet policy. You can download the Pet Policy PDF file and add it to a new or existing Vectara corpus.</p> <p><code>VECTARA_API_KEY</code>: An API key that can perform queries on this corpus.</p> <p>Agent type, LLMs and model names:</p> <p><code>VECTARA_AGENTIC_AGENT_TYPE</code>: Agent type, either OPENAI (default), REACT, LATS, or LLMCOMPILER (make sure you have an OpenAI API key if you use the OpenAI agent).</p> <p><code>VECTARA_AGENTIC_MAIN_LLM_PROVIDER</code>: The LLM used for the agent, either OPENAI (default), ANTHROPIC, GEMINI, TOGETHER, COHERE, BEDROCK, GROQ, or FIREWORKS. Note that to use the OPENAI agent type, you must use OPENAI as the main LLM provider.</p> <p><code>VECTARA_AGENTIC_TOOL_LLM_PROVIDER</code>: The LLM used for the agent tools, either OPENAI (default), ANTHROPIC, GEMINI, TOGETHER, COHERE, BEDROCK, GROQ, or FIREWORKS.</p> <p><code>OPENAI_API_KEY</code>, <code>ANTHROPIC_API_KEY</code>, <code>GOOGLE_API_KEY</code>, <code>TOGETHER_API_KEY</code>, <code>COHERE_API_KEY</code>, <code>BEDROCK_API_KEY</code>, <code>GROQ_API_KEY</code>, or <code>FIREWORKS_API_KEY</code>: Your API key for the agent or tool LLM, if you choose to use these services.</p> <p>With any LLM provider choice, you can also specify the model type to use via these environment variables:</p> <p><code>VECTARA_AGENTIC_MAIN_MODEL_NAME</code>: specifies the model name for the main LLM provider.</p> <p><code>VECTARA_AGENTIC_TOOL_MODEL_NAME</code>: specifies the model name for the tool LLM provider.</p> <p>Defaults:</p> <ol> <li>For <code>OPENAI</code>, the default is gpt-4o.</li> <li>For <code>ANTHROPIC</code>, the default is Claude-3-5-Sonnet-20241022.</li> <li>For <code>GEMINI</code>, the default is gemini-1.5-flash.</li> <li>For <code>TOGETHER.AI</code>, the default is Meta-Llama-3.3-70B-Instruct-Turbo.</li> <li>For <code>COHERE</code>, the default is command-r-plus.</li> <li>For <code>BEDROCK</code>, the default is anthropic.claude-3-5-sonnet-20241022-v2:0.</li> <li>For <code>GROQ</code>, the default is llama-3.3-70b-versatile.</li> <li>For <code>FIREWORKS</code>, the default is Firefunction-v2.</li> </ol>"},{"location":"endpoint/","title":"API Endpoint","text":"<p>It\\'s super easy to host your vectara-agentic assistant or agent behind an API endpoint:</p> <p><code>vectara-agentic</code> can be easily hosted locally or on a remote machine behind an API endpoint, by following theses steps:</p> <p>1. Setup your API key Ensure that you have your API key set up as an environment variable:</p> <pre><code>export VECTARA_AGENTIC_API_KEY=&lt;YOUR-ENDPOINT-API-KEY&gt;\n</code></pre> <p>2. Start the API Server Initialize the agent and start the FastAPI server by following this example:</p> <pre><code>from agent import Agent\nfrom agent_endpoint import start_app\nagent = Agent(...)            # Initialize your agent with appropriate parameters\nstart_app(agent)\n</code></pre> <p>You can customize the host and port by passing them as arguments to start_app().</p> <p>For example:</p> <pre><code>start_app(agent, host=\"0.0.0.0\", port=8000)\n</code></pre> <p>3. Access the API Endpoint Once the server is running, you can interact with it using curl or any HTTP client. For example:</p> <pre><code>curl -G \"http://&lt;remote-server-ip&gt;:8000/chat\" \\\n--data-urlencode \"message=What is Vectara?\" \\\n-H \"X-API-Key: &lt;YOUR-API-KEY&gt;\"\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>You can install vectara-agentic using pip as follows:</p> <pre><code>pip install vectara-agentic\n</code></pre>"},{"location":"tools/","title":"Tools","text":"<p>vectara-agentic provides a set of pre-built tools that you can use out-of-the-box for various purposes.</p>"},{"location":"tools/#standard-tools","title":"Standard Tools","text":"<p>The standard tools includes two tools that can be used for general purposes:</p> <ol> <li><code>summarize_text</code>: a tool that summarizes text, given a certain     perspective and expertise. For example, you can use this tool to     summarize text as a math teacher, a lawyer, or a doctor.</li> <li><code>rephrase_text</code>: a tool that rephrases text given instructions. For     example, you can instruct the tool to rephrase a response for a     5-year-old's understanding or to adapt it to a formal tone.</li> </ol>"},{"location":"tools/#finance-tools","title":"Finance Tools","text":"<p>vectara-agentic includes a few financial tools you can use right away in your agent, based on the LlamaIndex YahooFinanceToolSpec:</p> <ol> <li><code>balance_sheet</code>: A tool that returns the balance sheet of a company.</li> <li><code>income_statement</code>: A tool that returns the income statement of a     company.</li> <li><code>cash_flow</code>: A tool that returns the cash flow of a company.</li> <li><code>stock_news</code>: A tool that returns the latest news about a company.</li> <li><code>stock_basic_info</code>: A tool that returns basic information about a     company including price.</li> <li><code>stock_analyst_recommendations</code>: A tool that returns analyst     recommendations for a company.</li> </ol>"},{"location":"tools/#legal-tools","title":"Legal Tools","text":"<p>vectara-agentic includes a few tools for the legal space:</p> <ol> <li><code>summarize_legal_text</code>: A tool that summarizes legal text.</li> <li><code>critique_as_judge</code>: A tool that critiques legal text from the     perspective of an expert judge.</li> </ol>"},{"location":"tools/#guardrail-tools","title":"Guardrail Tools","text":"<p>These specialized tools help you AI assistnat or agent to avoid certain topics or responses that are prohibited by your organization or by law.</p> <p>The <code>get_bad_topics</code> tool returns a list of topics that are prohibited (politics, religion, violence, hate speech, adult content, illegal activities). The agent prompt has special instructions to call this tool if it exists, and avoid these topics.</p> <p>If you want to create your own set of topics, you can define a new tool by the same name that returns a list of different topics, and the agent will use that list to avoid these topics.</p>"},{"location":"tools/#database-tools","title":"Database Tools","text":"<p>Database tools are quite useful if your agent requires access to a combination of RAG tools along with analytics capabilities. For example, consider an EV-assistant demo, providing answers about electric vehicles.</p> <p>We have provided this assistant with the following tools:</p> <ol> <li><code>ask_vehicles</code>: A Vectara RAG tool that answers general questions     about electric vehicles.</li> <li><code>ask_policies</code>: A Vectara RAG tool that answers questions about     electric vehicle policies.</li> <li>The <code>database_tools</code> that can help the agent answer analytics     queries based on three datasets: EV population data, EV population     size history by county, and EV title and registration activity.</li> </ol> <p>With the <code>ask_vehicles</code> and <code>ask_policies</code> tools, the ev-assistant can answer questions based on text, and it will use the database tools to answer analytical questions, based on the data.</p> <p>Setting up the Database Tools</p> <p>The database tools are based on the LlamaIndex DatabaseToolSpec You can define these database tools in two ways:</p> <ol> <li>Specify a <code>sql_database</code> argument OR</li> <li>Specify the dbname, host, scheme, port, user and password, and the     tool will generate the sql_database object for you from those.</li> </ol> <p>This creates four tools:</p> <ol> <li><code>list_tables</code>: A tool that lists the tables in the database.</li> <li><code>describe_tables</code>: A tool that describes the schema of a table.</li> <li><code>load_data</code>: A tool that loads data from a table.</li> <li><code>load_sample_data</code> tool which provides a sample of the data from a     table.</li> <li><code>load_unique_values</code> tool which provides unique values for a set of     columns in a table.</li> </ol> <p>Together, these 4 tools provide a comprehensive set of capabilities for an agent to interact with a database. For example, an agent can use the <code>list_tables</code> tool to get a list of tables in the database, and then use the <code>describe_tables</code> tool to get the schema of a specific table. It will use the <code>load_sample_data</code> to get a sample of the data in the table, or the <code>load_unique_values</code> to explore the type of values valid for a column. Finally, the agent can use the <code>load_data</code> tool to load the data into the agent's memory.</p> <p>Multiple databases</p> <p>In the case of EV-assistant, we use only a single database with 3 tables, and <code>tool_name_prefix=\"ev\"</code></p> <p>If your use-case includes multiple databases, you can define multiple database tools: each with a different database connection and a different <code>tool_name_prefix</code>.</p>"},{"location":"tools/#other-tools","title":"Other Tools","text":"<p>In addition to the tools above, vectara-agentic also supports these additional tools from the LlamaIndex Tools hub:</p> <ol> <li><code>arxiv</code>: A tool that queries the arXiv respository of papers.</li> <li><code>tavily_research</code>: A tool that queries the web using Tavily</li> <li><code>kuzu</code>: A tool that queries the Kuzu graph database.</li> <li><code>exa.ai</code>: A tool that uses Exa.ai search.</li> <li><code>neo4j</code>: A tool that queries a Neo4J graph database.</li> <li><code>google</code>: A set of tools that interact with Google services,     including Gmail, Google Calendar, and Google Search.</li> <li><code>slack</code>: A tool that interacts with Slack.</li> </ol>"},{"location":"usage/","title":"Usage","text":"<p>Let's walk through a complete example of creating an AI assistant using vectara-agentic. We will build a finance assistant that can answer questions about the annual financial reports for Apple Computer, Google, Amazon, Snowflake, Atlassian, Tesla, Nvidia, Microsoft, Advanced Micro Devices, Intel, and Netflix between the years 2020 and 2024.</p>"},{"location":"usage/#import-dependencies","title":"Import Dependencies","text":"<p>First, we must import some libraries and define some constants for our demo.</p> <pre><code>import os\nfrom dotenv import load_dotenv\nimport streamlit as st\nimport pandas as pd\nimport requests\nfrom pydantic import Field\n\nload_dotenv(override=True)\n</code></pre> <p>We then use the <code>load_dotenv</code> function to load our environment variables from a <code>.env</code> file.</p>"},{"location":"usage/#create-tools","title":"Create Tools","text":"<p>Next, we will create the tools for our agent.</p> <p>There are three categories of tools you can use with vectara-agentic:</p> <ol> <li>A query tool that connects to Vectara to ask a question about data     in a Vectara corpus.</li> <li>Pre-built tools that are available out of the box, or ready to use     tools from the LlamaIndex Tools     Hub.</li> <li>Any other tool that you want to make for your agent, based on custom     code in Python.</li> </ol> <p>Vectara RAG Query Tool</p> <p>Let's see how to create a Vectara query tool. In order to use this tool, you need to create a corpus and API key with a Vectara account. In this example, we will create the <code>ask_transcripts</code> tool, which can be used to perform RAG queries on analyst call transcripts. You can see this tool in use with our Finance Assistant demo.</p> <pre><code>from pydantic import BaseModel\n\n# define the arguments schema for the tool\nclass QueryTranscriptsArgs(BaseModel):\n    query: str = Field(..., description=\"The user query.\")\n    year: int = Field(..., description=f\"The year. An integer between {min(years)} and {max(years)}.\")\n    ticker: str = Field(..., description=f\"The company ticker. Must be a valid ticket symbol from the list {tickers.keys()}.\")\n</code></pre> <p>Note that the arguments for this tool are defined using Python's <code>pydantic</code> package with the <code>Field</code> class. By defining the tool in this way, we provide a good description for each argument so that the agent LLM can easily understand the tool's functionality and how to use it properly.</p> <p>You can also define an argument to support optional conditional arguments, for example:</p> <pre><code>from pydantic import BaseModel\n\n# define the arguments schema for the tool\nclass QueryTranscriptsArgs(BaseModel):\n    query: str = Field(..., description=\"The user query.\")\n    year: int | str = Field(\n        default=None,\n        description=f\"The year this query relates to. An integer between {min(years)} and {max(years)} or a string specifying a condition on the year\",\n        examples=[2020, '&gt;2021', '&lt;2023', '&gt;=2021', '&lt;=2023', '[2021, 2023]', '[2021, 2023)']\n    )\n    ticker: str = Field(..., description=f\"The company ticker. Must be a valid ticket symbol from the list {tickers.keys()}.\")\n</code></pre> <p>With this change for the <code>year</code> argument, we are telling the agent that both an int value (e.g. 2022) or a string value (e.g. '&gt;2022' or '\\&lt;2022') are valid inputs for this argument. You can also use range filters (e.g. '[2021, 2023]') to specify a range of years. If a string value is provided, <code>vectara-agentic</code> knows how to parse it properly in the backend and set a metadata filter with the right condition for Vectara.</p> <p>Now to create the actual tool, we use the <code>create_rag_tool()</code> method from the <code>VectaraToolFactory</code> class as follows:</p> <pre><code>from vectara_agentic.tools import VectaraToolFactory\n\nvec_factory = VectaraToolFactory(vectara_api_key=vectara_api_key,\n                                 vectara_customer_id=vectara_customer_id,\n                                 vectara_corpus_id=vectar_corpus_id)\n\nask_transcripts = vec_factory.create_rag_tool(\n    tool_name = \"ask_transcripts\",\n    tool_description = \"\"\"\n    Given a company name and year,\n    returns a response (str) to a user question about a company, based on analyst call transcripts about the company's financial reports for that year.\n    You can ask this tool any question about the compaany including risks, opportunities, financial performance, competitors and more.\n    Make sure to provide the a valid company ticker and year.\n    \"\"\",\n    tool_args_schema = QueryTranscriptsArgs,\n    tool_args_type = {\n      \"year\": \"doc\",\n      \"ticker\": \"doc\"\n    }\n    reranker = \"chain\", rerank_k = 100,\n    rerank_chain = [\n      {\n        \"type\": \"slingshot\"\n      },\n      {\n        \"type\": \"userfn\",\n        \"user_function\": \"knee()\"\n      }\n      {\n        \"type\": \"mmr\",\n        \"diversity_bias\": 0.1\n      }\n    ],\n    n_sentences_before = 2, n_sentences_after = 2, lambda_val = 0.005,\n    summary_num_results = 10,\n    vectara_summarizer = 'vectara-summary-ext-24-05-med-omni',\n    include_citations = False,\n    fcs_threshold = 0.2\n)\n</code></pre> <p>In the code above, we did the following:</p> <ul> <li>First, we initialized the <code>VectaraToolFactory</code> with the Vectara     customer ID, corpus ID, and API key. If you don't want to     explicitly pass in these arguments, you can specify them in your     environment as <code>VECTARA_CUSTOMER_ID</code>, <code>VECTARA_CORPUS_ID</code>, and     <code>VECTARA_API_KEY</code>. Additionally, you can also create a single     <code>VectaraToolFactory</code> that queries multiple corpora. This may be     helpful if you have related information across multiple corpora in     Vectara. To do this, create a query API key on the     Authorization     page and give it to access to all the corpora you want for this     query tool. When specifying your environment variables, set     <code>VECTARA_CORPUS_ID</code> to a list of corpus IDs separated by commas     (e.g. <code>5,6,19</code>).</li> <li>Then we called <code>create_rag_tool()</code>, specifying the tool name,     description and schema for the tool, followed by various optional     parameters to control the Vectara RAG query tool. Notice that we     also specified the type of each additional argument in the schema.     The type of each argument can be <code>\"doc\"</code> or <code>\"part\"</code>, corresponding     to whether the metadata argument is document metadata or part     metadata in the Vectara corpus. See this     page     on metadata for more information.</li> </ul> <p>One important parameter to point out is <code>fcs_threshold</code>. This allows you to specify a minimum factual consistency score (between 0 and 1) for the response to be considered a \"good\" response. If the generated response has an <code>FCS</code> below this threshold, the agent will not use the generated summary (considering it a hallucination). You can think of this as a hallucination guardrail. The higher you set <code>fcs_threshold</code>, the stricter your guardrail will be.</p> <p>If your agent continuously rejects all of the generated responses, consider lowering the threshold.</p> <p>Another important parameter is <code>reranker</code>. In this example, we are using a chain reranker, which chains together multiple reranking methods to achieve better control over the reranking and combines the strengths of various reranking methods. In the example above, we use the multilingual (or slingshot) reranker followed by a user-defined function (the knee reranker), and finally the MMR reranker with a diversity bias of 0.1. You can also supply other parameters to each reranker, such as a <code>cutoff</code> parameter, which removes documents that have scores below this threshold value after applying the given reranker. Lastly, you can add another user defined function reranker as the last reranker in the chain to specify a customized expression to rerank results in a way that is relevant to your specific application. If you want to learn more about reranking tips and best practices, check out our blog posts on user defined functions and knee reranking as well as this example notebook on user defined functions for some guidance and inspiration.</p> <p>That's it: now the <code>ask_transcripts</code> tool is ready to be added to the agent.</p> <p>You can use the <code>VectaraToolFactory</code> to generate more than one RAG tool with different parameters, depending on your needs.</p> <p>Vectara Search Tool</p> <p>In most cases, you will likely want to use the Vectara RAG query tool, which generates a summary to return to the agent along with the source text and documents used to generate that summary.</p> <p>In some applications, you may want the tool to only retrieve the actual text/documents that best match the query rather than summarizing all of the results. For example, you may ask your agent \"How many documents mention information about tax laws and regulations?\". The agent will be able to get a list of documents from your Vectara corpus and analyze the results to answer your question.</p> <p>Metadata Filtering</p> <p>In most cases, you will want to use the <code>tool_args_schema</code> to define the metadata fields used in your Vectara RAG or Search tool. Defining your parameters in this way allows the agent to interpret the user query and determine if any of these filters should be applied on that particular query.</p> <p>In some instances you may want to have a metadata filter that applies in every call to a Vectara RAG or search tool. For example, you may want to enforce that the oldest possible search results are from 2022. In this case, you can use the <code>fixed_filter</code> parameter to the <code>create_rag_tool()</code> or <code>create_search_tool()</code> functions.</p> <p>In our example where we want all results to be from 2022 and later, we would specify <code>fixed_filter = \"doc.year &gt;= 2022\"</code>.</p> <p>Additional Tools</p> <p>To generate non-RAG tools, you can use the <code>ToolsFactory</code> class, which provides some out-of-the-box tools that you might find helpful when building your agents, as well as an easy way to create custom tools.</p> <p>Currently, we have a few tool groups you may want to consider using:</p> <ul> <li><code>standard_tools()</code>: These are basic tools that can be helpful, and     include the <code>summarize_text</code> tool and <code>rephrase_text</code> tool.</li> <li><code>finance_tools()</code>: includes a set of financial query tools based on     Yahoo! finance.</li> <li><code>legal_tools()</code>: These tools are designed to help with legal     queries, and include <code>critique_as_judge</code> and <code>summarize_legal_text</code>.</li> <li><code>database_tools()</code>: tools to explore SQL databases and make queries     based on user prompts.</li> <li><code>guardrail_tools()</code>: These tools are designed to help the agent     avoid certain topics from its response.</li> </ul> <p>For example, to get access to all the legal tools, you can use the following:</p> <pre><code>from vectara_agentic.tools import ToolsFactory\n\nlegal_tools = ToolsFactory().legal_tools()\n</code></pre> <p>For more details about the tools see <code>Tools &lt;tools&gt;</code>{.interpreted-text role=\"doc\"}.</p> <p>Create your own tool</p> <p>You can also create your own tool directly by defining a Python function:</p> <pre><code>def earnings_per_share(\n  net_income: float = Field(description=\"the net income for the company\"),\n  number_of_shares: float = Field(description=\"the number of oustanding shares\"),\n) -&gt; float:\n    \"\"\"\n    This tool returns the EPS (earnings per share).\n    \"\"\"\n    return np.round(net_income / number_of_shares,4)\n\nmy_tool = tools_factory.create_tool(earnings_per_share)\n</code></pre> <p>A few important things to note:</p> <ol> <li>A tool may accept any type of argument (e.g. float, int) and return     any type of value (e.g. float). The <code>create_tool()</code> method will     handle the conversion of the arguments and response into strings     (which is type the agent expects).</li> <li>It is important to define a clear and concise docstring for your     tool. This will help the agent understand what the tool does and how     to use it.</li> </ol> <p>Here are some functions we will define for our finance assistant example:</p> <pre><code>tickers = {\n  \"AAPL\": \"Apple Computer\", \n  \"GOOG\": \"Google\", \n  \"AMZN\": \"Amazon\",\n  \"SNOW\": \"Snowflake\",\n  \"TEAM\": \"Atlassian\",\n  \"TSLA\": \"Tesla\",\n  \"NVDA\": \"Nvidia\",\n  \"MSFT\": \"Microsoft\",\n  \"AMD\": \"Advanced Micro Devices\",\n  \"INTC\": \"Intel\",\n  \"NFLX\": \"Netflix\",\n}\nyears = [2020, 2021, 2022, 2023, 2024]\n\ndef get_company_info() -&gt; list[str]:\n\"\"\"\nReturns a dictionary of companies you can query about. Always check this before using any other tool.\nThe output is a dictionary of valid ticker symbols mapped to company names.\nYou can use this to identify the companies you can query about, and their ticker information.\n\"\"\"\nreturn tickers\n\ndef get_valid_years() -&gt; list[str]:\n\"\"\"\nReturns a list of the years for which financial reports are available.\nAlways check this before using any other tool.\n\"\"\"\nreturn years\n\n# Tool to get the income statement for a given company and year using the FMP API\ndef get_income_statement(\nticker=Field(description=\"the ticker symbol of the company.\"),\nyear=Field(description=\"the year for which to get the income statement.\"),\n) -&gt; str:\n\"\"\"\nGet the income statement for a given company and year using the FMP (https://financialmodelingprep.com) API.\nReturns a dictionary with the income statement data. All data is in USD, but you can convert it to more compact form like K, M, B.\n\"\"\"\nfmp_api_key = os.environ.get(\"FMP_API_KEY\", None)\nif fmp_api_key is None:\n   return \"FMP_API_KEY environment variable not set. This tool does not work.\"\nurl = f\"https://financialmodelingprep.com/api/v3/income-statement/{ticker}?apikey={fmp_api_key}\"\nresponse = requests.get(url)\nif response.status_code == 200:\n   data = response.json()\n   income_statement = pd.DataFrame(data)\n   income_statement[\"date\"] = pd.to_datetime(income_statement[\"date\"])\n   income_statement_specific_year = income_statement[\n     income_statement[\"date\"].dt.year == int(year)\n   ]\n   values_dict = income_statement_specific_year.to_dict(orient=\"records\")[0]\n   return f\"Financial results: {', '.join([f'{key}: {value}' for key, value in values_dict.items() if key not in ['date', 'cik', 'link', 'finalLink']])}\"\nelse:\n   return \"FMP API returned error. This tool does not work.\"\n</code></pre> <p>The <code>get_income_statement()</code> tool utilizes the FMP API to get the income statement for a given company and year. Notice how the tool description is structured. We describe each of the expected arguments to the function using pydantic's <code>Field</code> class. The function description only describes to the agent what the function does and how the agent should use the tool. This function definition follows best practices for defining tools. You should make this description detailed enough so that your agent knows when to use each of your tools.</p> <p>Your tools should also handle any exceptions gracefully by returning an <code>Exception</code> or a string describing the failure. The agent can interpret that string and then decide how to deal with the failure (either calling another tool to accomplish the task or telling the user that their request was unable to be processed).</p> <p>Finally, notice that we have used snake_case for all of our function names. While this is not required, it's a best practice that we recommend for you to follow.</p>"},{"location":"usage/#initialize-the-agent","title":"Initialize The Agent","text":"<p>Now that we have our tools, let's create the agent, using the following arguments:</p> <ol> <li><code>tools: list[FunctionTool]</code>: A list of tools that the agent will use     to interact with information and apply actions. For any tools you     create yourself, make sure to pass them to the <code>create_tool()</code>     method of your <code>ToolsFactory</code> object.</li> <li><code>topic: str = \"general\"</code>: This is simply a string (should be a noun)     that is used to identify the agent's area of expertise. For our     example we set this to <code>financial analyst</code>.</li> <li><code>custom_instructions: str = \"\"</code>: This is a set of instructions that     the agent will follow. These instructions should not tell the agent     what your tools do (that's what the tool descriptions are for) but     rather any particular behavior you want your LLM to have, such as     how to present the information it receives from the tools to the     user.</li> <li><code>update_func: Optional[Callable[[AgentStatusType, str], None]] = None</code>:     This is an optional callback function that will be called on every     agent step. It can be used to update the user interface or the steps     of the agent.</li> </ol> <p>Every agent has its own default set of instructions that it follows to interpret users' messages and use the necessary tools to complete its task. However, we can (and often should) define custom instructions (via the <code>custom_instructions</code> argument) for our AI assistant. Here are some guidelines to follow when creating your instructions:</p> <ul> <li>Write precise and clear instructions without overcomplicating the     agent.</li> <li>Consider edge cases and unusual or atypical scenarios.</li> <li>Be cautious to not over-specify behavior based on your primary use     case as this may limit the agent's ability to behave properly in     other situations.</li> </ul> <p>Here are the instructions we are using for our financial AI assistant:</p> <pre><code>financial_assistant_instructions = \"\"\"\n  - You are a helpful financial assistant, with expertise in financial reporting, in conversation with a user.\n  - Never discuss politics, and always respond politely.\n  - Respond in a compact format by using appropriate units of measure (e.g., K for thousands, M for millions, B for billions).\n  - Do not report the same number twice (e.g. $100K and 100,000 USD).\n  - Always check the get_company_info and get_valid_years tools to validate company and year are valid.\n  - When querying a tool for a numeric value or KPI, use a concise and non-ambiguous description of what you are looking for.\n  - If you calculate a metric, make sure you have all the necessary information to complete the calculation. Don't guess.\n\"\"\"\n</code></pre> <p>Notice how these instructions are different from the tool function descriptions. These instructions are general rules that the agent should follow. At times, these instructions may refer to specific tools, but in general, the agent should be able to decide for itself what tools it should call. This is what makes agents very powerful and makes our job as coders much simpler.</p> <p>update_func callback</p> <p>The <code>update_func</code> is an optional <code>Callable</code> function that can serve a variety of purposes for your assistant. It is a callback function that is managed by the agent, and it will be called anytime the agent is updated, such as when calling a tool, or when receiving a response from a tool.</p> <p>In our example, we will use it to log the actions of our agent so users can see the steps the agent is taking as it answers their questions. Since our assistant is using streamlit to display the results, we will append the log messages to the session state.</p> <pre><code>from vectara_agentic.agent import AgentStatusType\n\ndef update_func(status_type: AgentStatusType, msg: str):\n  output = f\"{status_type.value} - {msg}\"\n  st.session_state.log_messages.append(output)\n</code></pre> <p>agent_config</p> <p>The <code>agent_config</code> argument is an optional object that you can use to explicitly specify the configuration of your agent, including the agent type, model names and providers, and API keys. By default, each of these parameters will be read from your environment, but you can also explicitly define them with the <code>AgentConfig</code> class.</p> <p>For example, here is how we can define an <code>AgentConfig</code> object to create a ReAct agent using OPENAI as the LLM for the agent and Cohere as the LLM for the agent's tools:</p> <pre><code>from vectara_agentic.agent_config import AgentConfig\n\nconfig = AgentConfig(\n  agent_type=\"REACT\",\n  main_llm_provider=\"OPENAI\",\n  tool_llm_provider=\"COHERE\"\n)\n</code></pre> <p>Creating the agent</p> <p>Here is how we will instantiate our finance assistant:</p> <pre><code>from vectara_agentic import Agent\n\nagent = Agent(\n     tools=[tools_factory.create_tool(tool, tool_type=\"query\") for tool in\n               [\n                   get_company_info,\n                   get_valid_years,\n                   get_income_statement\n               ]\n           ] +\n           tools_factory.standard_tools() +\n           tools_factory.financial_tools() +\n           tools_factory.guardrail_tools() +\n           [ask_transcripts],\n     topic=\"10-K annual financial reports\",\n     custom_instructions=financial_assistant_instructions,\n     update_func=update_func\n)\n</code></pre> <p>Notice that when we call the <code>create_tool()</code> method, we specified a <code>tool_type</code>. This can either be <code>\"query\"</code> (default) or <code>\"action\"</code>. For our example, all of the tools are query tools, so we can easily add all of them to our agent with a list comprehension, as shown above.</p>"},{"location":"usage/#chat-with-your-assistant","title":"Chat with your Assistant","text":"<p>Once you have created your agent, using it is quite simple. All you have to do is call its <code>chat()</code> method, which prompts your agent to answer the user's query using its available set of tools. It's that easy.</p> <pre><code>query = \"Which 3 companies had the highest revenue in 2022, and how did they do in 2021?\"\nprint(str(agent.chat(query)))\n</code></pre> <p>The agent returns the response:</p> <p>The three companies with the highest revenue in 2022 were:</p> <ol> <li>Amazon (AMZN): $513.98B</li> <li>Apple (AAPL): $394.33B</li> <li>Google (GOOG): $282.84B</li> </ol> <p>Their revenues in 2021 were:</p> <ol> <li>Amazon (AMZN): $469.82B</li> <li>Apple (AAPL): $365.82B</li> <li>Google (GOOG): $257.64B</li> </ol> <p>The <code>chat()</code> function returns an <code>AgentResponse</code> object, which includes the agent's generated response text and a list of <code>ToolOutput</code> objects. The agent's response text can easily be retrieved <code>response</code> member (or simply by using <code>str()</code>). The tool information can be extracted with the <code>sources</code> member of the <code>AgentResponse</code> class and will return a list of tool outputs, including the name of each tool that was called and the output from that tool that was given to the agent.</p> <p>To make a full Streamlit app, there is some extra code that is necessary to configure the demo layout. You can check out the full code and demo for this app on Hugging Face.</p>"},{"location":"usage/#other-chat-options","title":"Other Chat Options","text":"<p>The standard <code>chat()</code> method will run synchronously, so your application will wait until the agent finishes generating its response before making any other function calls. If you would prefer to run your queries asynchronously with your application, you can use the <code>achat()</code> method.</p> <p>The <code>chat()</code> function also returns the response as a single string, which could be a lengthy text. If you would prefer to stream the agent's response by chunks, you can use the <code>stream_chat()</code> method (or <code>astream_chat()</code> method to run asynchronously). This will return an <code>AgentStreamingResponse</code> object. If you want to directly print out the response, you can use the <code>print_response_stream()</code> method. If you need to yield the chunks in some other way for your application, you can obtain the generator object by accessing the <code>chat_stream</code> member.</p>"},{"location":"usage/#additional-information","title":"Additional Information","text":"<p>Agent Information</p> <p>The <code>Agent</code> class defines a few helpful methods to help you understand the internals of your application.</p> <ol> <li>The <code>report()</code> method prints out the agent object's type (REACT,     OPENAI, or LLMCOMPILER), the tools, and the LLMs used for the main     agent and tool calling.</li> <li>The <code>token_counts()</code> method tells you how many tokens you have used     in the current session for both the main agent and tool calling     LLMs. This can be helpful for users who want to track how many     tokens have been used, which translates to how much money they are     spending.</li> </ol> <p>If you have any other information that you would like to be accessible to users, feel free to make a suggestion on our community server.</p> <p>Observability</p> <p>You can also setup full observability for your vectara-agentic assistant or agent using Arize Phoenix. This allows you to view LLM prompt inputs and outputs, the latency of each task and subtask, and many of the individual function calls performed by the LLM, as well as FCS scores for each response.</p> <p>To set up observability for your app, follow these steps:</p> <ol> <li>Set <code>os[\"VECTARA_AGENTIC_OBSERVER_TYPE\"] = \"ARIZE_PHOENIX\"</code> or     specify <code>observer = \"ARIZE_PHOENIX\"</code> in your <code>AgentConfig</code>.</li> <li>Connect to a local phoenix server:<ol> <li>If you have a local phoenix server that you've run using e.g.     <code>python -m phoenix.server.main serve</code>, vectara-agentic will send     all traces to it automatically.</li> <li>If not, vectara-agentic will run a local instance during the     agent's lifecycle, and will close it when finished.</li> <li>In both cases, traces will be sent to the local instance, and     you can see the dashboard at http://localhost:6006.</li> </ol> </li> <li>Alternatively, you can connect to a Phoenix instance hosted on     Arize.<ol> <li>Go to https://app.phoenix.arize.com, and set up an account if     you don't have one.</li> <li>Create an API key and put it in the <code>PHOENIX_API_KEY</code> variable.     This variable indicates you want to use the hosted version.</li> <li>To view the traces go to https://app.phoenix.arize.com.</li> </ol> </li> </ol> <p>In addition to the raw traces, vectara-agentic also records <code>FCS</code> values into Arize for every Vectara RAG call. You can see those results in the <code>Feedback</code> column of the arize UI.</p> <p>Query Callback</p> <p>You can define a callback function to log query/response pairs in your agent. This function should be specified in the <code>query_logging_callback</code> argument when you create your agent and should take in two string arguments. The first argument passed to this function will be the user query and the second will be the agent's response.</p> <p>If defined, this function is called every time the agent receives a query and generates a response.</p>"}]}