{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":""},{"location":"#introduction","title":"Introduction","text":"<p>What is Agentic RAG?</p> <p>Agentic RAG combines retrieval-augmented-generation (RAG) with autonomous agents. While standard RAG retrieves relevant facts and generates responses, Agentic RAG uses an LLM to \"manage\" the process through reasoning, planning, and tool usage.</p> <p>With vanilla RAG, Vectara receives a user query, retrieves the most relevant facts from your data, and uses an LLM to generate the most accurate response based on those facts. (Unfamiliar with RAG? Check out this page to learn more!)</p> <p>Agentic RAG leverages an LLM to \"manage\" the process of answering the user query via reasoning, planning, and a provided set of \"tools\". Since a \"manager\" LLM-powered agent is in charge, it is smart enough to analyze the user query and properly call tools to obtain a comprehensive response to a complex user query.</p> <p>For example:</p> <ul> <li>The agent can rephrase the user query to fit a certain style, role,     or persona.</li> <li>The agent can break the query down into multiple (simpler)     sub-queries and call the RAG query tool for each sub-query, then     combine the responses to come up with a comprehensive response.</li> <li>The agent can identify filtering criteria in the user query and use     them to filter the results when using RAG query tool.</li> </ul> <p>The main tool used in vectara-agentic is the <code>Vectara RAG query tool</code>, which queries a Vectara corpus and returns the most relevant response. By using a RAG-based agent, you mitigate some of the issues with pure LLMs, particularly hallucinations and explainability.</p> <p>Another important tool that can be used to query a Vectara corpus is the <code>Vectara search tool</code>, which queries a Vectara corpus for the most relevant search results and documents that match a query.</p> <p>Additional tools give your application superpowers to retrieve up-to-date information, access enterprise specific data via APIs, make SQL queries to a database, or even perform actions such as creating a calendar event or sending an email.</p> <p>Let's demonstrate the advantage of Agentic RAG via a simple example.</p> <p>Imagine that you have ingested into Vectara all your Google Drive files, JIRA tickets, and product documentation. You build an Agentic RAG application using these tools:</p> <ol> <li>A JIRA RAG query tool</li> <li>A Google Drive RAG query tool</li> <li>A product docs RAG query tool</li> <li>A tool that can issue SQL queries against an internal database     containing customer support data</li> </ol> <p>Consider the query: \"What is the top issue reported by customers in the last 3 months? Who is working to solve it?\"</p> <p>A standard RAG pipeline would try to match this entire query to the most relevant facts in your data, and generate a response. It may fail to distinguish the query as two separate questions, and given the complexity, may fail to produce a good response.</p> <p>An Agentic RAG assistant would recognize the complexity of the user query, and decide to act in two steps. First it will form a query with its SQL tool to identify the top issue reported by customers in the last 3 months, and then it will call the JIRA tool to identify who is working on that issue from the first query.</p> <p>What is vectara-agentic?</p> <p>Vectara-agentic is a Python package for building Agentic RAG applications powered by Vectara. It:</p> <ul> <li>Provides a simple API to define tools, including Vectara RAG tool and Vectara search tool.</li> <li>Includes pre-built tools for various domains (legal, finance, etc).</li> <li>Integrates with multiple LLM providers (OpenAI, Anthropic, Gemini, Together.AI, Cohere, Bedrock, and GROQ).</li> <li>Supports advanced workflows for complex queries.</li> </ul>"},{"location":"#agent-architecture","title":"Agent Architecture","text":"<p>Vectara-agentic follows a typical agentic RAG architecture. It consists of the following components:</p> <ul> <li>One or more RAG tools for making queries to corpora in Vectara.</li> <li>A set of additional tools that the agent can use to retrieve     information, process data, or perform actions.</li> <li>A central LLM, or agent (based on <code>FunctionCalling</code> or <code>ReAct</code> agent type) that manages the process of interpreting     the user query, creating and executing a plan to collect information     needed to respond to that query, and crafting a final response.</li> </ul>"},{"location":"#basic-example","title":"Basic Example","text":"<p>Here's a simple example creating an agent with a single RAG tool:</p> <pre><code>from vectara_agentic.agent import Agent\nfrom vectara_agentic.tools import VectaraToolFactory\nfrom pydantic import Field, BaseModel\n\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\napi_key = str(os.environ['VECTARA_API_KEY'])\ncorpus_key = str(os.environ['VECTARA_CORPUS_KEY'])\n\nvec_factory = VectaraToolFactory(\n  vectara_api_key = api_key, \n  vectara_corpus_key = corpus_key\n)\n\nask_pet_policy_tool = vec_factory.create_rag_tool(\n    tool_name = \"ask_pet_policy\",\n    tool_description = \"Responds to questions about Vectara's pet policy.\",\n    summary_num_results = 10,\n    n_sentences_before = 3,\n    n_sentences_after = 3,\n    mmr_diversity_bias = 0.1,\n    include_citations = False\n)\n\nagent = Agent(\n    tools = [ask_pet_policy_tool],\n    topic = \"Vectara Pet Policy\"\n)\n\nagent.chat(\"What is Vectara's pet policy?\")\n</code></pre> <p>When we run this code, we get the following response:</p> <p>Vectara's pet policy does not allow common household pets like cats and dogs on their campuses. Instead, they welcome a select group of exotic creatures that reflect their innovative spirit and core values. Additionally, birds are not only permitted but encouraged in their workspace as part of their unique approach.</p> <p>In the above code, we defined a single RAG tool (<code>ask_pet_policy_tool</code>) for our <code>Agent</code> class, and then created an AI assistant with this tool.</p> <p>The above code demonstrates the typical flow for instantiating your <code>Agent</code> object when you are defining more than one tool, only in this case it only used a single tool. Since making a simple assistant like this with just one RAG tool is a common need, we have provided a single function that does all of this at once called <code>from_corpus()</code>.</p> <p>Here's how you can create a simple assistant that uses a single RAG tool for asking questions about Medicare:</p> <pre><code>agent = Agent.from_corpus(\n  vectara_corpus_key=corpus_key,\n  vectara_api_key=api_key,\n  data_description=\"medical plan benefits and pricing\",\n  assistant_specialty=\"Medicare\",\n  tool_name=\"ask_medicare\",\n)\n</code></pre>"},{"location":"#try-it-yourself","title":"Try it Yourself","text":"<ol> <li>Create a Vectara account</li> <li>Set up environment variables:</li> </ol> <p>Vectara Corpus:</p> <p><code>VECTARA_CORPUS_KEY</code>: The corpus key for the corpus that contains the Vectara pet policy. You can download the Pet Policy PDF file and add it to a new or existing Vectara corpus.</p> <p><code>VECTARA_API_KEY</code>: An API key that can perform queries on this corpus.</p> <p>Agent type, LLMs and model names:</p> <p><code>VECTARA_AGENTIC_AGENT_TYPE</code>: Agent type, either FUNCTION_CALLING (default) or REACT.</p> <p>note: OPENAI agent type has been removed; use FUNCTION_CALLING agent type when using OpenAI as the agent's LLM provider (see <code>VECTARA_AGENTIC_MAIN_LLM_PROVIDER</code> below).</p> <p><code>VECTARA_AGENTIC_MAIN_LLM_PROVIDER</code>: The LLM used for the agent, either OPENAI (default), ANTHROPIC, GEMINI, TOGETHER, COHERE, BEDROCK, or GROQ.</p> <p><code>VECTARA_AGENTIC_TOOL_LLM_PROVIDER</code>: The LLM used for the agent tools, either OPENAI (default), ANTHROPIC, GEMINI, TOGETHER, COHERE, BEDROCK, or GROQ.</p> <p><code>OPENAI_API_KEY</code>, <code>ANTHROPIC_API_KEY</code>, <code>GOOGLE_API_KEY</code>, <code>TOGETHER_API_KEY</code>, <code>COHERE_API_KEY</code>, <code>BEDROCK_API_KEY</code>, or <code>GROQ_API_KEY</code>: Your API key for the agent or tool LLM, if you choose to use these services.</p> <p>Note: Fireworks AI support has been removed. If you were using Fireworks, please migrate to one of the supported providers listed above.</p> <p>With any LLM provider choice, you can also specify the model type to use via these environment variables:</p> <p><code>VECTARA_AGENTIC_MAIN_MODEL_NAME</code>: specifies the model name for the main LLM provider.</p> <p><code>VECTARA_AGENTIC_TOOL_MODEL_NAME</code>: specifies the model name for the tool LLM provider.</p> <p>Defaults:</p> <ol> <li>For <code>OPENAI</code>, the default is <code>gpt-4.1-mini</code>.</li> <li>For <code>ANTHROPIC</code>, the default is <code>claude-sonnet-4-20250514</code>.</li> <li>For <code>GEMINI</code>, the default is <code>gemini-2.5-flash-lite</code>.</li> <li>For <code>TOGETHER.AI</code>, the default is <code>deepseek-ai/DeepSeek-V3</code>.</li> <li>For <code>COHERE</code>, the default is <code>command-a-03-2025</code>.</li> <li>For <code>BEDROCK</code>, the default is <code>us.anthropic.claude-sonnet-4-20250514-v1:0</code>.</li> <li>For <code>GROQ</code>, the default is <code>openai/gpt-oss-20b</code>.</li> </ol>"},{"location":"api/","title":"API Reference","text":"<p>vectara_agentic package.</p>"},{"location":"api/#vectara_agentic.Agent","title":"<code>Agent</code>","text":"<p>Agent class for handling different types of agents and their interactions.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>class Agent:\n    \"\"\"\n    Agent class for handling different types of agents and their interactions.\n    \"\"\"\n\n    def __init__(\n        self,\n        tools: List[\"FunctionTool\"],\n        topic: str = \"general\",\n        custom_instructions: str = \"\",\n        general_instructions: str = GENERAL_INSTRUCTIONS,\n        verbose: bool = False,\n        agent_progress_callback: Optional[\n            Callable[[AgentStatusType, dict, str], None]\n        ] = None,\n        query_logging_callback: Optional[Callable[[str, str], None]] = None,\n        agent_config: Optional[AgentConfig] = None,\n        fallback_agent_config: Optional[AgentConfig] = None,\n        chat_history: Optional[list[Tuple[str, str]]] = None,\n        validate_tools: bool = False,\n        workflow_cls: Optional[\"Workflow\"] = None,\n        workflow_timeout: int = 120,\n        vectara_api_key: Optional[str] = None,\n        session_id: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the agent with the specified type, tools, topic, and system message.\n\n        Args:\n\n            tools (list[FunctionTool]): A list of tools to be used by the agent.\n            topic (str, optional): The topic for the agent. Defaults to 'general'.\n            custom_instructions (str, optional): Custom instructions for the agent. Defaults to ''.\n            general_instructions (str, optional): General instructions for the agent.\n                The Agent has a default set of instructions that are crafted to help it operate effectively.\n                This allows you to customize the agent's behavior and personality, but use with caution.\n            verbose (bool, optional): Whether the agent should print its steps. Defaults to False.\n            agent_progress_callback (Callable): A callback function the code calls on any agent updates.\n            query_logging_callback (Callable): A callback function the code calls upon completion of a query\n            agent_config (AgentConfig, optional): The configuration of the agent.\n                Defaults to AgentConfig(), which reads from environment variables.\n            fallback_agent_config (AgentConfig, optional): The fallback configuration of the agent.\n                This config is used when the main agent config fails multiple times.\n            chat_history (Tuple[str, str], optional): A list of user/agent chat pairs to initialize the agent memory.\n            validate_tools (bool, optional): Whether to validate tool inconsistency with instructions.\n                Defaults to False.\n            workflow_cls (Workflow, optional): The workflow class to be used with run(). Defaults to None.\n            workflow_timeout (int, optional): The timeout for the workflow in seconds. Defaults to 120.\n            vectara_api_key (str, optional): The Vectara API key for VHC computation. Defaults to None.\n            session_id (str, optional): The session ID for memory persistence.\n                                        If None, auto-generates from topic and date. Defaults to None.\n        \"\"\"\n        self.agent_config = agent_config or AgentConfig()\n        self.agent_config_type = AgentConfigType.DEFAULT\n        self.tools = tools\n        if not any(tool.metadata.name == \"get_current_date\" for tool in self.tools):\n            self.tools += [\n                ToolsFactory().create_tool(get_current_date, vhc_eligible=False)\n            ]\n        self.agent_type = self.agent_config.agent_type\n        self._llm = None  # Lazy loading\n        self._custom_instructions = custom_instructions\n        self._general_instructions = general_instructions\n        self._topic = topic\n        self.agent_progress_callback = agent_progress_callback\n\n        self.query_logging_callback = query_logging_callback\n        self.workflow_cls = workflow_cls\n        self.workflow_timeout = workflow_timeout\n        self.vectara_api_key = vectara_api_key or os.environ.get(\"VECTARA_API_KEY\", \"\")\n\n        # Sanitize tools for Gemini if needed\n        if self.agent_config.main_llm_provider == ModelProvider.GEMINI:\n            self.tools = sanitize_tools_for_gemini(self.tools)\n\n        # Validate tools\n        if validate_tools:\n            validate_tool_consistency(\n                self.tools, self._custom_instructions, self.agent_config\n            )\n\n        # Setup callback manager\n        callbacks: list[BaseCallbackHandler] = [\n            AgentCallbackHandler(self.agent_progress_callback)\n        ]\n        self.callback_manager = CallbackManager(callbacks)  # type: ignore\n        self.verbose = verbose\n\n        self.session_id = (\n            session_id\n            or getattr(self, \"session_id\", None)\n            or f\"{topic}:{date.today().isoformat()}\"\n        )\n\n        self.memory = Memory.from_defaults(\n            session_id=self.session_id, token_limit=65536\n        )\n        if chat_history:\n            msgs = []\n            for u, a in chat_history:\n                msgs.append(ChatMessage.from_str(u, role=MessageRole.USER))\n                msgs.append(ChatMessage.from_str(a, role=MessageRole.ASSISTANT))\n            self.memory.put_messages(msgs)\n\n        # Set up main agent and fallback agent\n        self._agent = None  # Lazy loading\n        self.fallback_agent_config = fallback_agent_config\n        self._fallback_agent = None  # Lazy loading\n\n        # Setup observability\n        try:\n            self.observability_enabled = setup_observer(self.agent_config, self.verbose)\n        except Exception as e:\n            logger.warning(f\"Failed to set up observer ({e}), ignoring\")\n            self.observability_enabled = False\n\n        # VHC state tracking\n        self._vhc_cache = {}  # Cache VHC results by query hash\n        self._last_query = None\n        self._last_response = None\n        self._current_tool_outputs = []  # Store tool outputs from current query for VHC\n\n    @property\n    def llm(self):\n        \"\"\"Lazy-loads the LLM.\"\"\"\n        if self._llm is None:\n            self._llm = get_llm(LLMRole.MAIN, config=self.agent_config)\n        return self._llm\n\n    @property\n    def agent(self):\n        \"\"\"Lazy-loads the agent.\"\"\"\n        if self._agent is None:\n            self._agent = self._create_agent(self.agent_config, self.callback_manager)\n        return self._agent\n\n    @property\n    def fallback_agent(self):\n        \"\"\"Lazy-loads the fallback agent.\"\"\"\n        if self._fallback_agent is None and self.fallback_agent_config:\n            self._fallback_agent = self._create_agent(\n                self.fallback_agent_config, self.callback_manager\n            )\n        return self._fallback_agent\n\n    def _create_agent(\n        self, config: AgentConfig, llm_callback_manager: \"CallbackManager\"\n    ) -&gt; \"BaseWorkflowAgent\":\n        \"\"\"\n        Creates the agent based on the configuration object.\n\n        Args:\n            config: The configuration of the agent.\n            llm_callback_manager: The callback manager for the agent's llm.\n\n        Returns:\n            BaseWorkflowAgent: The configured agent object.\n        \"\"\"\n        # Use the same LLM instance for consistency\n        llm = (\n            self.llm\n            if config == self.agent_config\n            else get_llm(LLMRole.MAIN, config=config)\n        )\n        llm.callback_manager = llm_callback_manager\n\n        return create_agent_from_config(\n            tools=self.tools,\n            llm=llm,\n            memory=self.memory,\n            config=config,\n            callback_manager=llm_callback_manager,\n            general_instructions=self._general_instructions,\n            topic=self._topic,\n            custom_instructions=self._custom_instructions,\n            verbose=self.verbose,\n        )\n\n    def clear_memory(self) -&gt; None:\n        \"\"\"Clear the agent's memory and reset agent instances to ensure consistency.\"\"\"\n        self.memory.reset()\n        # Clear agent instances so they get recreated with the cleared memory\n        self._agent = None\n        self._fallback_agent = None\n\n    def __eq__(self, other):\n        if not isinstance(other, Agent):\n            logger.debug(\n                f\"Comparison failed: other is not an instance of Agent. (self: {type(self)}, other: {type(other)})\"\n            )\n            return False\n\n        # Compare agent_type\n        if self.agent_config.agent_type != other.agent_config.agent_type:\n            logger.debug(\n                f\"Comparison failed: agent_type differs. (self.agent_config.agent_type: {self.agent_config.agent_type},\"\n                f\" other.agent_config.agent_type: {other.agent_config.agent_type})\"\n            )\n            return False\n\n        # Compare tools\n        if self.tools != other.tools:\n            logger.debug(\n                \"Comparison failed: tools differ.\"\n                f\"(self.tools: {[t.metadata.name for t in self.tools]}, \"\n                f\"other.tools: {[t.metadata.name for t in other.tools]})\"\n            )\n            return False\n\n        # Compare topic\n        if self._topic != other._topic:\n            logger.debug(\n                f\"Comparison failed: topic differs. (self.topic: {self._topic}, other.topic: {other._topic})\"\n            )\n            return False\n\n        # Compare custom_instructions\n        if self._custom_instructions != other._custom_instructions:\n            logger.debug(\n                \"Comparison failed: custom_instructions differ. (self.custom_instructions: \"\n                f\"{self._custom_instructions}, other.custom_instructions: {other._custom_instructions})\"\n            )\n            return False\n\n        # Compare verbose\n        if self.verbose != other.verbose:\n            logger.debug(\n                f\"Comparison failed: verbose differs. (self.verbose: {self.verbose}, other.verbose: {other.verbose})\"\n            )\n            return False\n\n        # Compare agent memory\n        if self.memory.get() != other.memory.get():\n            logger.debug(\"Comparison failed: agent memory differs.\")\n            return False\n\n        # If all comparisons pass\n        logger.debug(\"All comparisons passed. Objects are equal.\")\n        return True\n\n    @classmethod\n    def from_tools(\n        cls,\n        tools: List[\"FunctionTool\"],\n        topic: str = \"general\",\n        custom_instructions: str = \"\",\n        verbose: bool = True,\n        agent_progress_callback: Optional[\n            Callable[[AgentStatusType, dict, str], None]\n        ] = None,\n        query_logging_callback: Optional[Callable[[str, str], None]] = None,\n        agent_config: AgentConfig = AgentConfig(),\n        validate_tools: bool = False,\n        fallback_agent_config: Optional[AgentConfig] = None,\n        chat_history: Optional[list[Tuple[str, str]]] = None,\n        workflow_cls: Optional[\"Workflow\"] = None,\n        workflow_timeout: int = 120,\n        session_id: Optional[str] = None,\n    ) -&gt; \"Agent\":\n        \"\"\"\n        Create an agent from tools, agent type, and language model.\n\n        Args:\n\n            tools (list[FunctionTool]): A list of tools to be used by the agent.\n            topic (str, optional): The topic for the agent. Defaults to 'general'.\n            custom_instructions (str, optional): custom instructions for the agent. Defaults to ''.\n            verbose (bool, optional): Whether the agent should print its steps. Defaults to True.\n            agent_progress_callback (Callable): A callback function the code calls on any agent updates.\n            query_logging_callback (Callable): A callback function the code calls upon completion of a query\n            agent_config (AgentConfig, optional): The configuration of the agent.\n            fallback_agent_config (AgentConfig, optional): The fallback configuration of the agent.\n            chat_history (Tuple[str, str], optional): A list of user/agent chat pairs to initialize the agent memory.\n            validate_tools (bool, optional): Whether to validate tool inconsistency with instructions.\n                Defaults to False.\n            workflow_cls (Workflow, optional): The workflow class to be used with run(). Defaults to None.\n            workflow_timeout (int, optional): The timeout for the workflow in seconds. Defaults to 120.\n            session_id (str, optional): The session ID for memory persistence.\n                                        If None, auto-generates from topic and date. Defaults to None.\n\n        Returns:\n            Agent: An instance of the Agent class.\n        \"\"\"\n        return cls(\n            tools=tools,\n            topic=topic,\n            custom_instructions=custom_instructions,\n            verbose=verbose,\n            agent_progress_callback=agent_progress_callback,\n            query_logging_callback=query_logging_callback,\n            agent_config=agent_config,\n            chat_history=chat_history,\n            validate_tools=validate_tools,\n            fallback_agent_config=fallback_agent_config,\n            workflow_cls=workflow_cls,\n            workflow_timeout=workflow_timeout,\n            session_id=session_id,\n        )\n\n    @classmethod\n    def from_corpus(\n        cls,\n        tool_name: str,\n        data_description: str,\n        assistant_specialty: str,\n        general_instructions: str = GENERAL_INSTRUCTIONS,\n        vectara_corpus_key: str = str(os.environ.get(\"VECTARA_CORPUS_KEY\", \"\")),\n        vectara_api_key: str = str(os.environ.get(\"VECTARA_API_KEY\", \"\")),\n        agent_progress_callback: Optional[\n            Callable[[AgentStatusType, dict, str], None]\n        ] = None,\n        query_logging_callback: Optional[Callable[[str, str], None]] = None,\n        agent_config: AgentConfig = AgentConfig(),\n        fallback_agent_config: Optional[AgentConfig] = None,\n        chat_history: Optional[list[Tuple[str, str]]] = None,\n        verbose: bool = False,\n        vectara_filter_fields: list[dict] = [],\n        vectara_offset: int = 0,\n        vectara_lambda_val: float = 0.005,\n        vectara_semantics: str = \"default\",\n        vectara_custom_dimensions: Dict = {},\n        vectara_reranker: str = \"slingshot\",\n        vectara_rerank_k: int = 50,\n        vectara_rerank_limit: Optional[int] = None,\n        vectara_rerank_cutoff: Optional[float] = None,\n        vectara_diversity_bias: float = 0.2,\n        vectara_udf_expression: Optional[str] = None,\n        vectara_rerank_chain: Optional[List[Dict]] = None,\n        vectara_n_sentences_before: int = 2,\n        vectara_n_sentences_after: int = 2,\n        vectara_summary_num_results: int = 10,\n        vectara_summarizer: str = \"vectara-summary-ext-24-05-med-omni\",\n        vectara_summary_response_language: str = \"eng\",\n        vectara_summary_prompt_text: Optional[str] = None,\n        vectara_max_response_chars: Optional[int] = None,\n        vectara_max_tokens: Optional[int] = None,\n        vectara_temperature: Optional[float] = None,\n        vectara_frequency_penalty: Optional[float] = None,\n        vectara_presence_penalty: Optional[float] = None,\n        vectara_save_history: bool = True,\n        return_direct: bool = False,\n        session_id: Optional[str] = None,\n    ) -&gt; \"Agent\":\n        \"\"\"Create an agent from a single Vectara corpus using the factory function.\n\n        Args:\n            tool_name (str): Name of the tool to be created.\n            data_description (str): Description of the data/corpus.\n            assistant_specialty (str): The specialty/topic of the assistant.\n            session_id (str, optional): The session ID for memory persistence.\n                                        If None, auto-generates from topic and date. Defaults to None.\n            ... (other parameters as documented in factory function)\n        \"\"\"\n        # Use the factory function to avoid code duplication\n        config = create_agent_from_corpus(\n            tool_name=tool_name,\n            data_description=data_description,\n            assistant_specialty=assistant_specialty,\n            general_instructions=general_instructions,\n            vectara_corpus_key=vectara_corpus_key,\n            vectara_api_key=vectara_api_key,\n            agent_config=agent_config,\n            fallback_agent_config=fallback_agent_config,\n            verbose=verbose,\n            vectara_filter_fields=vectara_filter_fields,\n            vectara_offset=vectara_offset,\n            vectara_lambda_val=vectara_lambda_val,\n            vectara_semantics=vectara_semantics,\n            vectara_custom_dimensions=vectara_custom_dimensions,\n            vectara_reranker=vectara_reranker,\n            vectara_rerank_k=vectara_rerank_k,\n            vectara_rerank_limit=vectara_rerank_limit,\n            vectara_rerank_cutoff=vectara_rerank_cutoff,\n            vectara_diversity_bias=vectara_diversity_bias,\n            vectara_udf_expression=vectara_udf_expression,\n            vectara_rerank_chain=vectara_rerank_chain,\n            vectara_n_sentences_before=vectara_n_sentences_before,\n            vectara_n_sentences_after=vectara_n_sentences_after,\n            vectara_summary_num_results=vectara_summary_num_results,\n            vectara_summarizer=vectara_summarizer,\n            vectara_summary_response_language=vectara_summary_response_language,\n            vectara_summary_prompt_text=vectara_summary_prompt_text,\n            vectara_max_response_chars=vectara_max_response_chars,\n            vectara_max_tokens=vectara_max_tokens,\n            vectara_temperature=vectara_temperature,\n            vectara_frequency_penalty=vectara_frequency_penalty,\n            vectara_presence_penalty=vectara_presence_penalty,\n            vectara_save_history=vectara_save_history,\n            return_direct=return_direct,\n        )\n\n        return cls(\n            chat_history=chat_history,\n            agent_progress_callback=agent_progress_callback,\n            query_logging_callback=query_logging_callback,\n            session_id=session_id,\n            **config,\n        )\n\n    def _switch_agent_config(self) -&gt; None:\n        \"\"\"\n        Switch the configuration type of the agent.\n        This function is called automatically to switch the agent configuration if the current configuration fails.\n        Ensures memory consistency by clearing agent instances so they are recreated with current memory.\n        \"\"\"\n        if self.agent_config_type == AgentConfigType.DEFAULT:\n            self.agent_config_type = AgentConfigType.FALLBACK\n            # Clear the fallback agent so it gets recreated with current memory\n            self._fallback_agent = None\n        else:\n            self.agent_config_type = AgentConfigType.DEFAULT\n            # Clear the main agent so it gets recreated with current memory\n            self._agent = None\n\n    def _reset_agent_state(self) -&gt; None:\n        \"\"\"\n        Reset agent state to recover from workflow runtime errors.\n        Clears both agent instances to force recreation with fresh state.\n        \"\"\"\n        self._agent = None\n        self._fallback_agent = None\n\n    def report(self, detailed: bool = False) -&gt; None:\n        \"\"\"\n        Get a report from the agent.\n\n        Args:\n            detailed (bool, optional): Whether to include detailed information. Defaults to False.\n\n        Returns:\n            str: The report from the agent.\n        \"\"\"\n        logger.info(\"Vectara agentic Report:\")\n        logger.info(f\"Agent Type = {self.agent_config.agent_type}\")\n        logger.info(f\"Topic = {self._topic}\")\n        logger.info(\"Tools:\")\n        for tool in self.tools:\n            if hasattr(tool, \"metadata\"):\n                if detailed:\n                    logger.info(f\"- {tool.metadata.description}\")\n                else:\n                    logger.info(f\"- {tool.metadata.name}\")\n            else:\n                logger.info(\"- tool without metadata\")\n        logger.info(\n            f\"Agent LLM = {get_llm(LLMRole.MAIN, config=self.agent_config).metadata.model_name}\"\n        )\n        logger.info(\n            f\"Tool LLM = {get_llm(LLMRole.TOOL, config=self.agent_config).metadata.model_name}\"\n        )\n\n    def _get_current_agent(self):\n        return (\n            self.agent\n            if self.agent_config_type == AgentConfigType.DEFAULT\n            else self.fallback_agent\n        )\n\n    def _get_current_agent_type(self):\n        return (\n            self.agent_config.agent_type\n            if self.agent_config_type == AgentConfigType.DEFAULT\n            or not self.fallback_agent_config\n            else self.fallback_agent_config.agent_type\n        )\n\n    def chat(self, prompt: str) -&gt; AgentResponse:\n        \"\"\"\n        Interact with the agent using a chat prompt.\n\n        Args:\n            prompt (str): The chat prompt.\n\n        Returns:\n            AgentResponse: The response from the agent.\n        \"\"\"\n        try:\n            loop = asyncio.get_running_loop()\n            if hasattr(loop, \"_nest_level\"):\n                return asyncio.run(self.achat(prompt))\n        except (RuntimeError, ImportError):\n            # No running loop or nest_asyncio not available\n            return asyncio.run(self.achat(prompt))\n\n        # We are inside a running loop without nest_asyncio\n        raise RuntimeError(\n            \"Use `await agent.achat(...)` inside an event loop (e.g. Jupyter).\"\n        )\n\n    async def achat(self, prompt: str) -&gt; AgentResponse:  # type: ignore\n        \"\"\"\n        Interact with the agent using a chat prompt.\n\n        Args:\n            prompt (str): The chat prompt.\n\n        Returns:\n            AgentResponse: The response from the agent.\n        \"\"\"\n        if not prompt or not prompt.strip():\n            return AgentResponse(response=\"Please provide a valid prompt.\")\n\n        max_attempts = 4 if self.fallback_agent_config else 2\n        attempt = 0\n        orig_llm = self.llm.metadata.model_name\n        last_error = None\n        while attempt &lt; max_attempts:\n            try:\n                current_agent = self._get_current_agent()\n\n                # Deal with workflow-based agent types (Function Calling and ReAct)\n                if self._get_current_agent_type() in [\n                    AgentType.FUNCTION_CALLING,\n                    AgentType.REACT,\n                ]:\n                    from llama_index.core.workflow import Context\n\n                    # Create context and pass memory to the workflow agent\n                    # According to LlamaIndex docs, we should let the workflow manage memory internally\n                    ctx = Context(current_agent)\n\n                    handler = current_agent.run(\n                        user_msg=prompt, memory=self.memory, ctx=ctx\n                    )\n\n                    # Listen to workflow events if progress callback is set\n                    if self.agent_progress_callback:\n                        # Import the event ID utility function\n                        from .agent_core.streaming import get_event_id\n\n                        async for event in handler.stream_events():\n                            # Use consistent event ID tracking to ensure tool calls and outputs are paired\n                            event_id = get_event_id(event)\n\n                            # Handle different types of workflow events using same logic as FunctionCallingStreamHandler\n                            from llama_index.core.agent.workflow import (\n                                ToolCall,\n                                ToolCallResult,\n                                AgentInput,\n                                AgentOutput,\n                            )\n\n                            if isinstance(event, ToolCall):\n                                self.agent_progress_callback(\n                                    status_type=AgentStatusType.TOOL_CALL,\n                                    msg={\n                                        \"tool_name\": event.tool_name,\n                                        \"arguments\": json.dumps(event.tool_kwargs),\n                                    },\n                                    event_id=event_id,\n                                )\n                            elif isinstance(event, ToolCallResult):\n                                self.agent_progress_callback(\n                                    status_type=AgentStatusType.TOOL_OUTPUT,\n                                    msg={\n                                        \"tool_name\": event.tool_name,\n                                        \"content\": str(event.tool_output),\n                                    },\n                                    event_id=event_id,\n                                )\n                            elif isinstance(event, AgentInput):\n                                self.agent_progress_callback(\n                                    status_type=AgentStatusType.AGENT_UPDATE,\n                                    msg={\"content\": f\"Agent input: {event.input}\"},\n                                    event_id=event_id,\n                                )\n                            elif isinstance(event, AgentOutput):\n                                self.agent_progress_callback(\n                                    status_type=AgentStatusType.AGENT_UPDATE,\n                                    msg={\"content\": f\"Agent output: {event.response}\"},\n                                    event_id=event_id,\n                                )\n\n                    result = await handler\n\n                    # Ensure we have an AgentResponse object with a string response\n                    if hasattr(result, \"response\"):\n                        response_text = result.response\n                    else:\n                        response_text = str(result)\n\n                    # Handle case where response is a ChatMessage object\n                    if hasattr(response_text, \"content\"):\n                        response_text = response_text.content\n                    elif not isinstance(response_text, str):\n                        response_text = str(response_text)\n\n                    if response_text is None or response_text == \"None\":\n                        # Try to find tool outputs in the result object\n                        response_text = None\n\n                        # Check various possible locations for tool outputs\n                        if hasattr(result, \"tool_outputs\") and result.tool_outputs:\n                            # Get the latest tool output\n                            latest_output = (\n                                result.tool_outputs[-1]\n                                if isinstance(result.tool_outputs, list)\n                                else result.tool_outputs\n                            )\n                            response_text = str(latest_output)\n\n                        # Check if there are tool_calls with results\n                        elif hasattr(result, \"tool_calls\") and result.tool_calls:\n                            # Tool calls might contain the outputs - let's try to extract them\n                            for tool_call in result.tool_calls:\n                                if (\n                                    hasattr(tool_call, \"tool_output\")\n                                    and tool_call.tool_output is not None\n                                ):\n                                    response_text = str(tool_call.tool_output)\n                                    break\n\n                        elif hasattr(result, \"sources\") or hasattr(\n                            result, \"source_nodes\"\n                        ):\n                            sources = getattr(\n                                result, \"sources\", getattr(result, \"source_nodes\", [])\n                            )\n                            if (\n                                sources\n                                and len(sources) &gt; 0\n                                and hasattr(sources[0], \"text\")\n                            ):\n                                response_text = sources[0].text\n\n                        # Check for workflow context or chat history that might contain tool results\n                        elif hasattr(result, \"chat_history\"):\n                            # Look for the most recent assistant message that might contain tool results\n                            chat_history = result.chat_history\n                            if chat_history and len(chat_history) &gt; 0:\n                                for msg in reversed(chat_history):\n                                    if (\n                                        msg.role == MessageRole.TOOL\n                                        and msg.content\n                                        and str(msg.content).strip()\n                                    ):\n                                        response_text = msg.content\n                                        break\n                                    if (\n                                        hasattr(msg, \"content\")\n                                        and msg.content\n                                        and str(msg.content).strip()\n                                    ):\n                                        response_text = msg.content\n                                        break\n\n                        # If we still don't have a response, provide a fallback\n                        if response_text is None or response_text == \"None\":\n                            response_text = \"Response completed.\"\n\n                    agent_response = AgentResponse(\n                        response=response_text, metadata=getattr(result, \"metadata\", {})\n                    )\n\n                    # Retrieve updated memory from workflow context\n                    # According to LlamaIndex docs, workflow agents manage memory internally\n                    # and we can access it via ctx.store.get(\"memory\")\n                    try:\n                        workflow_memory = await ctx.store.get(\"memory\")\n                        if workflow_memory:\n                            # Update our external memory with the workflow's memory\n                            self.memory = workflow_memory\n                    except Exception as e:\n                        # If we can't retrieve workflow memory, fall back to manual management\n                        warning_msg = (\n                            f\"Could not retrieve workflow memory, falling back to \"\n                            f\"manual management: {e}\"\n                        )\n                        logger.warning(warning_msg)\n                        user_msg = ChatMessage.from_str(prompt, role=MessageRole.USER)\n                        assistant_msg = ChatMessage.from_str(\n                            response_text, role=MessageRole.ASSISTANT\n                        )\n                        self.memory.put_messages([user_msg, assistant_msg])\n\n                # Standard chat interaction for other agent types\n                else:\n                    agent_response = await current_agent.achat(prompt)\n\n                # Post processing after response is generated\n                agent_response.metadata = agent_response.metadata or {}\n                user_metadata = agent_response.metadata\n                agent_response = await execute_post_stream_processing(\n                    agent_response, prompt, self, user_metadata\n                )\n                return agent_response\n\n            except Exception as e:\n                last_error = e\n                if self.verbose:\n                    logger.warning(\n                        f\"LLM call failed on attempt {attempt}. \" f\"Error: {e}.\"\n                    )\n                if attempt &gt;= 2 and self.fallback_agent_config:\n                    self._switch_agent_config()\n                await asyncio.sleep(1)\n                attempt += 1\n\n        return AgentResponse(\n            response=(\n                f\"For {orig_llm} LLM - failure can't be resolved after \"\n                f\"{max_attempts} attempts ({last_error}).\"\n            )\n        )\n\n    def stream_chat(self, prompt: str) -&gt; AgentStreamingResponse:\n        \"\"\"\n        Interact with the agent using a chat prompt with streaming.\n        Args:\n            prompt (str): The chat prompt.\n        Returns:\n            AgentStreamingResponse: The streaming response from the agent.\n        \"\"\"\n        try:\n            _ = asyncio.get_running_loop()\n        except RuntimeError:\n            return asyncio.run(self.astream_chat(prompt))\n        raise RuntimeError(\n            \"Use `await agent.astream_chat(...)` inside an event loop (e.g. Jupyter).\"\n        )\n\n    async def astream_chat(self, prompt: str) -&gt; AgentStreamingResponse:  # type: ignore\n        \"\"\"\n        Interact with the agent using a chat prompt asynchronously with streaming.\n        Args:\n            prompt (str): The chat prompt.\n        Returns:\n            AgentStreamingResponse: The streaming response from the agent.\n        \"\"\"\n        # Store query for VHC processing and clear previous tool outputs\n        self._last_query = prompt\n        self._clear_tool_outputs()\n        max_attempts = 4 if self.fallback_agent_config else 2\n        attempt = 0\n        orig_llm = self.llm.metadata.model_name\n        last_error = None\n        while attempt &lt; max_attempts:\n            try:\n                current_agent = self._get_current_agent()\n                user_meta: Dict[str, Any] = {}\n\n                # Deal with Function Calling agent type\n                if self._get_current_agent_type() == AgentType.FUNCTION_CALLING:\n                    from llama_index.core.workflow import Context\n\n                    # Create context and pass memory to the workflow agent\n                    # According to LlamaIndex docs, we should let the workflow manage memory internally\n                    ctx = Context(current_agent)\n\n                    handler = current_agent.run(\n                        user_msg=prompt, memory=self.memory, ctx=ctx\n                    )\n\n                    # Use the dedicated FunctionCallingStreamHandler\n                    stream_handler = FunctionCallingStreamHandler(self, handler, prompt)\n                    streaming_adapter = stream_handler.create_streaming_response(\n                        user_meta\n                    )\n\n                    return AgentStreamingResponse(\n                        base=streaming_adapter, metadata=user_meta\n                    )\n\n                # Deal with ReAct agent type\n                elif self._get_current_agent_type() == AgentType.REACT:\n                    from llama_index.core.workflow import Context\n\n                    # Create context and pass memory to the workflow agent\n                    ctx = Context(current_agent)\n\n                    handler = current_agent.run(\n                        user_msg=prompt, memory=self.memory, ctx=ctx\n                    )\n\n                    # Create a streaming adapter for ReAct with event handling\n                    react_stream_handler = ReActStreamHandler(self, handler, prompt)\n                    streaming_adapter = react_stream_handler.create_streaming_response(\n                        user_meta\n                    )\n\n                    return AgentStreamingResponse(\n                        base=streaming_adapter, metadata=user_meta\n                    )\n\n                #\n                # For other agent types, use the standard async chat method\n                #\n                li_stream = await current_agent.astream_chat(prompt)\n                orig_async = li_stream.async_response_gen\n\n                # Define a wrapper to preserve streaming behavior while executing post-stream logic.\n                async def _stream_response_wrapper():\n                    async for tok in orig_async():\n                        yield tok\n\n                    # Use shared post-processing function\n                    await execute_post_stream_processing(\n                        li_stream, prompt, self, user_meta\n                    )\n\n                li_stream.async_response_gen = _stream_response_wrapper\n                return AgentStreamingResponse(base=li_stream, metadata=user_meta)\n\n            except Exception as e:\n                last_error = e\n                if attempt &gt;= 2 and self.fallback_agent_config:\n                    self._switch_agent_config()\n                await asyncio.sleep(1)\n                attempt += 1\n\n        return AgentStreamingResponse.from_error(\n            f\"For {orig_llm} LLM - failure can't be resolved after \"\n            f\"{max_attempts} attempts ({last_error}).\"\n        )\n\n    def _clear_tool_outputs(self):\n        \"\"\"Clear stored tool outputs at the start of a new query.\"\"\"\n        self._current_tool_outputs.clear()\n        logging.info(\"\ud83d\udd27 [TOOL_STORAGE] Cleared stored tool outputs for new query\")\n\n    def _add_tool_output(self, tool_name: str, content: str):\n        \"\"\"Add a tool output to the current collection for VHC.\"\"\"\n        tool_output = {\n            \"status_type\": \"TOOL_OUTPUT\",\n            \"content\": content,\n            \"tool_name\": tool_name,\n        }\n        self._current_tool_outputs.append(tool_output)\n        logging.info(\n            f\"\ud83d\udd27 [TOOL_STORAGE] Added tool output from '{tool_name}': {len(content)} chars\"\n        )\n\n    def _get_stored_tool_outputs(self) -&gt; List[dict]:\n        \"\"\"Get the stored tool outputs from the current query.\"\"\"\n        logging.info(\n            f\"\ud83d\udd27 [TOOL_STORAGE] Retrieved {len(self._current_tool_outputs)} stored tool outputs\"\n        )\n        return self._current_tool_outputs.copy()\n\n    async def acompute_vhc(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Compute VHC for the last query/response pair (async version).\n        Results are cached for subsequent calls. Tool outputs are automatically\n        collected during streaming and used internally.\n\n        Returns:\n            Dict[str, Any]: Dictionary containing 'corrected_text' and 'corrections'\n        \"\"\"\n        logging.info(\n            f\"\ud83d\udd0d\ud83d\udd0d\ud83d\udd0d [VHC_AGENT_ENTRY] UNIQUE_DEBUG_MESSAGE acompute_vhc method called - \"\n            f\"stored_tool_outputs_count={len(self._current_tool_outputs)}\"\n        )\n        logging.info(\n            f\"\ud83d\udd0d\ud83d\udd0d\ud83d\udd0d [VHC_AGENT_ENTRY] _last_query: {'set' if self._last_query else 'None'}\"\n        )\n\n        if not self._last_query:\n            logging.info(\"\ud83d\udd0d [VHC_AGENT] Returning early - no _last_query\")\n            return {\"corrected_text\": None, \"corrections\": []}\n\n        # For VHC to work, we need the response text from memory\n        # Get the latest assistant response from memory\n        messages = self.memory.get()\n        logging.info(\n            f\"\ud83d\udd0d [VHC_AGENT] memory.get() returned {len(messages) if messages else 0} messages\"\n        )\n\n        if not messages:\n            logging.info(\"\ud83d\udd0d [VHC_AGENT] Returning early - no messages in memory\")\n            return {\"corrected_text\": None, \"corrections\": []}\n\n        # Find the last assistant message\n        last_response = None\n        for msg in reversed(messages):\n            if msg.role == MessageRole.ASSISTANT:\n                last_response = msg.content\n                break\n\n        logging.info(\n            f\"\ud83d\udd0d [VHC_AGENT] Found last_response: {'set' if last_response else 'None'}\"\n        )\n\n        if not last_response:\n            logging.info(\n                \"\ud83d\udd0d [VHC_AGENT] Returning early - no last assistant response found\"\n            )\n            return {\"corrected_text\": None, \"corrections\": []}\n\n        # Update stored response for caching\n        self._last_response = last_response\n\n        # Create cache key from query + response\n        cache_key = hash(f\"{self._last_query}:{self._last_response}\")\n\n        # Return cached results if available\n        if cache_key in self._vhc_cache:\n            return self._vhc_cache[cache_key]\n\n        # Check if we have VHC API key\n        logging.info(\n            f\"\ud83d\udd0d [VHC_AGENT] acompute_vhc called with vectara_api_key={'set' if self.vectara_api_key else 'None'}\"\n        )\n        if not self.vectara_api_key:\n            logging.info(\n                \"\ud83d\udd0d [VHC_AGENT] No vectara_api_key - returning early with None\"\n            )\n            return {\"corrected_text\": None, \"corrections\": []}\n\n        # Compute VHC using existing library function\n        from .agent_core.utils.hallucination import analyze_hallucinations\n\n        try:\n            # Use stored tool outputs from current query\n            stored_tool_outputs = self._get_stored_tool_outputs()\n            logging.info(\n                f\"\ud83d\udd27 [VHC_AGENT] Using {len(stored_tool_outputs)} stored tool outputs for VHC\"\n            )\n\n            corrected_text, corrections = analyze_hallucinations(\n                query=self._last_query,\n                chat_history=self.memory.get(),\n                agent_response=self._last_response,\n                tools=self.tools,\n                vectara_api_key=self.vectara_api_key,\n                tool_outputs=stored_tool_outputs,\n            )\n\n            # Cache results\n            results = {\"corrected_text\": corrected_text, \"corrections\": corrections}\n            self._vhc_cache[cache_key] = results\n\n            return results\n\n        except Exception as e:\n            logger.error(f\"VHC computation failed: {e}\")\n            return {\"corrected_text\": None, \"corrections\": []}\n\n    def compute_vhc(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Compute VHC for the last query/response pair (sync version).\n        Results are cached for subsequent calls. Tool outputs are automatically\n        collected during streaming and used internally.\n\n        Returns:\n            Dict[str, Any]: Dictionary containing 'corrected_text' and 'corrections'\n        \"\"\"\n        try:\n            loop = asyncio.get_event_loop()\n            return loop.run_until_complete(self.acompute_vhc())\n        except RuntimeError:\n            # No event loop running, create a new one\n            return asyncio.run(self.acompute_vhc())\n\n    #\n    # run() method for running a workflow\n    # workflow will always get these arguments in the StartEvent: agent, tools, llm, verbose\n    # the inputs argument comes from the call to run()\n    #\n    async def run(\n        self,\n        inputs: Any,\n        verbose: bool = False,\n    ) -&gt; Any:\n        \"\"\"\n        Run a workflow using the agent.\n        workflow class must be provided in the agent constructor.\n        Args:\n            inputs (Any): The inputs to the workflow.\n            verbose (bool, optional): Whether to print verbose output. Defaults to False.\n        Returns:\n            Any: The output or context of the workflow.\n        \"\"\"\n        # Create workflow\n        if self.workflow_cls:\n            workflow = self.workflow_cls(timeout=self.workflow_timeout, verbose=verbose)\n        else:\n            raise ValueError(\"Workflow is not defined.\")\n\n        # Validate inputs is in the form of workflow.InputsModel\n        if not isinstance(inputs, self.workflow_cls.InputsModel):\n            raise ValueError(f\"Inputs must be an instance of {workflow.InputsModel}.\")\n\n        outputs_model_on_fail_cls = getattr(\n            workflow.__class__, \"OutputModelOnFail\", None\n        )\n        if outputs_model_on_fail_cls:\n            fields_without_default = []\n            for name, field_info in outputs_model_on_fail_cls.model_fields.items():\n                if field_info.default_factory is PydanticUndefined:\n                    fields_without_default.append(name)\n            if fields_without_default:\n                raise ValueError(\n                    f\"Fields without default values: {fields_without_default}\"\n                )\n\n        from llama_index.core.workflow import Context\n\n        workflow_context = Context(workflow=workflow)\n        try:\n            # run workflow\n            result = await workflow.run(\n                ctx=workflow_context,\n                agent=self,\n                tools=self.tools,\n                llm=self.llm,\n                verbose=verbose,\n                inputs=inputs,\n            )\n\n            # return output in the form of workflow.OutputsModel(BaseModel)\n            try:\n                output = workflow.OutputsModel.model_validate(result)\n            except ValidationError as e:\n                raise ValueError(f\"Failed to map workflow output to model: {e}\") from e\n\n        except Exception as e:\n            _missing = object()\n            if outputs_model_on_fail_cls:\n                model_fields = outputs_model_on_fail_cls.model_fields\n                input_dict = {}\n                for key in model_fields:\n                    value = await workflow_context.get(key, default=_missing)\n                    if value is not _missing:\n                        input_dict[key] = value\n                output = outputs_model_on_fail_cls.model_validate(input_dict)\n            else:\n                logger.warning(\n                    f\"Vectara Agentic: Workflow failed with unexpected error: {e}\"\n                )\n                raise type(e)(str(e)).with_traceback(e.__traceback__)\n\n        return output\n\n    #\n    # Serialization methods\n    #\n    def dumps(self) -&gt; str:\n        \"\"\"Serialize the Agent instance to a JSON string.\"\"\"\n        return json.dumps(self.to_dict())\n\n    @classmethod\n    def loads(\n        cls,\n        data: str,\n        agent_progress_callback: Optional[\n            Callable[[AgentStatusType, dict, str], None]\n        ] = None,\n        query_logging_callback: Optional[Callable[[str, str], None]] = None,\n    ) -&gt; \"Agent\":\n        \"\"\"Create an Agent instance from a JSON string.\"\"\"\n        return cls.from_dict(\n            json.loads(data), agent_progress_callback, query_logging_callback\n        )\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Serialize the Agent instance to a dictionary.\"\"\"\n        return serialize_agent_to_dict(self)\n\n    @classmethod\n    def from_dict(\n        cls,\n        data: Dict[str, Any],\n        agent_progress_callback: Optional[Callable] = None,\n        query_logging_callback: Optional[Callable] = None,\n    ) -&gt; \"Agent\":\n        \"\"\"Create an Agent instance from a dictionary.\"\"\"\n        return deserialize_agent_from_dict(\n            cls, data, agent_progress_callback, query_logging_callback\n        )\n\n    def cleanup(self) -&gt; None:\n        \"\"\"Clean up resources used by the agent.\"\"\"\n        from ._observability import shutdown_observer\n\n        if hasattr(self, \"agent\") and hasattr(self.agent, \"_llm\"):\n            llm = self.agent._llm\n            if hasattr(llm, \"client\") and hasattr(llm.client, \"close\"):\n                try:\n                    if asyncio.iscoroutinefunction(llm.client.close):\n                        asyncio.run(llm.client.close())\n                    else:\n                        llm.client.close()\n                except Exception:\n                    pass\n\n        # Shutdown observability connections\n        shutdown_observer()\n\n    def __enter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit with cleanup.\"\"\"\n        self.cleanup()\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.agent","title":"<code>agent</code>  <code>property</code>","text":"<p>Lazy-loads the agent.</p>"},{"location":"api/#vectara_agentic.Agent.fallback_agent","title":"<code>fallback_agent</code>  <code>property</code>","text":"<p>Lazy-loads the fallback agent.</p>"},{"location":"api/#vectara_agentic.Agent.llm","title":"<code>llm</code>  <code>property</code>","text":"<p>Lazy-loads the LLM.</p>"},{"location":"api/#vectara_agentic.Agent.__enter__","title":"<code>__enter__()</code>","text":"<p>Context manager entry.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>def __enter__(self):\n    \"\"\"Context manager entry.\"\"\"\n    return self\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Context manager exit with cleanup.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit with cleanup.\"\"\"\n    self.cleanup()\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.__init__","title":"<code>__init__(tools, topic='general', custom_instructions='', general_instructions=GENERAL_INSTRUCTIONS, verbose=False, agent_progress_callback=None, query_logging_callback=None, agent_config=None, fallback_agent_config=None, chat_history=None, validate_tools=False, workflow_cls=None, workflow_timeout=120, vectara_api_key=None, session_id=None)</code>","text":"<p>Initialize the agent with the specified type, tools, topic, and system message.</p> <p>Args:</p> <pre><code>tools (list[FunctionTool]): A list of tools to be used by the agent.\ntopic (str, optional): The topic for the agent. Defaults to 'general'.\ncustom_instructions (str, optional): Custom instructions for the agent. Defaults to ''.\ngeneral_instructions (str, optional): General instructions for the agent.\n    The Agent has a default set of instructions that are crafted to help it operate effectively.\n    This allows you to customize the agent's behavior and personality, but use with caution.\nverbose (bool, optional): Whether the agent should print its steps. Defaults to False.\nagent_progress_callback (Callable): A callback function the code calls on any agent updates.\nquery_logging_callback (Callable): A callback function the code calls upon completion of a query\nagent_config (AgentConfig, optional): The configuration of the agent.\n    Defaults to AgentConfig(), which reads from environment variables.\nfallback_agent_config (AgentConfig, optional): The fallback configuration of the agent.\n    This config is used when the main agent config fails multiple times.\nchat_history (Tuple[str, str], optional): A list of user/agent chat pairs to initialize the agent memory.\nvalidate_tools (bool, optional): Whether to validate tool inconsistency with instructions.\n    Defaults to False.\nworkflow_cls (Workflow, optional): The workflow class to be used with run(). Defaults to None.\nworkflow_timeout (int, optional): The timeout for the workflow in seconds. Defaults to 120.\nvectara_api_key (str, optional): The Vectara API key for VHC computation. Defaults to None.\nsession_id (str, optional): The session ID for memory persistence.\n                            If None, auto-generates from topic and date. Defaults to None.\n</code></pre> Source code in <code>vectara_agentic/agent.py</code> <pre><code>def __init__(\n    self,\n    tools: List[\"FunctionTool\"],\n    topic: str = \"general\",\n    custom_instructions: str = \"\",\n    general_instructions: str = GENERAL_INSTRUCTIONS,\n    verbose: bool = False,\n    agent_progress_callback: Optional[\n        Callable[[AgentStatusType, dict, str], None]\n    ] = None,\n    query_logging_callback: Optional[Callable[[str, str], None]] = None,\n    agent_config: Optional[AgentConfig] = None,\n    fallback_agent_config: Optional[AgentConfig] = None,\n    chat_history: Optional[list[Tuple[str, str]]] = None,\n    validate_tools: bool = False,\n    workflow_cls: Optional[\"Workflow\"] = None,\n    workflow_timeout: int = 120,\n    vectara_api_key: Optional[str] = None,\n    session_id: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the agent with the specified type, tools, topic, and system message.\n\n    Args:\n\n        tools (list[FunctionTool]): A list of tools to be used by the agent.\n        topic (str, optional): The topic for the agent. Defaults to 'general'.\n        custom_instructions (str, optional): Custom instructions for the agent. Defaults to ''.\n        general_instructions (str, optional): General instructions for the agent.\n            The Agent has a default set of instructions that are crafted to help it operate effectively.\n            This allows you to customize the agent's behavior and personality, but use with caution.\n        verbose (bool, optional): Whether the agent should print its steps. Defaults to False.\n        agent_progress_callback (Callable): A callback function the code calls on any agent updates.\n        query_logging_callback (Callable): A callback function the code calls upon completion of a query\n        agent_config (AgentConfig, optional): The configuration of the agent.\n            Defaults to AgentConfig(), which reads from environment variables.\n        fallback_agent_config (AgentConfig, optional): The fallback configuration of the agent.\n            This config is used when the main agent config fails multiple times.\n        chat_history (Tuple[str, str], optional): A list of user/agent chat pairs to initialize the agent memory.\n        validate_tools (bool, optional): Whether to validate tool inconsistency with instructions.\n            Defaults to False.\n        workflow_cls (Workflow, optional): The workflow class to be used with run(). Defaults to None.\n        workflow_timeout (int, optional): The timeout for the workflow in seconds. Defaults to 120.\n        vectara_api_key (str, optional): The Vectara API key for VHC computation. Defaults to None.\n        session_id (str, optional): The session ID for memory persistence.\n                                    If None, auto-generates from topic and date. Defaults to None.\n    \"\"\"\n    self.agent_config = agent_config or AgentConfig()\n    self.agent_config_type = AgentConfigType.DEFAULT\n    self.tools = tools\n    if not any(tool.metadata.name == \"get_current_date\" for tool in self.tools):\n        self.tools += [\n            ToolsFactory().create_tool(get_current_date, vhc_eligible=False)\n        ]\n    self.agent_type = self.agent_config.agent_type\n    self._llm = None  # Lazy loading\n    self._custom_instructions = custom_instructions\n    self._general_instructions = general_instructions\n    self._topic = topic\n    self.agent_progress_callback = agent_progress_callback\n\n    self.query_logging_callback = query_logging_callback\n    self.workflow_cls = workflow_cls\n    self.workflow_timeout = workflow_timeout\n    self.vectara_api_key = vectara_api_key or os.environ.get(\"VECTARA_API_KEY\", \"\")\n\n    # Sanitize tools for Gemini if needed\n    if self.agent_config.main_llm_provider == ModelProvider.GEMINI:\n        self.tools = sanitize_tools_for_gemini(self.tools)\n\n    # Validate tools\n    if validate_tools:\n        validate_tool_consistency(\n            self.tools, self._custom_instructions, self.agent_config\n        )\n\n    # Setup callback manager\n    callbacks: list[BaseCallbackHandler] = [\n        AgentCallbackHandler(self.agent_progress_callback)\n    ]\n    self.callback_manager = CallbackManager(callbacks)  # type: ignore\n    self.verbose = verbose\n\n    self.session_id = (\n        session_id\n        or getattr(self, \"session_id\", None)\n        or f\"{topic}:{date.today().isoformat()}\"\n    )\n\n    self.memory = Memory.from_defaults(\n        session_id=self.session_id, token_limit=65536\n    )\n    if chat_history:\n        msgs = []\n        for u, a in chat_history:\n            msgs.append(ChatMessage.from_str(u, role=MessageRole.USER))\n            msgs.append(ChatMessage.from_str(a, role=MessageRole.ASSISTANT))\n        self.memory.put_messages(msgs)\n\n    # Set up main agent and fallback agent\n    self._agent = None  # Lazy loading\n    self.fallback_agent_config = fallback_agent_config\n    self._fallback_agent = None  # Lazy loading\n\n    # Setup observability\n    try:\n        self.observability_enabled = setup_observer(self.agent_config, self.verbose)\n    except Exception as e:\n        logger.warning(f\"Failed to set up observer ({e}), ignoring\")\n        self.observability_enabled = False\n\n    # VHC state tracking\n    self._vhc_cache = {}  # Cache VHC results by query hash\n    self._last_query = None\n    self._last_response = None\n    self._current_tool_outputs = []  # Store tool outputs from current query for VHC\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.achat","title":"<code>achat(prompt)</code>  <code>async</code>","text":"<p>Interact with the agent using a chat prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The chat prompt.</p> required <p>Returns:</p> Name Type Description <code>AgentResponse</code> <code>AgentResponse</code> <p>The response from the agent.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>async def achat(self, prompt: str) -&gt; AgentResponse:  # type: ignore\n    \"\"\"\n    Interact with the agent using a chat prompt.\n\n    Args:\n        prompt (str): The chat prompt.\n\n    Returns:\n        AgentResponse: The response from the agent.\n    \"\"\"\n    if not prompt or not prompt.strip():\n        return AgentResponse(response=\"Please provide a valid prompt.\")\n\n    max_attempts = 4 if self.fallback_agent_config else 2\n    attempt = 0\n    orig_llm = self.llm.metadata.model_name\n    last_error = None\n    while attempt &lt; max_attempts:\n        try:\n            current_agent = self._get_current_agent()\n\n            # Deal with workflow-based agent types (Function Calling and ReAct)\n            if self._get_current_agent_type() in [\n                AgentType.FUNCTION_CALLING,\n                AgentType.REACT,\n            ]:\n                from llama_index.core.workflow import Context\n\n                # Create context and pass memory to the workflow agent\n                # According to LlamaIndex docs, we should let the workflow manage memory internally\n                ctx = Context(current_agent)\n\n                handler = current_agent.run(\n                    user_msg=prompt, memory=self.memory, ctx=ctx\n                )\n\n                # Listen to workflow events if progress callback is set\n                if self.agent_progress_callback:\n                    # Import the event ID utility function\n                    from .agent_core.streaming import get_event_id\n\n                    async for event in handler.stream_events():\n                        # Use consistent event ID tracking to ensure tool calls and outputs are paired\n                        event_id = get_event_id(event)\n\n                        # Handle different types of workflow events using same logic as FunctionCallingStreamHandler\n                        from llama_index.core.agent.workflow import (\n                            ToolCall,\n                            ToolCallResult,\n                            AgentInput,\n                            AgentOutput,\n                        )\n\n                        if isinstance(event, ToolCall):\n                            self.agent_progress_callback(\n                                status_type=AgentStatusType.TOOL_CALL,\n                                msg={\n                                    \"tool_name\": event.tool_name,\n                                    \"arguments\": json.dumps(event.tool_kwargs),\n                                },\n                                event_id=event_id,\n                            )\n                        elif isinstance(event, ToolCallResult):\n                            self.agent_progress_callback(\n                                status_type=AgentStatusType.TOOL_OUTPUT,\n                                msg={\n                                    \"tool_name\": event.tool_name,\n                                    \"content\": str(event.tool_output),\n                                },\n                                event_id=event_id,\n                            )\n                        elif isinstance(event, AgentInput):\n                            self.agent_progress_callback(\n                                status_type=AgentStatusType.AGENT_UPDATE,\n                                msg={\"content\": f\"Agent input: {event.input}\"},\n                                event_id=event_id,\n                            )\n                        elif isinstance(event, AgentOutput):\n                            self.agent_progress_callback(\n                                status_type=AgentStatusType.AGENT_UPDATE,\n                                msg={\"content\": f\"Agent output: {event.response}\"},\n                                event_id=event_id,\n                            )\n\n                result = await handler\n\n                # Ensure we have an AgentResponse object with a string response\n                if hasattr(result, \"response\"):\n                    response_text = result.response\n                else:\n                    response_text = str(result)\n\n                # Handle case where response is a ChatMessage object\n                if hasattr(response_text, \"content\"):\n                    response_text = response_text.content\n                elif not isinstance(response_text, str):\n                    response_text = str(response_text)\n\n                if response_text is None or response_text == \"None\":\n                    # Try to find tool outputs in the result object\n                    response_text = None\n\n                    # Check various possible locations for tool outputs\n                    if hasattr(result, \"tool_outputs\") and result.tool_outputs:\n                        # Get the latest tool output\n                        latest_output = (\n                            result.tool_outputs[-1]\n                            if isinstance(result.tool_outputs, list)\n                            else result.tool_outputs\n                        )\n                        response_text = str(latest_output)\n\n                    # Check if there are tool_calls with results\n                    elif hasattr(result, \"tool_calls\") and result.tool_calls:\n                        # Tool calls might contain the outputs - let's try to extract them\n                        for tool_call in result.tool_calls:\n                            if (\n                                hasattr(tool_call, \"tool_output\")\n                                and tool_call.tool_output is not None\n                            ):\n                                response_text = str(tool_call.tool_output)\n                                break\n\n                    elif hasattr(result, \"sources\") or hasattr(\n                        result, \"source_nodes\"\n                    ):\n                        sources = getattr(\n                            result, \"sources\", getattr(result, \"source_nodes\", [])\n                        )\n                        if (\n                            sources\n                            and len(sources) &gt; 0\n                            and hasattr(sources[0], \"text\")\n                        ):\n                            response_text = sources[0].text\n\n                    # Check for workflow context or chat history that might contain tool results\n                    elif hasattr(result, \"chat_history\"):\n                        # Look for the most recent assistant message that might contain tool results\n                        chat_history = result.chat_history\n                        if chat_history and len(chat_history) &gt; 0:\n                            for msg in reversed(chat_history):\n                                if (\n                                    msg.role == MessageRole.TOOL\n                                    and msg.content\n                                    and str(msg.content).strip()\n                                ):\n                                    response_text = msg.content\n                                    break\n                                if (\n                                    hasattr(msg, \"content\")\n                                    and msg.content\n                                    and str(msg.content).strip()\n                                ):\n                                    response_text = msg.content\n                                    break\n\n                    # If we still don't have a response, provide a fallback\n                    if response_text is None or response_text == \"None\":\n                        response_text = \"Response completed.\"\n\n                agent_response = AgentResponse(\n                    response=response_text, metadata=getattr(result, \"metadata\", {})\n                )\n\n                # Retrieve updated memory from workflow context\n                # According to LlamaIndex docs, workflow agents manage memory internally\n                # and we can access it via ctx.store.get(\"memory\")\n                try:\n                    workflow_memory = await ctx.store.get(\"memory\")\n                    if workflow_memory:\n                        # Update our external memory with the workflow's memory\n                        self.memory = workflow_memory\n                except Exception as e:\n                    # If we can't retrieve workflow memory, fall back to manual management\n                    warning_msg = (\n                        f\"Could not retrieve workflow memory, falling back to \"\n                        f\"manual management: {e}\"\n                    )\n                    logger.warning(warning_msg)\n                    user_msg = ChatMessage.from_str(prompt, role=MessageRole.USER)\n                    assistant_msg = ChatMessage.from_str(\n                        response_text, role=MessageRole.ASSISTANT\n                    )\n                    self.memory.put_messages([user_msg, assistant_msg])\n\n            # Standard chat interaction for other agent types\n            else:\n                agent_response = await current_agent.achat(prompt)\n\n            # Post processing after response is generated\n            agent_response.metadata = agent_response.metadata or {}\n            user_metadata = agent_response.metadata\n            agent_response = await execute_post_stream_processing(\n                agent_response, prompt, self, user_metadata\n            )\n            return agent_response\n\n        except Exception as e:\n            last_error = e\n            if self.verbose:\n                logger.warning(\n                    f\"LLM call failed on attempt {attempt}. \" f\"Error: {e}.\"\n                )\n            if attempt &gt;= 2 and self.fallback_agent_config:\n                self._switch_agent_config()\n            await asyncio.sleep(1)\n            attempt += 1\n\n    return AgentResponse(\n        response=(\n            f\"For {orig_llm} LLM - failure can't be resolved after \"\n            f\"{max_attempts} attempts ({last_error}).\"\n        )\n    )\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.acompute_vhc","title":"<code>acompute_vhc()</code>  <code>async</code>","text":"<p>Compute VHC for the last query/response pair (async version). Results are cached for subsequent calls. Tool outputs are automatically collected during streaming and used internally.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary containing 'corrected_text' and 'corrections'</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>async def acompute_vhc(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Compute VHC for the last query/response pair (async version).\n    Results are cached for subsequent calls. Tool outputs are automatically\n    collected during streaming and used internally.\n\n    Returns:\n        Dict[str, Any]: Dictionary containing 'corrected_text' and 'corrections'\n    \"\"\"\n    logging.info(\n        f\"\ud83d\udd0d\ud83d\udd0d\ud83d\udd0d [VHC_AGENT_ENTRY] UNIQUE_DEBUG_MESSAGE acompute_vhc method called - \"\n        f\"stored_tool_outputs_count={len(self._current_tool_outputs)}\"\n    )\n    logging.info(\n        f\"\ud83d\udd0d\ud83d\udd0d\ud83d\udd0d [VHC_AGENT_ENTRY] _last_query: {'set' if self._last_query else 'None'}\"\n    )\n\n    if not self._last_query:\n        logging.info(\"\ud83d\udd0d [VHC_AGENT] Returning early - no _last_query\")\n        return {\"corrected_text\": None, \"corrections\": []}\n\n    # For VHC to work, we need the response text from memory\n    # Get the latest assistant response from memory\n    messages = self.memory.get()\n    logging.info(\n        f\"\ud83d\udd0d [VHC_AGENT] memory.get() returned {len(messages) if messages else 0} messages\"\n    )\n\n    if not messages:\n        logging.info(\"\ud83d\udd0d [VHC_AGENT] Returning early - no messages in memory\")\n        return {\"corrected_text\": None, \"corrections\": []}\n\n    # Find the last assistant message\n    last_response = None\n    for msg in reversed(messages):\n        if msg.role == MessageRole.ASSISTANT:\n            last_response = msg.content\n            break\n\n    logging.info(\n        f\"\ud83d\udd0d [VHC_AGENT] Found last_response: {'set' if last_response else 'None'}\"\n    )\n\n    if not last_response:\n        logging.info(\n            \"\ud83d\udd0d [VHC_AGENT] Returning early - no last assistant response found\"\n        )\n        return {\"corrected_text\": None, \"corrections\": []}\n\n    # Update stored response for caching\n    self._last_response = last_response\n\n    # Create cache key from query + response\n    cache_key = hash(f\"{self._last_query}:{self._last_response}\")\n\n    # Return cached results if available\n    if cache_key in self._vhc_cache:\n        return self._vhc_cache[cache_key]\n\n    # Check if we have VHC API key\n    logging.info(\n        f\"\ud83d\udd0d [VHC_AGENT] acompute_vhc called with vectara_api_key={'set' if self.vectara_api_key else 'None'}\"\n    )\n    if not self.vectara_api_key:\n        logging.info(\n            \"\ud83d\udd0d [VHC_AGENT] No vectara_api_key - returning early with None\"\n        )\n        return {\"corrected_text\": None, \"corrections\": []}\n\n    # Compute VHC using existing library function\n    from .agent_core.utils.hallucination import analyze_hallucinations\n\n    try:\n        # Use stored tool outputs from current query\n        stored_tool_outputs = self._get_stored_tool_outputs()\n        logging.info(\n            f\"\ud83d\udd27 [VHC_AGENT] Using {len(stored_tool_outputs)} stored tool outputs for VHC\"\n        )\n\n        corrected_text, corrections = analyze_hallucinations(\n            query=self._last_query,\n            chat_history=self.memory.get(),\n            agent_response=self._last_response,\n            tools=self.tools,\n            vectara_api_key=self.vectara_api_key,\n            tool_outputs=stored_tool_outputs,\n        )\n\n        # Cache results\n        results = {\"corrected_text\": corrected_text, \"corrections\": corrections}\n        self._vhc_cache[cache_key] = results\n\n        return results\n\n    except Exception as e:\n        logger.error(f\"VHC computation failed: {e}\")\n        return {\"corrected_text\": None, \"corrections\": []}\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.astream_chat","title":"<code>astream_chat(prompt)</code>  <code>async</code>","text":"<p>Interact with the agent using a chat prompt asynchronously with streaming. Args:     prompt (str): The chat prompt. Returns:     AgentStreamingResponse: The streaming response from the agent.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>async def astream_chat(self, prompt: str) -&gt; AgentStreamingResponse:  # type: ignore\n    \"\"\"\n    Interact with the agent using a chat prompt asynchronously with streaming.\n    Args:\n        prompt (str): The chat prompt.\n    Returns:\n        AgentStreamingResponse: The streaming response from the agent.\n    \"\"\"\n    # Store query for VHC processing and clear previous tool outputs\n    self._last_query = prompt\n    self._clear_tool_outputs()\n    max_attempts = 4 if self.fallback_agent_config else 2\n    attempt = 0\n    orig_llm = self.llm.metadata.model_name\n    last_error = None\n    while attempt &lt; max_attempts:\n        try:\n            current_agent = self._get_current_agent()\n            user_meta: Dict[str, Any] = {}\n\n            # Deal with Function Calling agent type\n            if self._get_current_agent_type() == AgentType.FUNCTION_CALLING:\n                from llama_index.core.workflow import Context\n\n                # Create context and pass memory to the workflow agent\n                # According to LlamaIndex docs, we should let the workflow manage memory internally\n                ctx = Context(current_agent)\n\n                handler = current_agent.run(\n                    user_msg=prompt, memory=self.memory, ctx=ctx\n                )\n\n                # Use the dedicated FunctionCallingStreamHandler\n                stream_handler = FunctionCallingStreamHandler(self, handler, prompt)\n                streaming_adapter = stream_handler.create_streaming_response(\n                    user_meta\n                )\n\n                return AgentStreamingResponse(\n                    base=streaming_adapter, metadata=user_meta\n                )\n\n            # Deal with ReAct agent type\n            elif self._get_current_agent_type() == AgentType.REACT:\n                from llama_index.core.workflow import Context\n\n                # Create context and pass memory to the workflow agent\n                ctx = Context(current_agent)\n\n                handler = current_agent.run(\n                    user_msg=prompt, memory=self.memory, ctx=ctx\n                )\n\n                # Create a streaming adapter for ReAct with event handling\n                react_stream_handler = ReActStreamHandler(self, handler, prompt)\n                streaming_adapter = react_stream_handler.create_streaming_response(\n                    user_meta\n                )\n\n                return AgentStreamingResponse(\n                    base=streaming_adapter, metadata=user_meta\n                )\n\n            #\n            # For other agent types, use the standard async chat method\n            #\n            li_stream = await current_agent.astream_chat(prompt)\n            orig_async = li_stream.async_response_gen\n\n            # Define a wrapper to preserve streaming behavior while executing post-stream logic.\n            async def _stream_response_wrapper():\n                async for tok in orig_async():\n                    yield tok\n\n                # Use shared post-processing function\n                await execute_post_stream_processing(\n                    li_stream, prompt, self, user_meta\n                )\n\n            li_stream.async_response_gen = _stream_response_wrapper\n            return AgentStreamingResponse(base=li_stream, metadata=user_meta)\n\n        except Exception as e:\n            last_error = e\n            if attempt &gt;= 2 and self.fallback_agent_config:\n                self._switch_agent_config()\n            await asyncio.sleep(1)\n            attempt += 1\n\n    return AgentStreamingResponse.from_error(\n        f\"For {orig_llm} LLM - failure can't be resolved after \"\n        f\"{max_attempts} attempts ({last_error}).\"\n    )\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.chat","title":"<code>chat(prompt)</code>","text":"<p>Interact with the agent using a chat prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The chat prompt.</p> required <p>Returns:</p> Name Type Description <code>AgentResponse</code> <code>AgentResponse</code> <p>The response from the agent.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>def chat(self, prompt: str) -&gt; AgentResponse:\n    \"\"\"\n    Interact with the agent using a chat prompt.\n\n    Args:\n        prompt (str): The chat prompt.\n\n    Returns:\n        AgentResponse: The response from the agent.\n    \"\"\"\n    try:\n        loop = asyncio.get_running_loop()\n        if hasattr(loop, \"_nest_level\"):\n            return asyncio.run(self.achat(prompt))\n    except (RuntimeError, ImportError):\n        # No running loop or nest_asyncio not available\n        return asyncio.run(self.achat(prompt))\n\n    # We are inside a running loop without nest_asyncio\n    raise RuntimeError(\n        \"Use `await agent.achat(...)` inside an event loop (e.g. Jupyter).\"\n    )\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.cleanup","title":"<code>cleanup()</code>","text":"<p>Clean up resources used by the agent.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>def cleanup(self) -&gt; None:\n    \"\"\"Clean up resources used by the agent.\"\"\"\n    from ._observability import shutdown_observer\n\n    if hasattr(self, \"agent\") and hasattr(self.agent, \"_llm\"):\n        llm = self.agent._llm\n        if hasattr(llm, \"client\") and hasattr(llm.client, \"close\"):\n            try:\n                if asyncio.iscoroutinefunction(llm.client.close):\n                    asyncio.run(llm.client.close())\n                else:\n                    llm.client.close()\n            except Exception:\n                pass\n\n    # Shutdown observability connections\n    shutdown_observer()\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.clear_memory","title":"<code>clear_memory()</code>","text":"<p>Clear the agent's memory and reset agent instances to ensure consistency.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>def clear_memory(self) -&gt; None:\n    \"\"\"Clear the agent's memory and reset agent instances to ensure consistency.\"\"\"\n    self.memory.reset()\n    # Clear agent instances so they get recreated with the cleared memory\n    self._agent = None\n    self._fallback_agent = None\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.compute_vhc","title":"<code>compute_vhc()</code>","text":"<p>Compute VHC for the last query/response pair (sync version). Results are cached for subsequent calls. Tool outputs are automatically collected during streaming and used internally.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary containing 'corrected_text' and 'corrections'</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>def compute_vhc(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Compute VHC for the last query/response pair (sync version).\n    Results are cached for subsequent calls. Tool outputs are automatically\n    collected during streaming and used internally.\n\n    Returns:\n        Dict[str, Any]: Dictionary containing 'corrected_text' and 'corrections'\n    \"\"\"\n    try:\n        loop = asyncio.get_event_loop()\n        return loop.run_until_complete(self.acompute_vhc())\n    except RuntimeError:\n        # No event loop running, create a new one\n        return asyncio.run(self.acompute_vhc())\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.dumps","title":"<code>dumps()</code>","text":"<p>Serialize the Agent instance to a JSON string.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>def dumps(self) -&gt; str:\n    \"\"\"Serialize the Agent instance to a JSON string.\"\"\"\n    return json.dumps(self.to_dict())\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.from_corpus","title":"<code>from_corpus(tool_name, data_description, assistant_specialty, general_instructions=GENERAL_INSTRUCTIONS, vectara_corpus_key=str(os.environ.get('VECTARA_CORPUS_KEY', '')), vectara_api_key=str(os.environ.get('VECTARA_API_KEY', '')), agent_progress_callback=None, query_logging_callback=None, agent_config=AgentConfig(), fallback_agent_config=None, chat_history=None, verbose=False, vectara_filter_fields=[], vectara_offset=0, vectara_lambda_val=0.005, vectara_semantics='default', vectara_custom_dimensions={}, vectara_reranker='slingshot', vectara_rerank_k=50, vectara_rerank_limit=None, vectara_rerank_cutoff=None, vectara_diversity_bias=0.2, vectara_udf_expression=None, vectara_rerank_chain=None, vectara_n_sentences_before=2, vectara_n_sentences_after=2, vectara_summary_num_results=10, vectara_summarizer='vectara-summary-ext-24-05-med-omni', vectara_summary_response_language='eng', vectara_summary_prompt_text=None, vectara_max_response_chars=None, vectara_max_tokens=None, vectara_temperature=None, vectara_frequency_penalty=None, vectara_presence_penalty=None, vectara_save_history=True, return_direct=False, session_id=None)</code>  <code>classmethod</code>","text":"<p>Create an agent from a single Vectara corpus using the factory function.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>Name of the tool to be created.</p> required <code>data_description</code> <code>str</code> <p>Description of the data/corpus.</p> required <code>assistant_specialty</code> <code>str</code> <p>The specialty/topic of the assistant.</p> required <code>session_id</code> <code>str</code> <p>The session ID for memory persistence.                         If None, auto-generates from topic and date. Defaults to None.</p> <code>None</code> Source code in <code>vectara_agentic/agent.py</code> <pre><code>@classmethod\ndef from_corpus(\n    cls,\n    tool_name: str,\n    data_description: str,\n    assistant_specialty: str,\n    general_instructions: str = GENERAL_INSTRUCTIONS,\n    vectara_corpus_key: str = str(os.environ.get(\"VECTARA_CORPUS_KEY\", \"\")),\n    vectara_api_key: str = str(os.environ.get(\"VECTARA_API_KEY\", \"\")),\n    agent_progress_callback: Optional[\n        Callable[[AgentStatusType, dict, str], None]\n    ] = None,\n    query_logging_callback: Optional[Callable[[str, str], None]] = None,\n    agent_config: AgentConfig = AgentConfig(),\n    fallback_agent_config: Optional[AgentConfig] = None,\n    chat_history: Optional[list[Tuple[str, str]]] = None,\n    verbose: bool = False,\n    vectara_filter_fields: list[dict] = [],\n    vectara_offset: int = 0,\n    vectara_lambda_val: float = 0.005,\n    vectara_semantics: str = \"default\",\n    vectara_custom_dimensions: Dict = {},\n    vectara_reranker: str = \"slingshot\",\n    vectara_rerank_k: int = 50,\n    vectara_rerank_limit: Optional[int] = None,\n    vectara_rerank_cutoff: Optional[float] = None,\n    vectara_diversity_bias: float = 0.2,\n    vectara_udf_expression: Optional[str] = None,\n    vectara_rerank_chain: Optional[List[Dict]] = None,\n    vectara_n_sentences_before: int = 2,\n    vectara_n_sentences_after: int = 2,\n    vectara_summary_num_results: int = 10,\n    vectara_summarizer: str = \"vectara-summary-ext-24-05-med-omni\",\n    vectara_summary_response_language: str = \"eng\",\n    vectara_summary_prompt_text: Optional[str] = None,\n    vectara_max_response_chars: Optional[int] = None,\n    vectara_max_tokens: Optional[int] = None,\n    vectara_temperature: Optional[float] = None,\n    vectara_frequency_penalty: Optional[float] = None,\n    vectara_presence_penalty: Optional[float] = None,\n    vectara_save_history: bool = True,\n    return_direct: bool = False,\n    session_id: Optional[str] = None,\n) -&gt; \"Agent\":\n    \"\"\"Create an agent from a single Vectara corpus using the factory function.\n\n    Args:\n        tool_name (str): Name of the tool to be created.\n        data_description (str): Description of the data/corpus.\n        assistant_specialty (str): The specialty/topic of the assistant.\n        session_id (str, optional): The session ID for memory persistence.\n                                    If None, auto-generates from topic and date. Defaults to None.\n        ... (other parameters as documented in factory function)\n    \"\"\"\n    # Use the factory function to avoid code duplication\n    config = create_agent_from_corpus(\n        tool_name=tool_name,\n        data_description=data_description,\n        assistant_specialty=assistant_specialty,\n        general_instructions=general_instructions,\n        vectara_corpus_key=vectara_corpus_key,\n        vectara_api_key=vectara_api_key,\n        agent_config=agent_config,\n        fallback_agent_config=fallback_agent_config,\n        verbose=verbose,\n        vectara_filter_fields=vectara_filter_fields,\n        vectara_offset=vectara_offset,\n        vectara_lambda_val=vectara_lambda_val,\n        vectara_semantics=vectara_semantics,\n        vectara_custom_dimensions=vectara_custom_dimensions,\n        vectara_reranker=vectara_reranker,\n        vectara_rerank_k=vectara_rerank_k,\n        vectara_rerank_limit=vectara_rerank_limit,\n        vectara_rerank_cutoff=vectara_rerank_cutoff,\n        vectara_diversity_bias=vectara_diversity_bias,\n        vectara_udf_expression=vectara_udf_expression,\n        vectara_rerank_chain=vectara_rerank_chain,\n        vectara_n_sentences_before=vectara_n_sentences_before,\n        vectara_n_sentences_after=vectara_n_sentences_after,\n        vectara_summary_num_results=vectara_summary_num_results,\n        vectara_summarizer=vectara_summarizer,\n        vectara_summary_response_language=vectara_summary_response_language,\n        vectara_summary_prompt_text=vectara_summary_prompt_text,\n        vectara_max_response_chars=vectara_max_response_chars,\n        vectara_max_tokens=vectara_max_tokens,\n        vectara_temperature=vectara_temperature,\n        vectara_frequency_penalty=vectara_frequency_penalty,\n        vectara_presence_penalty=vectara_presence_penalty,\n        vectara_save_history=vectara_save_history,\n        return_direct=return_direct,\n    )\n\n    return cls(\n        chat_history=chat_history,\n        agent_progress_callback=agent_progress_callback,\n        query_logging_callback=query_logging_callback,\n        session_id=session_id,\n        **config,\n    )\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.from_dict","title":"<code>from_dict(data, agent_progress_callback=None, query_logging_callback=None)</code>  <code>classmethod</code>","text":"<p>Create an Agent instance from a dictionary.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>@classmethod\ndef from_dict(\n    cls,\n    data: Dict[str, Any],\n    agent_progress_callback: Optional[Callable] = None,\n    query_logging_callback: Optional[Callable] = None,\n) -&gt; \"Agent\":\n    \"\"\"Create an Agent instance from a dictionary.\"\"\"\n    return deserialize_agent_from_dict(\n        cls, data, agent_progress_callback, query_logging_callback\n    )\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.from_tools","title":"<code>from_tools(tools, topic='general', custom_instructions='', verbose=True, agent_progress_callback=None, query_logging_callback=None, agent_config=AgentConfig(), validate_tools=False, fallback_agent_config=None, chat_history=None, workflow_cls=None, workflow_timeout=120, session_id=None)</code>  <code>classmethod</code>","text":"<p>Create an agent from tools, agent type, and language model.</p> <p>Args:</p> <pre><code>tools (list[FunctionTool]): A list of tools to be used by the agent.\ntopic (str, optional): The topic for the agent. Defaults to 'general'.\ncustom_instructions (str, optional): custom instructions for the agent. Defaults to ''.\nverbose (bool, optional): Whether the agent should print its steps. Defaults to True.\nagent_progress_callback (Callable): A callback function the code calls on any agent updates.\nquery_logging_callback (Callable): A callback function the code calls upon completion of a query\nagent_config (AgentConfig, optional): The configuration of the agent.\nfallback_agent_config (AgentConfig, optional): The fallback configuration of the agent.\nchat_history (Tuple[str, str], optional): A list of user/agent chat pairs to initialize the agent memory.\nvalidate_tools (bool, optional): Whether to validate tool inconsistency with instructions.\n    Defaults to False.\nworkflow_cls (Workflow, optional): The workflow class to be used with run(). Defaults to None.\nworkflow_timeout (int, optional): The timeout for the workflow in seconds. Defaults to 120.\nsession_id (str, optional): The session ID for memory persistence.\n                            If None, auto-generates from topic and date. Defaults to None.\n</code></pre> <p>Returns:</p> Name Type Description <code>Agent</code> <code>Agent</code> <p>An instance of the Agent class.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>@classmethod\ndef from_tools(\n    cls,\n    tools: List[\"FunctionTool\"],\n    topic: str = \"general\",\n    custom_instructions: str = \"\",\n    verbose: bool = True,\n    agent_progress_callback: Optional[\n        Callable[[AgentStatusType, dict, str], None]\n    ] = None,\n    query_logging_callback: Optional[Callable[[str, str], None]] = None,\n    agent_config: AgentConfig = AgentConfig(),\n    validate_tools: bool = False,\n    fallback_agent_config: Optional[AgentConfig] = None,\n    chat_history: Optional[list[Tuple[str, str]]] = None,\n    workflow_cls: Optional[\"Workflow\"] = None,\n    workflow_timeout: int = 120,\n    session_id: Optional[str] = None,\n) -&gt; \"Agent\":\n    \"\"\"\n    Create an agent from tools, agent type, and language model.\n\n    Args:\n\n        tools (list[FunctionTool]): A list of tools to be used by the agent.\n        topic (str, optional): The topic for the agent. Defaults to 'general'.\n        custom_instructions (str, optional): custom instructions for the agent. Defaults to ''.\n        verbose (bool, optional): Whether the agent should print its steps. Defaults to True.\n        agent_progress_callback (Callable): A callback function the code calls on any agent updates.\n        query_logging_callback (Callable): A callback function the code calls upon completion of a query\n        agent_config (AgentConfig, optional): The configuration of the agent.\n        fallback_agent_config (AgentConfig, optional): The fallback configuration of the agent.\n        chat_history (Tuple[str, str], optional): A list of user/agent chat pairs to initialize the agent memory.\n        validate_tools (bool, optional): Whether to validate tool inconsistency with instructions.\n            Defaults to False.\n        workflow_cls (Workflow, optional): The workflow class to be used with run(). Defaults to None.\n        workflow_timeout (int, optional): The timeout for the workflow in seconds. Defaults to 120.\n        session_id (str, optional): The session ID for memory persistence.\n                                    If None, auto-generates from topic and date. Defaults to None.\n\n    Returns:\n        Agent: An instance of the Agent class.\n    \"\"\"\n    return cls(\n        tools=tools,\n        topic=topic,\n        custom_instructions=custom_instructions,\n        verbose=verbose,\n        agent_progress_callback=agent_progress_callback,\n        query_logging_callback=query_logging_callback,\n        agent_config=agent_config,\n        chat_history=chat_history,\n        validate_tools=validate_tools,\n        fallback_agent_config=fallback_agent_config,\n        workflow_cls=workflow_cls,\n        workflow_timeout=workflow_timeout,\n        session_id=session_id,\n    )\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.loads","title":"<code>loads(data, agent_progress_callback=None, query_logging_callback=None)</code>  <code>classmethod</code>","text":"<p>Create an Agent instance from a JSON string.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>@classmethod\ndef loads(\n    cls,\n    data: str,\n    agent_progress_callback: Optional[\n        Callable[[AgentStatusType, dict, str], None]\n    ] = None,\n    query_logging_callback: Optional[Callable[[str, str], None]] = None,\n) -&gt; \"Agent\":\n    \"\"\"Create an Agent instance from a JSON string.\"\"\"\n    return cls.from_dict(\n        json.loads(data), agent_progress_callback, query_logging_callback\n    )\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.report","title":"<code>report(detailed=False)</code>","text":"<p>Get a report from the agent.</p> <p>Parameters:</p> Name Type Description Default <code>detailed</code> <code>bool</code> <p>Whether to include detailed information. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>None</code> <p>The report from the agent.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>def report(self, detailed: bool = False) -&gt; None:\n    \"\"\"\n    Get a report from the agent.\n\n    Args:\n        detailed (bool, optional): Whether to include detailed information. Defaults to False.\n\n    Returns:\n        str: The report from the agent.\n    \"\"\"\n    logger.info(\"Vectara agentic Report:\")\n    logger.info(f\"Agent Type = {self.agent_config.agent_type}\")\n    logger.info(f\"Topic = {self._topic}\")\n    logger.info(\"Tools:\")\n    for tool in self.tools:\n        if hasattr(tool, \"metadata\"):\n            if detailed:\n                logger.info(f\"- {tool.metadata.description}\")\n            else:\n                logger.info(f\"- {tool.metadata.name}\")\n        else:\n            logger.info(\"- tool without metadata\")\n    logger.info(\n        f\"Agent LLM = {get_llm(LLMRole.MAIN, config=self.agent_config).metadata.model_name}\"\n    )\n    logger.info(\n        f\"Tool LLM = {get_llm(LLMRole.TOOL, config=self.agent_config).metadata.model_name}\"\n    )\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.run","title":"<code>run(inputs, verbose=False)</code>  <code>async</code>","text":"<p>Run a workflow using the agent. workflow class must be provided in the agent constructor. Args:     inputs (Any): The inputs to the workflow.     verbose (bool, optional): Whether to print verbose output. Defaults to False. Returns:     Any: The output or context of the workflow.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>async def run(\n    self,\n    inputs: Any,\n    verbose: bool = False,\n) -&gt; Any:\n    \"\"\"\n    Run a workflow using the agent.\n    workflow class must be provided in the agent constructor.\n    Args:\n        inputs (Any): The inputs to the workflow.\n        verbose (bool, optional): Whether to print verbose output. Defaults to False.\n    Returns:\n        Any: The output or context of the workflow.\n    \"\"\"\n    # Create workflow\n    if self.workflow_cls:\n        workflow = self.workflow_cls(timeout=self.workflow_timeout, verbose=verbose)\n    else:\n        raise ValueError(\"Workflow is not defined.\")\n\n    # Validate inputs is in the form of workflow.InputsModel\n    if not isinstance(inputs, self.workflow_cls.InputsModel):\n        raise ValueError(f\"Inputs must be an instance of {workflow.InputsModel}.\")\n\n    outputs_model_on_fail_cls = getattr(\n        workflow.__class__, \"OutputModelOnFail\", None\n    )\n    if outputs_model_on_fail_cls:\n        fields_without_default = []\n        for name, field_info in outputs_model_on_fail_cls.model_fields.items():\n            if field_info.default_factory is PydanticUndefined:\n                fields_without_default.append(name)\n        if fields_without_default:\n            raise ValueError(\n                f\"Fields without default values: {fields_without_default}\"\n            )\n\n    from llama_index.core.workflow import Context\n\n    workflow_context = Context(workflow=workflow)\n    try:\n        # run workflow\n        result = await workflow.run(\n            ctx=workflow_context,\n            agent=self,\n            tools=self.tools,\n            llm=self.llm,\n            verbose=verbose,\n            inputs=inputs,\n        )\n\n        # return output in the form of workflow.OutputsModel(BaseModel)\n        try:\n            output = workflow.OutputsModel.model_validate(result)\n        except ValidationError as e:\n            raise ValueError(f\"Failed to map workflow output to model: {e}\") from e\n\n    except Exception as e:\n        _missing = object()\n        if outputs_model_on_fail_cls:\n            model_fields = outputs_model_on_fail_cls.model_fields\n            input_dict = {}\n            for key in model_fields:\n                value = await workflow_context.get(key, default=_missing)\n                if value is not _missing:\n                    input_dict[key] = value\n            output = outputs_model_on_fail_cls.model_validate(input_dict)\n        else:\n            logger.warning(\n                f\"Vectara Agentic: Workflow failed with unexpected error: {e}\"\n            )\n            raise type(e)(str(e)).with_traceback(e.__traceback__)\n\n    return output\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.stream_chat","title":"<code>stream_chat(prompt)</code>","text":"<p>Interact with the agent using a chat prompt with streaming. Args:     prompt (str): The chat prompt. Returns:     AgentStreamingResponse: The streaming response from the agent.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>def stream_chat(self, prompt: str) -&gt; AgentStreamingResponse:\n    \"\"\"\n    Interact with the agent using a chat prompt with streaming.\n    Args:\n        prompt (str): The chat prompt.\n    Returns:\n        AgentStreamingResponse: The streaming response from the agent.\n    \"\"\"\n    try:\n        _ = asyncio.get_running_loop()\n    except RuntimeError:\n        return asyncio.run(self.astream_chat(prompt))\n    raise RuntimeError(\n        \"Use `await agent.astream_chat(...)` inside an event loop (e.g. Jupyter).\"\n    )\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.to_dict","title":"<code>to_dict()</code>","text":"<p>Serialize the Agent instance to a dictionary.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Serialize the Agent instance to a dictionary.\"\"\"\n    return serialize_agent_to_dict(self)\n</code></pre>"},{"location":"api/#vectara_agentic.AgentConfig","title":"<code>AgentConfig</code>  <code>dataclass</code>","text":"<p>Centralized configuration for the Vectara Agentic utilities.</p> <p>Each field can default to either a hard-coded value or an environment variable. For example, if you have environment variables you want to fall back on, you can default to them here.</p> Source code in <code>vectara_agentic/agent_config.py</code> <pre><code>@dataclass(eq=True, frozen=True)\nclass AgentConfig:\n    \"\"\"\n    Centralized configuration for the Vectara Agentic utilities.\n\n    Each field can default to either a hard-coded value or an environment\n    variable. For example, if you have environment variables you want to\n    fall back on, you can default to them here.\n    \"\"\"\n\n    # Agent type\n    agent_type: AgentType = field(\n        default_factory=lambda: AgentType(\n            os.getenv(\"VECTARA_AGENTIC_AGENT_TYPE\", AgentType.FUNCTION_CALLING.value)\n        )\n    )\n\n    # Main LLM provider &amp; model name\n    main_llm_provider: ModelProvider = field(\n        default_factory=lambda: ModelProvider(\n            os.getenv(\"VECTARA_AGENTIC_MAIN_LLM_PROVIDER\", ModelProvider.OPENAI.value)\n        )\n    )\n\n    main_llm_model_name: str = field(\n        default_factory=lambda: os.getenv(\"VECTARA_AGENTIC_MAIN_MODEL_NAME\", \"\")\n    )\n\n    # Tool LLM provider &amp; model name\n    tool_llm_provider: ModelProvider = field(\n        default_factory=lambda: ModelProvider(\n            os.getenv(\"VECTARA_AGENTIC_TOOL_LLM_PROVIDER\", ModelProvider.OPENAI.value)\n        )\n    )\n    tool_llm_model_name: str = field(\n        default_factory=lambda: os.getenv(\"VECTARA_AGENTIC_TOOL_MODEL_NAME\", \"\")\n    )\n\n    # Params for Private LLM endpoint if used\n    private_llm_api_base: str = field(\n        default_factory=lambda: os.getenv(\n            \"VECTARA_AGENTIC_PRIVATE_LLM_API_BASE\",\n            \"http://private-endpoint.company.com:5000/v1\",\n        )\n    )\n    private_llm_api_key: str = field(\n        default_factory=lambda: os.getenv(\n            \"VECTARA_AGENTIC_PRIVATE_LLM_API_KEY\", \"&lt;private-api-key&gt;\"\n        )\n    )\n\n    # Observer\n    observer: ObserverType = field(\n        default_factory=lambda: ObserverType(\n            os.getenv(\"VECTARA_AGENTIC_OBSERVER_TYPE\", \"NO_OBSERVER\")\n        )\n    )\n\n    # Endpoint API key\n    endpoint_api_key: str = field(\n        default_factory=lambda: os.getenv(\"VECTARA_AGENTIC_API_KEY\", \"dev-api-key\")\n    )\n\n    def __post_init__(self):\n        # Use object.__setattr__ since the dataclass is frozen\n        if isinstance(self.agent_type, str):\n            object.__setattr__(self, \"agent_type\", AgentType(self.agent_type))\n        if isinstance(self.main_llm_provider, str):\n            object.__setattr__(\n                self, \"main_llm_provider\", ModelProvider(self.main_llm_provider)\n            )\n        if isinstance(self.tool_llm_provider, str):\n            object.__setattr__(\n                self, \"tool_llm_provider\", ModelProvider(self.tool_llm_provider)\n            )\n        if isinstance(self.observer, str):\n            object.__setattr__(self, \"observer\", ObserverType(self.observer))\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"\n        Convert the AgentConfig to a dictionary.\n        \"\"\"\n        return {\n            \"agent_type\": self.agent_type.value,\n            \"main_llm_provider\": self.main_llm_provider.value,\n            \"main_llm_model_name\": self.main_llm_model_name,\n            \"tool_llm_provider\": self.tool_llm_provider.value,\n            \"tool_llm_model_name\": self.tool_llm_model_name,\n            \"observer\": self.observer.value,\n            \"endpoint_api_key\": self.endpoint_api_key,\n        }\n\n    @classmethod\n    def from_dict(cls, config_dict: dict) -&gt; \"AgentConfig\":\n        \"\"\"\n        Create an AgentConfig from a dictionary.\n        \"\"\"\n        return cls(\n            agent_type=AgentType(config_dict[\"agent_type\"]),\n            main_llm_provider=ModelProvider(config_dict[\"main_llm_provider\"]),\n            main_llm_model_name=config_dict[\"main_llm_model_name\"],\n            tool_llm_provider=ModelProvider(config_dict[\"tool_llm_provider\"]),\n            tool_llm_model_name=config_dict[\"tool_llm_model_name\"],\n            observer=ObserverType(config_dict[\"observer\"]),\n            endpoint_api_key=config_dict[\"endpoint_api_key\"],\n        )\n</code></pre>"},{"location":"api/#vectara_agentic.AgentConfig.from_dict","title":"<code>from_dict(config_dict)</code>  <code>classmethod</code>","text":"<p>Create an AgentConfig from a dictionary.</p> Source code in <code>vectara_agentic/agent_config.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: dict) -&gt; \"AgentConfig\":\n    \"\"\"\n    Create an AgentConfig from a dictionary.\n    \"\"\"\n    return cls(\n        agent_type=AgentType(config_dict[\"agent_type\"]),\n        main_llm_provider=ModelProvider(config_dict[\"main_llm_provider\"]),\n        main_llm_model_name=config_dict[\"main_llm_model_name\"],\n        tool_llm_provider=ModelProvider(config_dict[\"tool_llm_provider\"]),\n        tool_llm_model_name=config_dict[\"tool_llm_model_name\"],\n        observer=ObserverType(config_dict[\"observer\"]),\n        endpoint_api_key=config_dict[\"endpoint_api_key\"],\n    )\n</code></pre>"},{"location":"api/#vectara_agentic.AgentConfig.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert the AgentConfig to a dictionary.</p> Source code in <code>vectara_agentic/agent_config.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"\n    Convert the AgentConfig to a dictionary.\n    \"\"\"\n    return {\n        \"agent_type\": self.agent_type.value,\n        \"main_llm_provider\": self.main_llm_provider.value,\n        \"main_llm_model_name\": self.main_llm_model_name,\n        \"tool_llm_provider\": self.tool_llm_provider.value,\n        \"tool_llm_model_name\": self.tool_llm_model_name,\n        \"observer\": self.observer.value,\n        \"endpoint_api_key\": self.endpoint_api_key,\n    }\n</code></pre>"},{"location":"api/#vectara_agentic.AgentStatusType","title":"<code>AgentStatusType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration for different types of agent statuses.</p> Source code in <code>vectara_agentic/types.py</code> <pre><code>class AgentStatusType(Enum):\n    \"\"\"Enumeration for different types of agent statuses.\"\"\"\n\n    AGENT_UPDATE = \"agent_update\"\n    TOOL_CALL = \"tool_call\"\n    TOOL_OUTPUT = \"tool_output\"\n    AGENT_STEP = \"agent_step\"\n</code></pre>"},{"location":"api/#vectara_agentic.AgentType","title":"<code>AgentType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration for different types of agents.</p> Source code in <code>vectara_agentic/types.py</code> <pre><code>class AgentType(Enum):\n    \"\"\"Enumeration for different types of agents.\"\"\"\n\n    REACT = \"REACT\"\n    FUNCTION_CALLING = \"FUNCTION_CALLING\"\n</code></pre>"},{"location":"api/#vectara_agentic.LLMRole","title":"<code>LLMRole</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration for different types of LLM roles.</p> Source code in <code>vectara_agentic/types.py</code> <pre><code>class LLMRole(Enum):\n    \"\"\"Enumeration for different types of LLM roles.\"\"\"\n\n    MAIN = \"MAIN\"\n    TOOL = \"TOOL\"\n</code></pre>"},{"location":"api/#vectara_agentic.ModelProvider","title":"<code>ModelProvider</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration for different types of model providers.</p> Source code in <code>vectara_agentic/types.py</code> <pre><code>class ModelProvider(Enum):\n    \"\"\"Enumeration for different types of model providers.\"\"\"\n\n    OPENAI = \"OPENAI\"\n    ANTHROPIC = \"ANTHROPIC\"\n    TOGETHER = \"TOGETHER\"\n    GROQ = \"GROQ\"\n    COHERE = \"COHERE\"\n    GEMINI = \"GEMINI\"\n    BEDROCK = \"BEDROCK\"\n    PRIVATE = \"PRIVATE\"\n</code></pre>"},{"location":"api/#vectara_agentic.ObserverType","title":"<code>ObserverType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration for different types of observability integrations.</p> Source code in <code>vectara_agentic/types.py</code> <pre><code>class ObserverType(Enum):\n    \"\"\"Enumeration for different types of observability integrations.\"\"\"\n\n    NO_OBSERVER = \"NO_OBSERVER\"\n    ARIZE_PHOENIX = \"ARIZE_PHOENIX\"\n</code></pre>"},{"location":"api/#vectara_agentic.ToolType","title":"<code>ToolType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration for different types of tools.</p> Source code in <code>vectara_agentic/types.py</code> <pre><code>class ToolType(Enum):\n    \"\"\"Enumeration for different types of tools.\"\"\"\n\n    QUERY = \"query\"\n    ACTION = \"action\"\n</code></pre>"},{"location":"api/#vectara_agentic.ToolsCatalog","title":"<code>ToolsCatalog</code>","text":"<p>A curated set of tools for vectara-agentic</p> Source code in <code>vectara_agentic/tools_catalog.py</code> <pre><code>class ToolsCatalog:\n    \"\"\"\n    A curated set of tools for vectara-agentic\n    \"\"\"\n\n    def __init__(self, agent_config: AgentConfig):\n        self.agent_config = agent_config\n\n    @remove_self_from_signature\n    def summarize_text(\n        self,\n        text: str = Field(description=\"the original text.\"),\n        expertise: str = Field(\n            description=\"the expertise to apply to the summarization.\",\n        ),\n    ) -&gt; str:\n        \"\"\"\n        This is a helper tool.\n        Use this tool to summarize text using a given expertise\n        with no more than summary_max_length characters.\n\n        Args:\n            text (str): The original text.\n            expertise (str): The expertise to apply to the summarization.\n\n        Returns:\n            str: The summarized text.\n        \"\"\"\n        if not isinstance(expertise, str):\n            return \"Please provide a valid string for expertise.\"\n        if not isinstance(text, str):\n            return \"Please provide a valid string for text.\"\n        expertise = \"general\" if len(expertise) &lt; 3 else expertise.lower()\n        prompt = (\n            f\"As an expert in {expertise}, summarize the provided text \"\n            \"into a concise summary.\\n\"\n            f\"Original text: {text}\\nSummary:\"\n        )\n        llm = get_llm(LLMRole.TOOL, config=self.agent_config)\n        response = llm.complete(prompt)\n        return response.text\n\n    @remove_self_from_signature\n    def rephrase_text(\n        self,\n        text: str = Field(description=\"the original text.\"),\n        instructions: str = Field(\n            description=\"the specific instructions for how to rephrase the text.\"\n        ),\n    ) -&gt; str:\n        \"\"\"\n        This is a helper tool.\n        Use this tool to rephrase the text according to the provided instructions.\n        For example, instructions could be \"as a 5 year old would say it.\"\n\n        Args:\n            text (str): The original text.\n            instructions (str): The specific instructions for how to rephrase the text.\n\n        Returns:\n            str: The rephrased text.\n        \"\"\"\n        prompt = (\n            f\"Rephrase the provided text according to the following instructions: {instructions}.\\n\"\n            \"If the input is Markdown, keep the output in Markdown as well.\\n\"\n            f\"Original text: {text}\\nRephrased text:\"\n        )\n        llm = get_llm(LLMRole.TOOL, config=self.agent_config)\n        response = llm.complete(prompt)\n        return response.text\n\n    @remove_self_from_signature\n    def critique_text(\n        self,\n        text: str = Field(description=\"the original text.\"),\n        role: str = Field(\n            default=None, description=\"the role of the person providing critique.\"\n        ),\n        point_of_view: str = Field(\n            default=None,\n            description=\"the point of view with which to provide critique.\",\n        ),\n    ) -&gt; str:\n        \"\"\"\n        This is a helper tool.\n        Critique the text from the specified point of view.\n\n        Args:\n            text (str): The original text.\n            role (str): The role of the person providing critique.\n            point_of_view (str): The point of view with which to provide critique.\n\n        Returns:\n            str: The critique of the text.\n        \"\"\"\n        if role:\n            prompt = f\"As a {role}, critique the provided text from the point of view of {point_of_view}.\"\n        else:\n            prompt = (\n                f\"Critique the provided text from the point of view of {point_of_view}.\"\n            )\n        prompt += \"\\nStructure the critique as bullet points.\\n\"\n        prompt += f\"Original text: {text}\\nCritique:\"\n        llm = get_llm(LLMRole.TOOL, config=self.agent_config)\n        response = llm.complete(prompt)\n        return response.text\n</code></pre>"},{"location":"api/#vectara_agentic.ToolsCatalog.critique_text","title":"<code>critique_text(text=Field(description='the original text.'), role=Field(default=None, description='the role of the person providing critique.'), point_of_view=Field(default=None, description='the point of view with which to provide critique.'))</code>","text":"<p>This is a helper tool. Critique the text from the specified point of view.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The original text.</p> <code>Field(description='the original text.')</code> <code>role</code> <code>str</code> <p>The role of the person providing critique.</p> <code>Field(default=None, description='the role of the person providing critique.')</code> <code>point_of_view</code> <code>str</code> <p>The point of view with which to provide critique.</p> <code>Field(default=None, description='the point of view with which to provide critique.')</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The critique of the text.</p> Source code in <code>vectara_agentic/tools_catalog.py</code> <pre><code>@remove_self_from_signature\ndef critique_text(\n    self,\n    text: str = Field(description=\"the original text.\"),\n    role: str = Field(\n        default=None, description=\"the role of the person providing critique.\"\n    ),\n    point_of_view: str = Field(\n        default=None,\n        description=\"the point of view with which to provide critique.\",\n    ),\n) -&gt; str:\n    \"\"\"\n    This is a helper tool.\n    Critique the text from the specified point of view.\n\n    Args:\n        text (str): The original text.\n        role (str): The role of the person providing critique.\n        point_of_view (str): The point of view with which to provide critique.\n\n    Returns:\n        str: The critique of the text.\n    \"\"\"\n    if role:\n        prompt = f\"As a {role}, critique the provided text from the point of view of {point_of_view}.\"\n    else:\n        prompt = (\n            f\"Critique the provided text from the point of view of {point_of_view}.\"\n        )\n    prompt += \"\\nStructure the critique as bullet points.\\n\"\n    prompt += f\"Original text: {text}\\nCritique:\"\n    llm = get_llm(LLMRole.TOOL, config=self.agent_config)\n    response = llm.complete(prompt)\n    return response.text\n</code></pre>"},{"location":"api/#vectara_agentic.ToolsCatalog.rephrase_text","title":"<code>rephrase_text(text=Field(description='the original text.'), instructions=Field(description='the specific instructions for how to rephrase the text.'))</code>","text":"<p>This is a helper tool. Use this tool to rephrase the text according to the provided instructions. For example, instructions could be \"as a 5 year old would say it.\"</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The original text.</p> <code>Field(description='the original text.')</code> <code>instructions</code> <code>str</code> <p>The specific instructions for how to rephrase the text.</p> <code>Field(description='the specific instructions for how to rephrase the text.')</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The rephrased text.</p> Source code in <code>vectara_agentic/tools_catalog.py</code> <pre><code>@remove_self_from_signature\ndef rephrase_text(\n    self,\n    text: str = Field(description=\"the original text.\"),\n    instructions: str = Field(\n        description=\"the specific instructions for how to rephrase the text.\"\n    ),\n) -&gt; str:\n    \"\"\"\n    This is a helper tool.\n    Use this tool to rephrase the text according to the provided instructions.\n    For example, instructions could be \"as a 5 year old would say it.\"\n\n    Args:\n        text (str): The original text.\n        instructions (str): The specific instructions for how to rephrase the text.\n\n    Returns:\n        str: The rephrased text.\n    \"\"\"\n    prompt = (\n        f\"Rephrase the provided text according to the following instructions: {instructions}.\\n\"\n        \"If the input is Markdown, keep the output in Markdown as well.\\n\"\n        f\"Original text: {text}\\nRephrased text:\"\n    )\n    llm = get_llm(LLMRole.TOOL, config=self.agent_config)\n    response = llm.complete(prompt)\n    return response.text\n</code></pre>"},{"location":"api/#vectara_agentic.ToolsCatalog.summarize_text","title":"<code>summarize_text(text=Field(description='the original text.'), expertise=Field(description='the expertise to apply to the summarization.'))</code>","text":"<p>This is a helper tool. Use this tool to summarize text using a given expertise with no more than summary_max_length characters.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The original text.</p> <code>Field(description='the original text.')</code> <code>expertise</code> <code>str</code> <p>The expertise to apply to the summarization.</p> <code>Field(description='the expertise to apply to the summarization.')</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The summarized text.</p> Source code in <code>vectara_agentic/tools_catalog.py</code> <pre><code>@remove_self_from_signature\ndef summarize_text(\n    self,\n    text: str = Field(description=\"the original text.\"),\n    expertise: str = Field(\n        description=\"the expertise to apply to the summarization.\",\n    ),\n) -&gt; str:\n    \"\"\"\n    This is a helper tool.\n    Use this tool to summarize text using a given expertise\n    with no more than summary_max_length characters.\n\n    Args:\n        text (str): The original text.\n        expertise (str): The expertise to apply to the summarization.\n\n    Returns:\n        str: The summarized text.\n    \"\"\"\n    if not isinstance(expertise, str):\n        return \"Please provide a valid string for expertise.\"\n    if not isinstance(text, str):\n        return \"Please provide a valid string for text.\"\n    expertise = \"general\" if len(expertise) &lt; 3 else expertise.lower()\n    prompt = (\n        f\"As an expert in {expertise}, summarize the provided text \"\n        \"into a concise summary.\\n\"\n        f\"Original text: {text}\\nSummary:\"\n    )\n    llm = get_llm(LLMRole.TOOL, config=self.agent_config)\n    response = llm.complete(prompt)\n    return response.text\n</code></pre>"},{"location":"api/#vectara_agentic.ToolsFactory","title":"<code>ToolsFactory</code>","text":"<p>A factory class for creating agent tools.</p> Source code in <code>vectara_agentic/tools.py</code> <pre><code>class ToolsFactory:\n    \"\"\"\n    A factory class for creating agent tools.\n    \"\"\"\n\n    def __init__(self, agent_config: AgentConfig = None) -&gt; None:\n        self.agent_config = agent_config\n\n    def create_tool(\n        self,\n        function: Callable,\n        tool_type: ToolType = ToolType.QUERY,\n        vhc_eligible: bool = True,\n    ) -&gt; VectaraTool:\n        \"\"\"\n        Create a tool from a function.\n\n        Args:\n            function (Callable): a function to convert into a tool.\n            tool_type (ToolType): the type of tool.\n\n        Returns:\n            VectaraTool: A VectaraTool object.\n        \"\"\"\n        return VectaraTool.from_defaults(\n            tool_type=tool_type, fn=function, vhc_eligible=vhc_eligible,\n            description=function.__doc__,\n        )\n\n    def get_llama_index_tools(\n        self,\n        tool_package_name: str,\n        tool_spec_name: str,\n        tool_name_prefix: str = \"\",\n        **kwargs: dict,\n    ) -&gt; List[VectaraTool]:\n        \"\"\"\n        Get a tool from the llama_index hub.\n\n        Args:\n            tool_package_name (str): The name of the tool package.\n            tool_spec_name (str): The name of the tool spec.\n            tool_name_prefix (str, optional): The prefix to add to the tool names (added to every tool in the spec).\n            kwargs (dict): The keyword arguments to pass to the tool constructor (see Hub for tool specific details).\n\n        Returns:\n            List[VectaraTool]: A list of VectaraTool objects.\n        \"\"\"\n        # Dynamically install and import the module\n        if tool_package_name not in LI_packages:\n            raise ValueError(\n                f\"Tool package {tool_package_name} from LlamaIndex not supported by Vectara-agentic.\"\n            )\n\n        module_name = f\"llama_index.tools.{tool_package_name}\"\n        module = importlib.import_module(module_name)\n\n        # Get the tool spec class or function from the module\n        tool_spec = getattr(module, tool_spec_name)\n        func_type = LI_packages[tool_package_name]\n        tools = tool_spec(**kwargs).to_tool_list()\n        vtools = []\n        for tool in tools:\n            if len(tool_name_prefix) &gt; 0:\n                tool.metadata.name = tool_name_prefix + \"_\" + tool.metadata.name\n            if isinstance(func_type, dict):\n                if tool_spec_name not in func_type.keys():\n                    raise ValueError(\n                        f\"Tool spec {tool_spec_name} not found in package {tool_package_name}.\"\n                    )\n                tool_type = func_type[tool_spec_name]\n            else:\n                tool_type = func_type\n            vtool = VectaraTool(\n                tool_type=tool_type,\n                fn=tool.fn,\n                metadata=tool.metadata,\n                async_fn=tool.async_fn,\n            )\n            vtools.append(vtool)\n        return vtools\n\n    def standard_tools(self) -&gt; List[FunctionTool]:\n        \"\"\"\n        Create a list of standard tools.\n        \"\"\"\n        tc = ToolsCatalog(self.agent_config)\n        return [\n            self.create_tool(tool, vhc_eligible=True)\n            for tool in [tc.summarize_text, tc.rephrase_text, tc.critique_text]\n        ]\n\n    def guardrail_tools(self) -&gt; List[FunctionTool]:\n        \"\"\"\n        Create a list of guardrail tools to avoid controversial topics.\n        \"\"\"\n        return [self.create_tool(get_bad_topics, vhc_eligible=False)]\n\n    def financial_tools(self):\n        \"\"\"\n        Create a list of financial tools.\n        \"\"\"\n        return self.get_llama_index_tools(\n            tool_package_name=\"yahoo_finance\", tool_spec_name=\"YahooFinanceToolSpec\"\n        )\n\n    def legal_tools(self) -&gt; List[FunctionTool]:\n        \"\"\"\n        Create a list of legal tools.\n        \"\"\"\n\n        def summarize_legal_text(\n            text: str = Field(description=\"the original text.\"),\n        ) -&gt; str:\n            \"\"\"\n            Use this tool to summarize legal text with no more than summary_max_length characters.\n            \"\"\"\n            tc = ToolsCatalog(self.agent_config)\n            return tc.summarize_text(text, expertise=\"law\")\n\n        def critique_as_judge(\n            text: str = Field(description=\"the original text.\"),\n        ) -&gt; str:\n            \"\"\"\n            Critique the legal document.\n            \"\"\"\n            tc = ToolsCatalog(self.agent_config)\n            return tc.critique_text(\n                text,\n                role=\"judge\",\n                point_of_view=\"\"\"\n                an experienced judge evaluating a legal document to provide areas of concern\n                or that may require further legal scrutiny or legal argument.\n                \"\"\",\n            )\n\n        return [\n            self.create_tool(tool, vhc_eligible=False)\n            for tool in [summarize_legal_text, critique_as_judge]\n        ]\n\n    def database_tools(\n        self,\n        tool_name_prefix: str = \"\",\n        content_description: Optional[str] = None,\n        sql_database: Optional[SQLDatabase] = None,\n        scheme: Optional[str] = None,\n        host: str = \"localhost\",\n        port: str = \"5432\",\n        user: str = \"postgres\",\n        password: str = \"Password\",\n        dbname: str = \"postgres\",\n        max_rows: int = 1000,\n    ) -&gt; List[VectaraTool]:\n        \"\"\"\n        Returns a list of database tools.\n\n        Args:\n\n            tool_name_prefix (str, optional): The prefix to add to the tool names. Defaults to \"\".\n            content_description (str, optional): The content description for the database. Defaults to None.\n            sql_database (SQLDatabase, optional): The SQLDatabase object. Defaults to None.\n            scheme (str, optional): The database scheme. Defaults to None.\n            host (str, optional): The database host. Defaults to \"localhost\".\n            port (str, optional): The database port. Defaults to \"5432\".\n            user (str, optional): The database user. Defaults to \"postgres\".\n            password (str, optional): The database password. Defaults to \"Password\".\n            dbname (str, optional): The database name. Defaults to \"postgres\".\n               You must specify either the sql_database object or the scheme, host, port, user, password, and dbname.\n            max_rows (int, optional): if specified, instructs the load_data tool to never return more than max_rows\n               rows. Defaults to 1000.\n\n        Returns:\n            List[VectaraTool]: A list of VectaraTool objects.\n        \"\"\"\n        if sql_database:\n            dbt = DatabaseTools(\n                tool_name_prefix=tool_name_prefix,\n                sql_database=sql_database,\n                max_rows=max_rows,\n            )\n        else:\n            if scheme in [\"postgresql\", \"mysql\", \"sqlite\", \"mssql\", \"oracle\"]:\n                dbt = DatabaseTools(\n                    tool_name_prefix=tool_name_prefix,\n                    scheme=scheme,\n                    host=host,\n                    port=port,\n                    user=user,\n                    password=password,\n                    dbname=dbname,\n                    max_rows=max_rows,\n                )\n            else:\n                raise ValueError(\n                    \"Please provide a SqlDatabase option or a valid DB scheme type \"\n                    \" (postgresql, mysql, sqlite, mssql, oracle).\"\n                )\n\n        # Update tools with description\n        tools = dbt.to_tool_list()\n        vtools = []\n        for tool in tools:\n            if content_description:\n                tool.metadata.description = (\n                    tool.metadata.description\n                    + f\"The database tables include data about {content_description}.\"\n                )\n            vtool = VectaraTool(\n                tool_type=ToolType.QUERY,\n                fn=tool.fn,\n                async_fn=tool.async_fn,\n                metadata=tool.metadata,\n                vhc_eligible=True,\n            )\n            vtools.append(vtool)\n        return vtools\n</code></pre>"},{"location":"api/#vectara_agentic.ToolsFactory.create_tool","title":"<code>create_tool(function, tool_type=ToolType.QUERY, vhc_eligible=True)</code>","text":"<p>Create a tool from a function.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>a function to convert into a tool.</p> required <code>tool_type</code> <code>ToolType</code> <p>the type of tool.</p> <code>QUERY</code> <p>Returns:</p> Name Type Description <code>VectaraTool</code> <code>VectaraTool</code> <p>A VectaraTool object.</p> Source code in <code>vectara_agentic/tools.py</code> <pre><code>def create_tool(\n    self,\n    function: Callable,\n    tool_type: ToolType = ToolType.QUERY,\n    vhc_eligible: bool = True,\n) -&gt; VectaraTool:\n    \"\"\"\n    Create a tool from a function.\n\n    Args:\n        function (Callable): a function to convert into a tool.\n        tool_type (ToolType): the type of tool.\n\n    Returns:\n        VectaraTool: A VectaraTool object.\n    \"\"\"\n    return VectaraTool.from_defaults(\n        tool_type=tool_type, fn=function, vhc_eligible=vhc_eligible,\n        description=function.__doc__,\n    )\n</code></pre>"},{"location":"api/#vectara_agentic.ToolsFactory.database_tools","title":"<code>database_tools(tool_name_prefix='', content_description=None, sql_database=None, scheme=None, host='localhost', port='5432', user='postgres', password='Password', dbname='postgres', max_rows=1000)</code>","text":"<p>Returns a list of database tools.</p> <p>Args:</p> <pre><code>tool_name_prefix (str, optional): The prefix to add to the tool names. Defaults to \"\".\ncontent_description (str, optional): The content description for the database. Defaults to None.\nsql_database (SQLDatabase, optional): The SQLDatabase object. Defaults to None.\nscheme (str, optional): The database scheme. Defaults to None.\nhost (str, optional): The database host. Defaults to \"localhost\".\nport (str, optional): The database port. Defaults to \"5432\".\nuser (str, optional): The database user. Defaults to \"postgres\".\npassword (str, optional): The database password. Defaults to \"Password\".\ndbname (str, optional): The database name. Defaults to \"postgres\".\n   You must specify either the sql_database object or the scheme, host, port, user, password, and dbname.\nmax_rows (int, optional): if specified, instructs the load_data tool to never return more than max_rows\n   rows. Defaults to 1000.\n</code></pre> <p>Returns:</p> Type Description <code>List[VectaraTool]</code> <p>List[VectaraTool]: A list of VectaraTool objects.</p> Source code in <code>vectara_agentic/tools.py</code> <pre><code>def database_tools(\n    self,\n    tool_name_prefix: str = \"\",\n    content_description: Optional[str] = None,\n    sql_database: Optional[SQLDatabase] = None,\n    scheme: Optional[str] = None,\n    host: str = \"localhost\",\n    port: str = \"5432\",\n    user: str = \"postgres\",\n    password: str = \"Password\",\n    dbname: str = \"postgres\",\n    max_rows: int = 1000,\n) -&gt; List[VectaraTool]:\n    \"\"\"\n    Returns a list of database tools.\n\n    Args:\n\n        tool_name_prefix (str, optional): The prefix to add to the tool names. Defaults to \"\".\n        content_description (str, optional): The content description for the database. Defaults to None.\n        sql_database (SQLDatabase, optional): The SQLDatabase object. Defaults to None.\n        scheme (str, optional): The database scheme. Defaults to None.\n        host (str, optional): The database host. Defaults to \"localhost\".\n        port (str, optional): The database port. Defaults to \"5432\".\n        user (str, optional): The database user. Defaults to \"postgres\".\n        password (str, optional): The database password. Defaults to \"Password\".\n        dbname (str, optional): The database name. Defaults to \"postgres\".\n           You must specify either the sql_database object or the scheme, host, port, user, password, and dbname.\n        max_rows (int, optional): if specified, instructs the load_data tool to never return more than max_rows\n           rows. Defaults to 1000.\n\n    Returns:\n        List[VectaraTool]: A list of VectaraTool objects.\n    \"\"\"\n    if sql_database:\n        dbt = DatabaseTools(\n            tool_name_prefix=tool_name_prefix,\n            sql_database=sql_database,\n            max_rows=max_rows,\n        )\n    else:\n        if scheme in [\"postgresql\", \"mysql\", \"sqlite\", \"mssql\", \"oracle\"]:\n            dbt = DatabaseTools(\n                tool_name_prefix=tool_name_prefix,\n                scheme=scheme,\n                host=host,\n                port=port,\n                user=user,\n                password=password,\n                dbname=dbname,\n                max_rows=max_rows,\n            )\n        else:\n            raise ValueError(\n                \"Please provide a SqlDatabase option or a valid DB scheme type \"\n                \" (postgresql, mysql, sqlite, mssql, oracle).\"\n            )\n\n    # Update tools with description\n    tools = dbt.to_tool_list()\n    vtools = []\n    for tool in tools:\n        if content_description:\n            tool.metadata.description = (\n                tool.metadata.description\n                + f\"The database tables include data about {content_description}.\"\n            )\n        vtool = VectaraTool(\n            tool_type=ToolType.QUERY,\n            fn=tool.fn,\n            async_fn=tool.async_fn,\n            metadata=tool.metadata,\n            vhc_eligible=True,\n        )\n        vtools.append(vtool)\n    return vtools\n</code></pre>"},{"location":"api/#vectara_agentic.ToolsFactory.financial_tools","title":"<code>financial_tools()</code>","text":"<p>Create a list of financial tools.</p> Source code in <code>vectara_agentic/tools.py</code> <pre><code>def financial_tools(self):\n    \"\"\"\n    Create a list of financial tools.\n    \"\"\"\n    return self.get_llama_index_tools(\n        tool_package_name=\"yahoo_finance\", tool_spec_name=\"YahooFinanceToolSpec\"\n    )\n</code></pre>"},{"location":"api/#vectara_agentic.ToolsFactory.get_llama_index_tools","title":"<code>get_llama_index_tools(tool_package_name, tool_spec_name, tool_name_prefix='', **kwargs)</code>","text":"<p>Get a tool from the llama_index hub.</p> <p>Parameters:</p> Name Type Description Default <code>tool_package_name</code> <code>str</code> <p>The name of the tool package.</p> required <code>tool_spec_name</code> <code>str</code> <p>The name of the tool spec.</p> required <code>tool_name_prefix</code> <code>str</code> <p>The prefix to add to the tool names (added to every tool in the spec).</p> <code>''</code> <code>kwargs</code> <code>dict</code> <p>The keyword arguments to pass to the tool constructor (see Hub for tool specific details).</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[VectaraTool]</code> <p>List[VectaraTool]: A list of VectaraTool objects.</p> Source code in <code>vectara_agentic/tools.py</code> <pre><code>def get_llama_index_tools(\n    self,\n    tool_package_name: str,\n    tool_spec_name: str,\n    tool_name_prefix: str = \"\",\n    **kwargs: dict,\n) -&gt; List[VectaraTool]:\n    \"\"\"\n    Get a tool from the llama_index hub.\n\n    Args:\n        tool_package_name (str): The name of the tool package.\n        tool_spec_name (str): The name of the tool spec.\n        tool_name_prefix (str, optional): The prefix to add to the tool names (added to every tool in the spec).\n        kwargs (dict): The keyword arguments to pass to the tool constructor (see Hub for tool specific details).\n\n    Returns:\n        List[VectaraTool]: A list of VectaraTool objects.\n    \"\"\"\n    # Dynamically install and import the module\n    if tool_package_name not in LI_packages:\n        raise ValueError(\n            f\"Tool package {tool_package_name} from LlamaIndex not supported by Vectara-agentic.\"\n        )\n\n    module_name = f\"llama_index.tools.{tool_package_name}\"\n    module = importlib.import_module(module_name)\n\n    # Get the tool spec class or function from the module\n    tool_spec = getattr(module, tool_spec_name)\n    func_type = LI_packages[tool_package_name]\n    tools = tool_spec(**kwargs).to_tool_list()\n    vtools = []\n    for tool in tools:\n        if len(tool_name_prefix) &gt; 0:\n            tool.metadata.name = tool_name_prefix + \"_\" + tool.metadata.name\n        if isinstance(func_type, dict):\n            if tool_spec_name not in func_type.keys():\n                raise ValueError(\n                    f\"Tool spec {tool_spec_name} not found in package {tool_package_name}.\"\n                )\n            tool_type = func_type[tool_spec_name]\n        else:\n            tool_type = func_type\n        vtool = VectaraTool(\n            tool_type=tool_type,\n            fn=tool.fn,\n            metadata=tool.metadata,\n            async_fn=tool.async_fn,\n        )\n        vtools.append(vtool)\n    return vtools\n</code></pre>"},{"location":"api/#vectara_agentic.ToolsFactory.guardrail_tools","title":"<code>guardrail_tools()</code>","text":"<p>Create a list of guardrail tools to avoid controversial topics.</p> Source code in <code>vectara_agentic/tools.py</code> <pre><code>def guardrail_tools(self) -&gt; List[FunctionTool]:\n    \"\"\"\n    Create a list of guardrail tools to avoid controversial topics.\n    \"\"\"\n    return [self.create_tool(get_bad_topics, vhc_eligible=False)]\n</code></pre>"},{"location":"api/#vectara_agentic.ToolsFactory.legal_tools","title":"<code>legal_tools()</code>","text":"<p>Create a list of legal tools.</p> Source code in <code>vectara_agentic/tools.py</code> <pre><code>def legal_tools(self) -&gt; List[FunctionTool]:\n    \"\"\"\n    Create a list of legal tools.\n    \"\"\"\n\n    def summarize_legal_text(\n        text: str = Field(description=\"the original text.\"),\n    ) -&gt; str:\n        \"\"\"\n        Use this tool to summarize legal text with no more than summary_max_length characters.\n        \"\"\"\n        tc = ToolsCatalog(self.agent_config)\n        return tc.summarize_text(text, expertise=\"law\")\n\n    def critique_as_judge(\n        text: str = Field(description=\"the original text.\"),\n    ) -&gt; str:\n        \"\"\"\n        Critique the legal document.\n        \"\"\"\n        tc = ToolsCatalog(self.agent_config)\n        return tc.critique_text(\n            text,\n            role=\"judge\",\n            point_of_view=\"\"\"\n            an experienced judge evaluating a legal document to provide areas of concern\n            or that may require further legal scrutiny or legal argument.\n            \"\"\",\n        )\n\n    return [\n        self.create_tool(tool, vhc_eligible=False)\n        for tool in [summarize_legal_text, critique_as_judge]\n    ]\n</code></pre>"},{"location":"api/#vectara_agentic.ToolsFactory.standard_tools","title":"<code>standard_tools()</code>","text":"<p>Create a list of standard tools.</p> Source code in <code>vectara_agentic/tools.py</code> <pre><code>def standard_tools(self) -&gt; List[FunctionTool]:\n    \"\"\"\n    Create a list of standard tools.\n    \"\"\"\n    tc = ToolsCatalog(self.agent_config)\n    return [\n        self.create_tool(tool, vhc_eligible=True)\n        for tool in [tc.summarize_text, tc.rephrase_text, tc.critique_text]\n    ]\n</code></pre>"},{"location":"api/#vectara_agentic.VectaraTool","title":"<code>VectaraTool</code>","text":"<p>               Bases: <code>FunctionTool</code></p> <p>A subclass of FunctionTool adding the tool_type attribute.</p> Source code in <code>vectara_agentic/tool_utils.py</code> <pre><code>class VectaraTool(FunctionTool):\n    \"\"\"\n    A subclass of FunctionTool adding the tool_type attribute.\n    \"\"\"\n\n    def __init__(\n        self,\n        tool_type: ToolType,\n        metadata: ToolMetadata,\n        fn: Optional[Callable[..., Any]] = None,\n        async_fn: Optional[AsyncCallable] = None,\n        vhc_eligible: bool = True,\n    ) -&gt; None:\n        # Use Pydantic v2 compatible method for extracting metadata\n        metadata_dict = (\n            metadata.model_dump()\n            if hasattr(metadata, \"model_dump\")\n            else metadata.dict() if hasattr(metadata, \"dict\") else metadata.__dict__\n        )\n        vm = VectaraToolMetadata(\n            tool_type=tool_type, vhc_eligible=vhc_eligible, **metadata_dict\n        )\n        super().__init__(fn, vm, async_fn)\n\n    @classmethod\n    def from_defaults(\n        cls,\n        fn: Optional[Callable[..., Any]] = None,\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n        return_direct: bool = False,\n        fn_schema: Optional[Type[BaseModel]] = None,\n        async_fn: Optional[AsyncCallable] = None,\n        tool_metadata: Optional[ToolMetadata] = None,\n        callback: Optional[Callable[[Any], Any]] = None,\n        async_callback: Optional[AsyncCallable] = None,\n        partial_params: Optional[Dict[str, Any]] = None,\n        tool_type: ToolType = ToolType.QUERY,\n        vhc_eligible: bool = True,\n    ) -&gt; \"VectaraTool\":\n        tool = FunctionTool.from_defaults(\n            fn,\n            name,\n            description,\n            return_direct,\n            fn_schema,\n            async_fn,\n            tool_metadata,\n            callback,\n            async_callback,\n            partial_params,\n        )\n        vectara_tool = cls(\n            tool_type=tool_type,\n            fn=tool.fn,\n            metadata=tool.metadata,\n            async_fn=tool.async_fn,\n            vhc_eligible=vhc_eligible,\n        )\n        return vectara_tool\n\n    def __str__(self) -&gt; str:\n        return f\"Tool(name={self.metadata.name}, \" f\"Tool metadata={self.metadata})\"\n\n    def __repr__(self) -&gt; str:\n        return str(self)\n\n    def __eq__(self, other):\n        try:\n            # Try to get schema as dict if possible\n            self_schema = self.metadata.fn_schema.model_json_schema()\n            other_schema = other.metadata.fn_schema.model_json_schema()\n        except Exception:\n            return False\n\n        is_equal = (\n            isinstance(other, VectaraTool)\n            and self.metadata.tool_type == other.metadata.tool_type\n            and self.metadata.name == other.metadata.name\n            and self_schema == other_schema\n        )\n        return is_equal\n\n    def _create_tool_error_output(\n        self, error: Exception, args: Any, kwargs: Any, include_traceback: bool = False\n    ) -&gt; ToolOutput:\n        \"\"\"Create standardized error output for tool execution failures.\"\"\"\n        if isinstance(error, TypeError):\n            # Parameter validation error handling\n            sig = inspect.signature(self.metadata.fn_schema)\n            valid_parameters = list(sig.parameters.keys())\n            params_str = \", \".join(valid_parameters)\n            return ToolOutput(\n                tool_name=self.metadata.name,\n                content=(\n                    f\"Wrong argument used when calling {self.metadata.name}: {str(error)}. \"\n                    f\"Valid arguments: {params_str}. Please call the tool again with the correct arguments.\"\n                ),\n                raw_input={\"args\": args, \"kwargs\": kwargs},\n                raw_output={\"response\": str(error)},\n            )\n        else:\n            # General execution error handling\n            content = f\"Tool {self.metadata.name} Malfunction: {str(error)}\"\n            if include_traceback:\n                content += f\", traceback: {traceback.format_exc()}\"\n\n            return ToolOutput(\n                tool_name=self.metadata.name,\n                content=content,\n                raw_input={\"args\": args, \"kwargs\": kwargs},\n                raw_output={\"response\": str(error)},\n            )\n\n    def call(\n        self, *args: Any, ctx: Optional[Context] = None, **kwargs: Any\n    ) -&gt; ToolOutput:\n        try:\n            # Only pass ctx if it's not None to avoid passing unwanted kwargs to the function\n            if ctx is not None:\n                result = super().call(*args, ctx=ctx, **kwargs)\n            else:\n                result = super().call(*args, **kwargs)\n            return self._format_tool_output(result)\n        except Exception as e:\n            return self._create_tool_error_output(e, args, kwargs)\n\n    async def acall(\n        self, *args: Any, ctx: Optional[Context] = None, **kwargs: Any\n    ) -&gt; ToolOutput:\n        try:\n            # Only pass ctx if it's not None to avoid passing unwanted kwargs to the function\n            if ctx is not None:\n                result = await super().acall(*args, ctx=ctx, **kwargs)\n            else:\n                result = await super().acall(*args, **kwargs)\n            return self._format_tool_output(result)\n        except Exception as e:\n            return self._create_tool_error_output(\n                e, args, kwargs, include_traceback=True\n            )\n\n    def _format_tool_output(self, result: ToolOutput) -&gt; ToolOutput:\n        \"\"\"Format tool output by converting human-readable wrappers to formatted content immediately.\"\"\"\n        import logging\n\n        # If the raw_output has human-readable formatting, use it for the content\n        if hasattr(result, \"raw_output\") and _is_human_readable_output(\n            result.raw_output\n        ):\n            try:\n                formatted_content = result.raw_output.to_human_readable()\n                # Replace the content with the formatted version\n                result.content = formatted_content\n            except Exception as e:\n                logging.warning(\n                    f\"{self.metadata.name}: Failed to convert to human-readable: {e}\"\n                )\n\n        return result\n</code></pre>"},{"location":"api/#vectara_agentic.VectaraToolFactory","title":"<code>VectaraToolFactory</code>","text":"<p>A factory class for creating Vectara RAG tools.</p> Source code in <code>vectara_agentic/tools.py</code> <pre><code>class VectaraToolFactory:\n    \"\"\"\n    A factory class for creating Vectara RAG tools.\n    \"\"\"\n\n    def __init__(\n        self,\n        vectara_corpus_key: str = str(os.environ.get(\"VECTARA_CORPUS_KEY\", \"\")),\n        vectara_api_key: str = str(os.environ.get(\"VECTARA_API_KEY\", \"\")),\n    ) -&gt; None:\n        \"\"\"\n        Initialize the VectaraToolFactory\n        Args:\n            vectara_corpus_key (str): The Vectara corpus key (or comma separated list of keys).\n            vectara_api_key (str): The Vectara API key.\n        \"\"\"\n        self.vectara_corpus_key = vectara_corpus_key\n        self.vectara_api_key = vectara_api_key\n        self.num_corpora = len(vectara_corpus_key.split(\",\"))\n\n    def create_search_tool(\n        self,\n        tool_name: str,\n        tool_description: str,\n        tool_args_schema: type[BaseModel] = None,\n        tool_args_type: Dict[str, str] = {},\n        summarize_docs: Optional[bool] = None,\n        summarize_llm_name: Optional[str] = None,\n        fixed_filter: str = \"\",\n        lambda_val: Union[List[float], float] = 0.005,\n        semantics: Union[List[str] | str] = \"default\",\n        custom_dimensions: Union[List[Dict], Dict] = {},\n        offset: int = 0,\n        n_sentences_before: int = 2,\n        n_sentences_after: int = 2,\n        reranker: str = \"slingshot\",\n        rerank_k: int = 50,\n        rerank_limit: Optional[int] = None,\n        rerank_cutoff: Optional[float] = None,\n        mmr_diversity_bias: float = 0.2,\n        udf_expression: str = None,\n        rerank_chain: List[Dict] = None,\n        return_direct: bool = False,\n        save_history: bool = True,\n        verbose: bool = False,\n        vectara_base_url: str = \"https://api.vectara.io\",\n        vectara_verify_ssl: bool = True,\n        vhc_eligible: bool = True,\n    ) -&gt; VectaraTool:\n        \"\"\"\n        Creates a Vectara search/retrieval tool\n\n        Args:\n            tool_name (str): The name of the tool.\n            tool_description (str): The description of the tool.\n            tool_args_schema (BaseModel, optional): The schema for the tool arguments.\n            tool_args_type (Dict[str, dict], optional): attributes for each argument where they key is the field name\n                and the value is a dictionary with the following keys:\n                - 'type': the type of each filter attribute in Vectara (doc or part).\n                - 'is_list': whether the filterable attribute is a list.\n                - 'filter_name': the name of the filterable attribute in Vectara.\n            summarize_docs (bool, optional): Whether to summarize the retrieved documents.\n            summarize_llm_name (str, optional): The name of the LLM to use for summarization.\n            fixed_filter (str, optional): A fixed Vectara filter condition to apply to all queries.\n            lambda_val (Union[List[float] | float], optional): Lambda value (or list of values for each corpora)\n                for the Vectara query, when using hybrid search.\n            semantics (Union[List[str], str], optional): Indicates whether the query is intended as a query or response.\n                Include list if using multiple corpora specifying the query type for each corpus.\n            custom_dimensions (Union[List[Dict] | Dict], optional): Custom dimensions for the query (for each corpora).\n            offset (int, optional): Number of results to skip.\n            n_sentences_before (int, optional): Number of sentences before the matching document part.\n            n_sentences_after (int, optional): Number of sentences after the matching document part.\n            reranker (str, optional): The reranker mode.\n            rerank_k (int, optional): Number of top-k documents for reranking.\n            rerank_limit (int, optional): Maximum number of results to return after reranking.\n            rerank_cutoff (float, optional): Minimum score threshold for results to include after reranking.\n            mmr_diversity_bias (float, optional): MMR diversity bias.\n            udf_expression (str, optional): the user defined expression for reranking results.\n            rerank_chain (List[Dict], optional): A list of rerankers to be applied sequentially.\n                Each dictionary should specify the \"type\" of reranker (mmr, slingshot, udf)\n                and any other parameters (e.g. \"limit\" or \"cutoff\" for any type,\n                \"diversity_bias\" for mmr, and \"user_function\" for udf).\n                If using slingshot/multilingual_reranker_v1, it must be first in the list.\n            save_history (bool, optional): Whether to save the query in history.\n            return_direct (bool, optional): Whether the agent should return the tool's response directly.\n            verbose (bool, optional): Whether to print verbose output.\n            vectara_base_url (str, optional): The base URL for the Vectara API.\n            vectara_verify_ssl (bool, optional): Whether to verify SSL certificates for the Vectara API.\n\n        Returns:\n            VectaraTool: A VectaraTool object.\n        \"\"\"\n\n        vectara = VectaraIndex(\n            vectara_api_key=self.vectara_api_key,\n            vectara_corpus_key=self.vectara_corpus_key,\n            x_source_str=\"vectara-agentic\",\n            vectara_base_url=vectara_base_url,\n            vectara_verify_ssl=vectara_verify_ssl,\n        )\n        vectara.vectara_api_timeout = 10\n\n        # Dynamically generate the search function\n        def search_function(*args: Any, **kwargs: Any) -&gt; list[dict]:\n            \"\"\"\n            Dynamically generated function for semantic search Vectara.\n            \"\"\"\n            # Convert args to kwargs using the function signature\n            sig = inspect.signature(search_function)\n            bound_args = sig.bind_partial(*args, **kwargs)\n            bound_args.apply_defaults()\n            kwargs = bound_args.arguments\n\n            query = kwargs.pop(\"query\")\n            top_k = kwargs.pop(\"top_k\", 10)\n            summarize = (\n                kwargs.pop(\"summarize\", True)\n                if summarize_docs is None\n                else summarize_docs\n            )\n            try:\n                filter_string = build_filter_string(\n                    kwargs, tool_args_type, fixed_filter\n                )\n            except ValueError as e:\n                msg = (\n                    f\"Building filter string failed in search tool due to invalid input or configuration ({e}). \"\n                    \"Please verify the input arguments and ensure they meet the expected format or conditions.\"\n                )\n                return [{\"text\": msg, \"metadata\": {\"args\": args, \"kwargs\": kwargs}}]\n\n            vectara_retriever = vectara.as_retriever(\n                summary_enabled=False,\n                similarity_top_k=top_k,\n                reranker=reranker,\n                rerank_k=(\n                    rerank_k\n                    if rerank_k * self.num_corpora &lt;= 100\n                    else int(100 / self.num_corpora)\n                ),\n                rerank_limit=rerank_limit,\n                rerank_cutoff=rerank_cutoff,\n                mmr_diversity_bias=mmr_diversity_bias,\n                udf_expression=udf_expression,\n                rerank_chain=rerank_chain,\n                lambda_val=lambda_val,\n                semantics=semantics,\n                custom_dimensions=custom_dimensions,\n                offset=offset,\n                filter=filter_string,\n                n_sentences_before=n_sentences_before,\n                n_sentences_after=n_sentences_after,\n                save_history=save_history,\n                x_source_str=\"vectara-agentic\",\n                verbose=verbose,\n            )\n            response = _retrieve_with_retry(vectara_retriever, query)\n\n            if len(response) == 0:\n                msg = \"Vectara Tool failed to retrieve any results for the query.\"\n                return [{\"text\": msg, \"metadata\": {\"args\": args, \"kwargs\": kwargs}}]\n            unique_ids = set()\n            docs = []\n            doc_matches = {}\n            for doc in response:\n                if doc.id_ in unique_ids:\n                    doc_matches[doc.id_].append(doc.node.get_content())\n                    continue\n                unique_ids.add(doc.id_)\n                doc_matches[doc.id_] = [doc.node.get_content()]\n                docs.append((doc.id_, doc.metadata))\n\n            res = []\n            if summarize:\n                summaries_dict = asyncio.run(\n                    summarize_documents(\n                        corpus_key=self.vectara_corpus_key,\n                        api_key=self.vectara_api_key,\n                        llm_name=summarize_llm_name,\n                        doc_ids=list(unique_ids),\n                    )\n                )\n            else:\n                summaries_dict = {}\n\n            for doc_id, metadata in docs:\n                res.append(\n                    {\n                        \"text\": summaries_dict.get(doc_id, \"\") if summarize else \"\",\n                        \"metadata\": {\n                            \"document_id\": doc_id,\n                            \"metadata\": metadata,\n                            \"matching_text\": doc_matches[doc_id],\n                        },\n                    }\n                )\n\n            # Create human-readable output using sequential format\n            def format_search_results(results):\n                if not results:\n                    return \"No search results found\"\n\n                # Create a sequential view for human reading\n                formatted_results = []\n                for i, result in enumerate(results, 1):\n                    result_str = f\"**Result #{i}**\\n\"\n                    result_str += f\"Document ID: {result['metadata']['document_id']}\\n\"\n                    if summarize and result[\"text\"]:\n                        result_str += f\"Summary: {result['text']}\\n\"\n\n                    # Add all matching text if available\n                    matches = result[\"metadata\"][\"matching_text\"]\n                    if matches:\n                        result_str += \"\".join(\n                            f\"Match #{inx} Text: {match}\\n\"\n                            for inx, match in enumerate(matches, 1)\n                        )\n                    formatted_results.append(result_str)\n                return \"\\n\".join(formatted_results)\n\n            return create_human_readable_output(res, format_search_results)\n\n        class SearchToolBaseParams(BaseModel):\n            \"\"\"Model for the base parameters of the search tool.\"\"\"\n\n            query: str = Field(\n                ...,\n                description=\"The search query to perform, in the form of a question.\",\n            )\n            top_k: int = Field(\n                default=10, description=\"The number of top documents to retrieve.\"\n            )\n            summarize: bool = Field(\n                True,\n                description=\"Whether to summarize the retrieved documents.\",\n            )\n\n        class SearchToolBaseParamsWithoutSummarize(BaseModel):\n            \"\"\"Model for the base parameters of the search tool.\"\"\"\n\n            query: str = Field(\n                ...,\n                description=\"The search query to perform, in the form of a question.\",\n            )\n            top_k: int = Field(\n                default=10, description=\"The number of top documents to retrieve.\"\n            )\n\n        search_tool_extra_desc = (\n            tool_description\n            + \"\\n\"\n            + \"Use this tool to search for relevant documents, not to ask questions.\"\n        )\n\n        tool = create_tool_from_dynamic_function(\n            search_function,\n            tool_name,\n            search_tool_extra_desc,\n            (\n                SearchToolBaseParams\n                if summarize_docs is None\n                else SearchToolBaseParamsWithoutSummarize\n            ),\n            tool_args_schema,\n            return_direct=return_direct,\n            vhc_eligible=vhc_eligible,\n        )\n        return tool\n\n    def create_rag_tool(\n        self,\n        tool_name: str,\n        tool_description: str,\n        tool_args_schema: type[BaseModel] = None,\n        tool_args_type: Dict[str, dict] = {},\n        fixed_filter: str = \"\",\n        vectara_summarizer: str = \"vectara-summary-ext-24-05-med-omni\",\n        vectara_prompt_text: str = None,\n        summary_num_results: int = 5,\n        summary_response_lang: str = \"eng\",\n        n_sentences_before: int = 2,\n        n_sentences_after: int = 2,\n        offset: int = 0,\n        lambda_val: Union[List[float], float] = 0.005,\n        semantics: Union[List[str] | str] = \"default\",\n        custom_dimensions: Union[List[Dict], Dict] = {},\n        reranker: str = \"slingshot\",\n        rerank_k: int = 50,\n        rerank_limit: Optional[int] = None,\n        rerank_cutoff: Optional[float] = None,\n        mmr_diversity_bias: float = 0.2,\n        udf_expression: str = None,\n        rerank_chain: List[Dict] = None,\n        max_response_chars: Optional[int] = None,\n        max_tokens: Optional[int] = None,\n        llm_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        include_citations: bool = True,\n        citation_pattern: str = None,\n        citation_url_pattern: str = \"{doc.url}\",\n        citation_text_pattern: str = \"{doc.title}\",\n        save_history: bool = False,\n        fcs_threshold: float = 0.0,\n        return_direct: bool = False,\n        return_human_readable_output: bool = False,\n        verbose: bool = False,\n        vectara_base_url: str = \"https://api.vectara.io\",\n        vectara_verify_ssl: bool = True,\n        vhc_eligible: bool = True,\n    ) -&gt; VectaraTool:\n        \"\"\"\n        Creates a RAG (Retrieve and Generate) tool.\n\n        Args:\n            tool_name (str): The name of the tool.\n            tool_description (str): The description of the tool.\n            tool_args_schema (BaseModel, optional): The schema for any tool arguments for filtering.\n            tool_args_type (Dict[str, dict], optional): attributes for each argument where they key is the field name\n                and the value is a dictionary with the following keys:\n                - 'type': the type of each filter attribute in Vectara (doc or part).\n                - 'is_list': whether the filterable attribute is a list.\n                - 'filter_name': the name of the filterable attribute in Vectara.\n            fixed_filter (str, optional): A fixed Vectara filter condition to apply to all queries.\n            vectara_summarizer (str, optional): The Vectara summarizer to use.\n            vectara_prompt_text (str, optional): The prompt text for the Vectara summarizer.\n            summary_num_results (int, optional): The number of summary results.\n            summary_response_lang (str, optional): The response language for the summary.\n            n_sentences_before (int, optional): Number of sentences before the summary.\n            n_sentences_after (int, optional): Number of sentences after the summary.\n            offset (int, optional): Number of results to skip.\n            lambda_val (Union[List[float] | float], optional): Lambda value (or list of values for each corpora)\n                for the Vectara query, when using hybrid search.\n            semantics (Union[List[str], str], optional): Indicates whether the query is intended as a query or response.\n                Include list if using multiple corpora specifying the query type for each corpus.\n            custom_dimensions (Union[List[Dict] | Dict], optional): Custom dimensions for the query (for each corpora).\n            reranker (str, optional): The reranker mode.\n            rerank_k (int, optional): Number of top-k documents for reranking.\n            rerank_limit (int, optional): Maximum number of results to return after reranking.\n            rerank_cutoff (float, optional): Minimum score threshold for results to include after reranking.\n            mmr_diversity_bias (float, optional): MMR diversity bias.\n            udf_expression (str, optional): The user defined expression for reranking results.\n            rerank_chain (List[Dict], optional): A list of rerankers to be applied sequentially.\n                Each dictionary should specify the \"type\" of reranker (mmr, slingshot, udf)\n                and any other parameters (e.g. \"limit\" or \"cutoff\" for any type,\n                \"diversity_bias\" for mmr, and \"user_function\" for udf).\n                If using slingshot/multilingual_reranker_v1, it must be first in the list.\n            max_response_chars (int, optional): The desired maximum number of characters for the generated summary.\n            max_tokens (int, optional): The maximum number of tokens to be returned by the LLM.\n            llm_name (str, optional): The name of the LLM to use for generation.\n            temperature (float, optional): The sampling temperature; higher values lead to more randomness.\n            frequency_penalty (float, optional): How much to penalize repeating tokens in the response,\n                higher values reducing likelihood of repeating the same line.\n            presence_penalty (float, optional): How much to penalize repeating tokens in the response,\n                higher values increasing the diversity of topics.\n            include_citations (bool, optional): Whether to include citations in the response.\n                If True, uses markdown vectara citations that requires the Vectara scale plan.\n            citation_url_pattern (str, optional): The pattern for the citations in the response.\n                Default is \"{doc.url}\" which uses the document URL.\n                If include_citations is False, this parameter is ignored.\n                citation_pattern (str, optional): old name for citation_url_pattern. Deprecated.\n            citation_text_pattern (str, optional): The text pattern for citations in the response.\n                Default is \"{doc.title}\" which uses the title of the document.\n                If include_citations is False, this parameter is ignored.\n            save_history (bool, optional): Whether to save the query in history.\n            fcs_threshold (float, optional): A threshold for factual consistency.\n                If set above 0, the tool notifies the calling agent that it \"cannot respond\" if FCS is too low.\n            return_direct (bool, optional): Whether the agent should return the tool's response directly.\n            return_human_readable_output (bool, optional): Whether to return the output in a human-readable format.\n            verbose (bool, optional): Whether to print verbose output.\n            vectara_base_url (str, optional): The base URL for the Vectara API.\n            vectara_verify_ssl (bool, optional): Whether to verify SSL certificates for the Vectara API.\n\n        Returns:\n            VectaraTool: A VectaraTool object.\n        \"\"\"\n\n        vectara = VectaraIndex(\n            vectara_api_key=self.vectara_api_key,\n            vectara_corpus_key=self.vectara_corpus_key,\n            x_source_str=\"vectara-agentic\",\n            vectara_base_url=vectara_base_url,\n            vectara_verify_ssl=vectara_verify_ssl,\n        )\n        vectara.vectara_api_timeout = 60\n        keys_to_ignore = [\"lang\", \"offset\", \"len\"]\n\n        # Dynamically generate the RAG function\n        def rag_function(*args: Any, **kwargs: Any) -&gt; dict:\n            \"\"\"\n            Dynamically generated function for RAG query with Vectara.\n            \"\"\"\n            # Convert args to kwargs using the function signature\n            sig = inspect.signature(rag_function)\n            bound_args = sig.bind_partial(*args, **kwargs)\n            bound_args.apply_defaults()\n            kwargs = bound_args.arguments\n\n            query = kwargs.pop(\"query\")\n            try:\n                filter_string = build_filter_string(\n                    kwargs, tool_args_type, fixed_filter\n                )\n            except ValueError as e:\n                msg = (\n                    f\"Building filter string failed in rag tool. \"\n                    f\"Reason: {e}. Ensure that the input arguments match the expected \"\n                    f\"format and include all required fields. \"\n                )\n                return {\"text\": msg, \"metadata\": {\"args\": args, \"kwargs\": kwargs}}\n\n            computed_citations_url_pattern = (\n                (\n                    citation_url_pattern\n                    if citation_url_pattern is not None\n                    else citation_pattern\n                )\n                if include_citations\n                else None\n            )\n            computed_citations_text_pattern = citation_text_pattern if include_citations else None\n\n            vectara_query_engine = vectara.as_query_engine(\n                summary_enabled=True,\n                similarity_top_k=summary_num_results,\n                summary_num_results=summary_num_results,\n                summary_response_lang=summary_response_lang,\n                summary_prompt_name=vectara_summarizer,\n                prompt_text=vectara_prompt_text,\n                reranker=reranker,\n                rerank_k=(\n                    rerank_k\n                    if rerank_k * self.num_corpora &lt;= 100\n                    else int(100 / self.num_corpora)\n                ),\n                rerank_limit=rerank_limit,\n                rerank_cutoff=rerank_cutoff,\n                mmr_diversity_bias=mmr_diversity_bias,\n                udf_expression=udf_expression,\n                rerank_chain=rerank_chain,\n                n_sentences_before=n_sentences_before,\n                n_sentences_after=n_sentences_after,\n                offset=offset,\n                lambda_val=lambda_val,\n                semantics=semantics,\n                custom_dimensions=custom_dimensions,\n                filter=filter_string,\n                max_response_chars=max_response_chars,\n                max_tokens=max_tokens,\n                llm_name=llm_name,\n                temperature=temperature,\n                frequency_penalty=frequency_penalty,\n                presence_penalty=presence_penalty,\n                citations_style=\"markdown\" if include_citations else None,\n                citations_url_pattern=computed_citations_url_pattern,\n                citations_text_pattern=computed_citations_text_pattern,\n                save_history=save_history,\n                x_source_str=\"vectara-agentic\",\n                verbose=verbose,\n            )\n            response = _query_with_retry(vectara_query_engine, query)\n\n            if len(response.source_nodes) == 0:\n                msg = (\n                    \"Tool failed to generate a response since no matches were found. \"\n                    \"Please check the arguments and try again.\"\n                )\n                kwargs[\"query\"] = query\n                return {\"text\": msg, \"metadata\": {\"args\": args, \"kwargs\": kwargs}}\n            if str(response) == \"None\":\n                msg = \"Tool failed to generate a response.\"\n                kwargs[\"query\"] = query\n                return {\"text\": msg, \"metadata\": {\"args\": args, \"kwargs\": kwargs}}\n\n            fcs = 0.0\n            fcs_str = response.metadata[\"fcs\"] if \"fcs\" in response.metadata else \"0.0\"\n            if fcs_str and is_float(fcs_str):\n                fcs = float(fcs_str)\n                if fcs &lt; fcs_threshold:\n                    msg = f\"Could not answer the query due to suspected hallucination (fcs={fcs}).\"\n                    return {\n                        \"text\": msg,\n                        \"metadata\": {\"args\": args, \"kwargs\": kwargs, \"fcs\": fcs},\n                    }\n\n            # Add source nodes to tool output\n            if ((not return_human_readable_output) and\n                (computed_citations_url_pattern is not None) and\n                (computed_citations_text_pattern is not None)):\n                response_text = str(response.response)\n                citation_metadata = []\n\n                # Converts a dictionary to an object with .&lt;field&gt; access\n                def to_obj(data):\n                    return type('obj', (object,), data)()\n\n                for source_node in response.source_nodes:\n                    node = source_node.node\n                    node_id = node.id_\n                    node_text = (\n                        node.text_resource.text if hasattr(node, 'text_resource')\n                        else getattr(node, 'text', '')\n                    )\n                    node_metadata = getattr(node, 'metadata', {})\n                    for key in keys_to_ignore:\n                        if key in node_metadata:\n                            del node_metadata[key]\n\n                    try:\n                        template_data = {}\n\n                        doc_data = node_metadata.get('document', {})\n                        template_data['doc'] = to_obj(doc_data)\n\n                        part_data = {k: v for k, v in node_metadata.items() if k != 'document'}\n                        template_data['part'] = to_obj(part_data)\n\n                        formatted_citation_text = computed_citations_text_pattern.format(**template_data)\n                        formatted_citation_url = computed_citations_url_pattern.format(**template_data)\n                        expected_citation = f\"[{formatted_citation_text}]({formatted_citation_url})\"\n\n                        if expected_citation in response_text:\n                            citation_metadata.append({\n                                'doc_id': node_id,\n                                'text': node_text,\n                                'metadata': node_metadata,\n                                'score': getattr(node, 'score', None)\n                            })\n\n                    except Exception as e:\n                        if verbose:\n                            print(f\"Could not format citation for search result {node_id}: {e}\")\n                        continue\n\n                res = {\"text\": response.response, \"citations\": citation_metadata}\n                if fcs:\n                    res[\"fcs\"] = fcs\n            else:\n                res = {\"text\": response.response}\n\n            # Create human-readable output\n            if return_human_readable_output:\n                def format_rag_response(result):\n                    text = result[\"text\"]\n                    return text\n\n                return create_human_readable_output(res, format_rag_response)\n\n            return res\n\n        class RagToolBaseParams(BaseModel):\n            \"\"\"Model for the base parameters of the RAG tool.\"\"\"\n\n            query: str = Field(\n                ...,\n                description=\"The search query to perform, in the form of a question\",\n            )\n\n        tool = create_tool_from_dynamic_function(\n            rag_function,\n            tool_name,\n            tool_description,\n            RagToolBaseParams,\n            tool_args_schema,\n            return_direct=return_direct,\n            vhc_eligible=vhc_eligible,\n        )\n        return tool\n</code></pre>"},{"location":"api/#vectara_agentic.VectaraToolFactory.__init__","title":"<code>__init__(vectara_corpus_key=str(os.environ.get('VECTARA_CORPUS_KEY', '')), vectara_api_key=str(os.environ.get('VECTARA_API_KEY', '')))</code>","text":"<p>Initialize the VectaraToolFactory Args:     vectara_corpus_key (str): The Vectara corpus key (or comma separated list of keys).     vectara_api_key (str): The Vectara API key.</p> Source code in <code>vectara_agentic/tools.py</code> <pre><code>def __init__(\n    self,\n    vectara_corpus_key: str = str(os.environ.get(\"VECTARA_CORPUS_KEY\", \"\")),\n    vectara_api_key: str = str(os.environ.get(\"VECTARA_API_KEY\", \"\")),\n) -&gt; None:\n    \"\"\"\n    Initialize the VectaraToolFactory\n    Args:\n        vectara_corpus_key (str): The Vectara corpus key (or comma separated list of keys).\n        vectara_api_key (str): The Vectara API key.\n    \"\"\"\n    self.vectara_corpus_key = vectara_corpus_key\n    self.vectara_api_key = vectara_api_key\n    self.num_corpora = len(vectara_corpus_key.split(\",\"))\n</code></pre>"},{"location":"api/#vectara_agentic.VectaraToolFactory.create_rag_tool","title":"<code>create_rag_tool(tool_name, tool_description, tool_args_schema=None, tool_args_type={}, fixed_filter='', vectara_summarizer='vectara-summary-ext-24-05-med-omni', vectara_prompt_text=None, summary_num_results=5, summary_response_lang='eng', n_sentences_before=2, n_sentences_after=2, offset=0, lambda_val=0.005, semantics='default', custom_dimensions={}, reranker='slingshot', rerank_k=50, rerank_limit=None, rerank_cutoff=None, mmr_diversity_bias=0.2, udf_expression=None, rerank_chain=None, max_response_chars=None, max_tokens=None, llm_name=None, temperature=None, frequency_penalty=None, presence_penalty=None, include_citations=True, citation_pattern=None, citation_url_pattern='{doc.url}', citation_text_pattern='{doc.title}', save_history=False, fcs_threshold=0.0, return_direct=False, return_human_readable_output=False, verbose=False, vectara_base_url='https://api.vectara.io', vectara_verify_ssl=True, vhc_eligible=True)</code>","text":"<p>Creates a RAG (Retrieve and Generate) tool.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>The name of the tool.</p> required <code>tool_description</code> <code>str</code> <p>The description of the tool.</p> required <code>tool_args_schema</code> <code>BaseModel</code> <p>The schema for any tool arguments for filtering.</p> <code>None</code> <code>tool_args_type</code> <code>Dict[str, dict]</code> <p>attributes for each argument where they key is the field name and the value is a dictionary with the following keys: - 'type': the type of each filter attribute in Vectara (doc or part). - 'is_list': whether the filterable attribute is a list. - 'filter_name': the name of the filterable attribute in Vectara.</p> <code>{}</code> <code>fixed_filter</code> <code>str</code> <p>A fixed Vectara filter condition to apply to all queries.</p> <code>''</code> <code>vectara_summarizer</code> <code>str</code> <p>The Vectara summarizer to use.</p> <code>'vectara-summary-ext-24-05-med-omni'</code> <code>vectara_prompt_text</code> <code>str</code> <p>The prompt text for the Vectara summarizer.</p> <code>None</code> <code>summary_num_results</code> <code>int</code> <p>The number of summary results.</p> <code>5</code> <code>summary_response_lang</code> <code>str</code> <p>The response language for the summary.</p> <code>'eng'</code> <code>n_sentences_before</code> <code>int</code> <p>Number of sentences before the summary.</p> <code>2</code> <code>n_sentences_after</code> <code>int</code> <p>Number of sentences after the summary.</p> <code>2</code> <code>offset</code> <code>int</code> <p>Number of results to skip.</p> <code>0</code> <code>lambda_val</code> <code>Union[List[float] | float]</code> <p>Lambda value (or list of values for each corpora) for the Vectara query, when using hybrid search.</p> <code>0.005</code> <code>semantics</code> <code>Union[List[str], str]</code> <p>Indicates whether the query is intended as a query or response. Include list if using multiple corpora specifying the query type for each corpus.</p> <code>'default'</code> <code>custom_dimensions</code> <code>Union[List[Dict] | Dict]</code> <p>Custom dimensions for the query (for each corpora).</p> <code>{}</code> <code>reranker</code> <code>str</code> <p>The reranker mode.</p> <code>'slingshot'</code> <code>rerank_k</code> <code>int</code> <p>Number of top-k documents for reranking.</p> <code>50</code> <code>rerank_limit</code> <code>int</code> <p>Maximum number of results to return after reranking.</p> <code>None</code> <code>rerank_cutoff</code> <code>float</code> <p>Minimum score threshold for results to include after reranking.</p> <code>None</code> <code>mmr_diversity_bias</code> <code>float</code> <p>MMR diversity bias.</p> <code>0.2</code> <code>udf_expression</code> <code>str</code> <p>The user defined expression for reranking results.</p> <code>None</code> <code>rerank_chain</code> <code>List[Dict]</code> <p>A list of rerankers to be applied sequentially. Each dictionary should specify the \"type\" of reranker (mmr, slingshot, udf) and any other parameters (e.g. \"limit\" or \"cutoff\" for any type, \"diversity_bias\" for mmr, and \"user_function\" for udf). If using slingshot/multilingual_reranker_v1, it must be first in the list.</p> <code>None</code> <code>max_response_chars</code> <code>int</code> <p>The desired maximum number of characters for the generated summary.</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to be returned by the LLM.</p> <code>None</code> <code>llm_name</code> <code>str</code> <p>The name of the LLM to use for generation.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>The sampling temperature; higher values lead to more randomness.</p> <code>None</code> <code>frequency_penalty</code> <code>float</code> <p>How much to penalize repeating tokens in the response, higher values reducing likelihood of repeating the same line.</p> <code>None</code> <code>presence_penalty</code> <code>float</code> <p>How much to penalize repeating tokens in the response, higher values increasing the diversity of topics.</p> <code>None</code> <code>include_citations</code> <code>bool</code> <p>Whether to include citations in the response. If True, uses markdown vectara citations that requires the Vectara scale plan.</p> <code>True</code> <code>citation_url_pattern</code> <code>str</code> <p>The pattern for the citations in the response. Default is \"{doc.url}\" which uses the document URL. If include_citations is False, this parameter is ignored. citation_pattern (str, optional): old name for citation_url_pattern. Deprecated.</p> <code>'{doc.url}'</code> <code>citation_text_pattern</code> <code>str</code> <p>The text pattern for citations in the response. Default is \"{doc.title}\" which uses the title of the document. If include_citations is False, this parameter is ignored.</p> <code>'{doc.title}'</code> <code>save_history</code> <code>bool</code> <p>Whether to save the query in history.</p> <code>False</code> <code>fcs_threshold</code> <code>float</code> <p>A threshold for factual consistency. If set above 0, the tool notifies the calling agent that it \"cannot respond\" if FCS is too low.</p> <code>0.0</code> <code>return_direct</code> <code>bool</code> <p>Whether the agent should return the tool's response directly.</p> <code>False</code> <code>return_human_readable_output</code> <code>bool</code> <p>Whether to return the output in a human-readable format.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose output.</p> <code>False</code> <code>vectara_base_url</code> <code>str</code> <p>The base URL for the Vectara API.</p> <code>'https://api.vectara.io'</code> <code>vectara_verify_ssl</code> <code>bool</code> <p>Whether to verify SSL certificates for the Vectara API.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>VectaraTool</code> <code>VectaraTool</code> <p>A VectaraTool object.</p> Source code in <code>vectara_agentic/tools.py</code> <pre><code>def create_rag_tool(\n    self,\n    tool_name: str,\n    tool_description: str,\n    tool_args_schema: type[BaseModel] = None,\n    tool_args_type: Dict[str, dict] = {},\n    fixed_filter: str = \"\",\n    vectara_summarizer: str = \"vectara-summary-ext-24-05-med-omni\",\n    vectara_prompt_text: str = None,\n    summary_num_results: int = 5,\n    summary_response_lang: str = \"eng\",\n    n_sentences_before: int = 2,\n    n_sentences_after: int = 2,\n    offset: int = 0,\n    lambda_val: Union[List[float], float] = 0.005,\n    semantics: Union[List[str] | str] = \"default\",\n    custom_dimensions: Union[List[Dict], Dict] = {},\n    reranker: str = \"slingshot\",\n    rerank_k: int = 50,\n    rerank_limit: Optional[int] = None,\n    rerank_cutoff: Optional[float] = None,\n    mmr_diversity_bias: float = 0.2,\n    udf_expression: str = None,\n    rerank_chain: List[Dict] = None,\n    max_response_chars: Optional[int] = None,\n    max_tokens: Optional[int] = None,\n    llm_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    include_citations: bool = True,\n    citation_pattern: str = None,\n    citation_url_pattern: str = \"{doc.url}\",\n    citation_text_pattern: str = \"{doc.title}\",\n    save_history: bool = False,\n    fcs_threshold: float = 0.0,\n    return_direct: bool = False,\n    return_human_readable_output: bool = False,\n    verbose: bool = False,\n    vectara_base_url: str = \"https://api.vectara.io\",\n    vectara_verify_ssl: bool = True,\n    vhc_eligible: bool = True,\n) -&gt; VectaraTool:\n    \"\"\"\n    Creates a RAG (Retrieve and Generate) tool.\n\n    Args:\n        tool_name (str): The name of the tool.\n        tool_description (str): The description of the tool.\n        tool_args_schema (BaseModel, optional): The schema for any tool arguments for filtering.\n        tool_args_type (Dict[str, dict], optional): attributes for each argument where they key is the field name\n            and the value is a dictionary with the following keys:\n            - 'type': the type of each filter attribute in Vectara (doc or part).\n            - 'is_list': whether the filterable attribute is a list.\n            - 'filter_name': the name of the filterable attribute in Vectara.\n        fixed_filter (str, optional): A fixed Vectara filter condition to apply to all queries.\n        vectara_summarizer (str, optional): The Vectara summarizer to use.\n        vectara_prompt_text (str, optional): The prompt text for the Vectara summarizer.\n        summary_num_results (int, optional): The number of summary results.\n        summary_response_lang (str, optional): The response language for the summary.\n        n_sentences_before (int, optional): Number of sentences before the summary.\n        n_sentences_after (int, optional): Number of sentences after the summary.\n        offset (int, optional): Number of results to skip.\n        lambda_val (Union[List[float] | float], optional): Lambda value (or list of values for each corpora)\n            for the Vectara query, when using hybrid search.\n        semantics (Union[List[str], str], optional): Indicates whether the query is intended as a query or response.\n            Include list if using multiple corpora specifying the query type for each corpus.\n        custom_dimensions (Union[List[Dict] | Dict], optional): Custom dimensions for the query (for each corpora).\n        reranker (str, optional): The reranker mode.\n        rerank_k (int, optional): Number of top-k documents for reranking.\n        rerank_limit (int, optional): Maximum number of results to return after reranking.\n        rerank_cutoff (float, optional): Minimum score threshold for results to include after reranking.\n        mmr_diversity_bias (float, optional): MMR diversity bias.\n        udf_expression (str, optional): The user defined expression for reranking results.\n        rerank_chain (List[Dict], optional): A list of rerankers to be applied sequentially.\n            Each dictionary should specify the \"type\" of reranker (mmr, slingshot, udf)\n            and any other parameters (e.g. \"limit\" or \"cutoff\" for any type,\n            \"diversity_bias\" for mmr, and \"user_function\" for udf).\n            If using slingshot/multilingual_reranker_v1, it must be first in the list.\n        max_response_chars (int, optional): The desired maximum number of characters for the generated summary.\n        max_tokens (int, optional): The maximum number of tokens to be returned by the LLM.\n        llm_name (str, optional): The name of the LLM to use for generation.\n        temperature (float, optional): The sampling temperature; higher values lead to more randomness.\n        frequency_penalty (float, optional): How much to penalize repeating tokens in the response,\n            higher values reducing likelihood of repeating the same line.\n        presence_penalty (float, optional): How much to penalize repeating tokens in the response,\n            higher values increasing the diversity of topics.\n        include_citations (bool, optional): Whether to include citations in the response.\n            If True, uses markdown vectara citations that requires the Vectara scale plan.\n        citation_url_pattern (str, optional): The pattern for the citations in the response.\n            Default is \"{doc.url}\" which uses the document URL.\n            If include_citations is False, this parameter is ignored.\n            citation_pattern (str, optional): old name for citation_url_pattern. Deprecated.\n        citation_text_pattern (str, optional): The text pattern for citations in the response.\n            Default is \"{doc.title}\" which uses the title of the document.\n            If include_citations is False, this parameter is ignored.\n        save_history (bool, optional): Whether to save the query in history.\n        fcs_threshold (float, optional): A threshold for factual consistency.\n            If set above 0, the tool notifies the calling agent that it \"cannot respond\" if FCS is too low.\n        return_direct (bool, optional): Whether the agent should return the tool's response directly.\n        return_human_readable_output (bool, optional): Whether to return the output in a human-readable format.\n        verbose (bool, optional): Whether to print verbose output.\n        vectara_base_url (str, optional): The base URL for the Vectara API.\n        vectara_verify_ssl (bool, optional): Whether to verify SSL certificates for the Vectara API.\n\n    Returns:\n        VectaraTool: A VectaraTool object.\n    \"\"\"\n\n    vectara = VectaraIndex(\n        vectara_api_key=self.vectara_api_key,\n        vectara_corpus_key=self.vectara_corpus_key,\n        x_source_str=\"vectara-agentic\",\n        vectara_base_url=vectara_base_url,\n        vectara_verify_ssl=vectara_verify_ssl,\n    )\n    vectara.vectara_api_timeout = 60\n    keys_to_ignore = [\"lang\", \"offset\", \"len\"]\n\n    # Dynamically generate the RAG function\n    def rag_function(*args: Any, **kwargs: Any) -&gt; dict:\n        \"\"\"\n        Dynamically generated function for RAG query with Vectara.\n        \"\"\"\n        # Convert args to kwargs using the function signature\n        sig = inspect.signature(rag_function)\n        bound_args = sig.bind_partial(*args, **kwargs)\n        bound_args.apply_defaults()\n        kwargs = bound_args.arguments\n\n        query = kwargs.pop(\"query\")\n        try:\n            filter_string = build_filter_string(\n                kwargs, tool_args_type, fixed_filter\n            )\n        except ValueError as e:\n            msg = (\n                f\"Building filter string failed in rag tool. \"\n                f\"Reason: {e}. Ensure that the input arguments match the expected \"\n                f\"format and include all required fields. \"\n            )\n            return {\"text\": msg, \"metadata\": {\"args\": args, \"kwargs\": kwargs}}\n\n        computed_citations_url_pattern = (\n            (\n                citation_url_pattern\n                if citation_url_pattern is not None\n                else citation_pattern\n            )\n            if include_citations\n            else None\n        )\n        computed_citations_text_pattern = citation_text_pattern if include_citations else None\n\n        vectara_query_engine = vectara.as_query_engine(\n            summary_enabled=True,\n            similarity_top_k=summary_num_results,\n            summary_num_results=summary_num_results,\n            summary_response_lang=summary_response_lang,\n            summary_prompt_name=vectara_summarizer,\n            prompt_text=vectara_prompt_text,\n            reranker=reranker,\n            rerank_k=(\n                rerank_k\n                if rerank_k * self.num_corpora &lt;= 100\n                else int(100 / self.num_corpora)\n            ),\n            rerank_limit=rerank_limit,\n            rerank_cutoff=rerank_cutoff,\n            mmr_diversity_bias=mmr_diversity_bias,\n            udf_expression=udf_expression,\n            rerank_chain=rerank_chain,\n            n_sentences_before=n_sentences_before,\n            n_sentences_after=n_sentences_after,\n            offset=offset,\n            lambda_val=lambda_val,\n            semantics=semantics,\n            custom_dimensions=custom_dimensions,\n            filter=filter_string,\n            max_response_chars=max_response_chars,\n            max_tokens=max_tokens,\n            llm_name=llm_name,\n            temperature=temperature,\n            frequency_penalty=frequency_penalty,\n            presence_penalty=presence_penalty,\n            citations_style=\"markdown\" if include_citations else None,\n            citations_url_pattern=computed_citations_url_pattern,\n            citations_text_pattern=computed_citations_text_pattern,\n            save_history=save_history,\n            x_source_str=\"vectara-agentic\",\n            verbose=verbose,\n        )\n        response = _query_with_retry(vectara_query_engine, query)\n\n        if len(response.source_nodes) == 0:\n            msg = (\n                \"Tool failed to generate a response since no matches were found. \"\n                \"Please check the arguments and try again.\"\n            )\n            kwargs[\"query\"] = query\n            return {\"text\": msg, \"metadata\": {\"args\": args, \"kwargs\": kwargs}}\n        if str(response) == \"None\":\n            msg = \"Tool failed to generate a response.\"\n            kwargs[\"query\"] = query\n            return {\"text\": msg, \"metadata\": {\"args\": args, \"kwargs\": kwargs}}\n\n        fcs = 0.0\n        fcs_str = response.metadata[\"fcs\"] if \"fcs\" in response.metadata else \"0.0\"\n        if fcs_str and is_float(fcs_str):\n            fcs = float(fcs_str)\n            if fcs &lt; fcs_threshold:\n                msg = f\"Could not answer the query due to suspected hallucination (fcs={fcs}).\"\n                return {\n                    \"text\": msg,\n                    \"metadata\": {\"args\": args, \"kwargs\": kwargs, \"fcs\": fcs},\n                }\n\n        # Add source nodes to tool output\n        if ((not return_human_readable_output) and\n            (computed_citations_url_pattern is not None) and\n            (computed_citations_text_pattern is not None)):\n            response_text = str(response.response)\n            citation_metadata = []\n\n            # Converts a dictionary to an object with .&lt;field&gt; access\n            def to_obj(data):\n                return type('obj', (object,), data)()\n\n            for source_node in response.source_nodes:\n                node = source_node.node\n                node_id = node.id_\n                node_text = (\n                    node.text_resource.text if hasattr(node, 'text_resource')\n                    else getattr(node, 'text', '')\n                )\n                node_metadata = getattr(node, 'metadata', {})\n                for key in keys_to_ignore:\n                    if key in node_metadata:\n                        del node_metadata[key]\n\n                try:\n                    template_data = {}\n\n                    doc_data = node_metadata.get('document', {})\n                    template_data['doc'] = to_obj(doc_data)\n\n                    part_data = {k: v for k, v in node_metadata.items() if k != 'document'}\n                    template_data['part'] = to_obj(part_data)\n\n                    formatted_citation_text = computed_citations_text_pattern.format(**template_data)\n                    formatted_citation_url = computed_citations_url_pattern.format(**template_data)\n                    expected_citation = f\"[{formatted_citation_text}]({formatted_citation_url})\"\n\n                    if expected_citation in response_text:\n                        citation_metadata.append({\n                            'doc_id': node_id,\n                            'text': node_text,\n                            'metadata': node_metadata,\n                            'score': getattr(node, 'score', None)\n                        })\n\n                except Exception as e:\n                    if verbose:\n                        print(f\"Could not format citation for search result {node_id}: {e}\")\n                    continue\n\n            res = {\"text\": response.response, \"citations\": citation_metadata}\n            if fcs:\n                res[\"fcs\"] = fcs\n        else:\n            res = {\"text\": response.response}\n\n        # Create human-readable output\n        if return_human_readable_output:\n            def format_rag_response(result):\n                text = result[\"text\"]\n                return text\n\n            return create_human_readable_output(res, format_rag_response)\n\n        return res\n\n    class RagToolBaseParams(BaseModel):\n        \"\"\"Model for the base parameters of the RAG tool.\"\"\"\n\n        query: str = Field(\n            ...,\n            description=\"The search query to perform, in the form of a question\",\n        )\n\n    tool = create_tool_from_dynamic_function(\n        rag_function,\n        tool_name,\n        tool_description,\n        RagToolBaseParams,\n        tool_args_schema,\n        return_direct=return_direct,\n        vhc_eligible=vhc_eligible,\n    )\n    return tool\n</code></pre>"},{"location":"api/#vectara_agentic.VectaraToolFactory.create_search_tool","title":"<code>create_search_tool(tool_name, tool_description, tool_args_schema=None, tool_args_type={}, summarize_docs=None, summarize_llm_name=None, fixed_filter='', lambda_val=0.005, semantics='default', custom_dimensions={}, offset=0, n_sentences_before=2, n_sentences_after=2, reranker='slingshot', rerank_k=50, rerank_limit=None, rerank_cutoff=None, mmr_diversity_bias=0.2, udf_expression=None, rerank_chain=None, return_direct=False, save_history=True, verbose=False, vectara_base_url='https://api.vectara.io', vectara_verify_ssl=True, vhc_eligible=True)</code>","text":"<p>Creates a Vectara search/retrieval tool</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>The name of the tool.</p> required <code>tool_description</code> <code>str</code> <p>The description of the tool.</p> required <code>tool_args_schema</code> <code>BaseModel</code> <p>The schema for the tool arguments.</p> <code>None</code> <code>tool_args_type</code> <code>Dict[str, dict]</code> <p>attributes for each argument where they key is the field name and the value is a dictionary with the following keys: - 'type': the type of each filter attribute in Vectara (doc or part). - 'is_list': whether the filterable attribute is a list. - 'filter_name': the name of the filterable attribute in Vectara.</p> <code>{}</code> <code>summarize_docs</code> <code>bool</code> <p>Whether to summarize the retrieved documents.</p> <code>None</code> <code>summarize_llm_name</code> <code>str</code> <p>The name of the LLM to use for summarization.</p> <code>None</code> <code>fixed_filter</code> <code>str</code> <p>A fixed Vectara filter condition to apply to all queries.</p> <code>''</code> <code>lambda_val</code> <code>Union[List[float] | float]</code> <p>Lambda value (or list of values for each corpora) for the Vectara query, when using hybrid search.</p> <code>0.005</code> <code>semantics</code> <code>Union[List[str], str]</code> <p>Indicates whether the query is intended as a query or response. Include list if using multiple corpora specifying the query type for each corpus.</p> <code>'default'</code> <code>custom_dimensions</code> <code>Union[List[Dict] | Dict]</code> <p>Custom dimensions for the query (for each corpora).</p> <code>{}</code> <code>offset</code> <code>int</code> <p>Number of results to skip.</p> <code>0</code> <code>n_sentences_before</code> <code>int</code> <p>Number of sentences before the matching document part.</p> <code>2</code> <code>n_sentences_after</code> <code>int</code> <p>Number of sentences after the matching document part.</p> <code>2</code> <code>reranker</code> <code>str</code> <p>The reranker mode.</p> <code>'slingshot'</code> <code>rerank_k</code> <code>int</code> <p>Number of top-k documents for reranking.</p> <code>50</code> <code>rerank_limit</code> <code>int</code> <p>Maximum number of results to return after reranking.</p> <code>None</code> <code>rerank_cutoff</code> <code>float</code> <p>Minimum score threshold for results to include after reranking.</p> <code>None</code> <code>mmr_diversity_bias</code> <code>float</code> <p>MMR diversity bias.</p> <code>0.2</code> <code>udf_expression</code> <code>str</code> <p>the user defined expression for reranking results.</p> <code>None</code> <code>rerank_chain</code> <code>List[Dict]</code> <p>A list of rerankers to be applied sequentially. Each dictionary should specify the \"type\" of reranker (mmr, slingshot, udf) and any other parameters (e.g. \"limit\" or \"cutoff\" for any type, \"diversity_bias\" for mmr, and \"user_function\" for udf). If using slingshot/multilingual_reranker_v1, it must be first in the list.</p> <code>None</code> <code>save_history</code> <code>bool</code> <p>Whether to save the query in history.</p> <code>True</code> <code>return_direct</code> <code>bool</code> <p>Whether the agent should return the tool's response directly.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose output.</p> <code>False</code> <code>vectara_base_url</code> <code>str</code> <p>The base URL for the Vectara API.</p> <code>'https://api.vectara.io'</code> <code>vectara_verify_ssl</code> <code>bool</code> <p>Whether to verify SSL certificates for the Vectara API.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>VectaraTool</code> <code>VectaraTool</code> <p>A VectaraTool object.</p> Source code in <code>vectara_agentic/tools.py</code> <pre><code>def create_search_tool(\n    self,\n    tool_name: str,\n    tool_description: str,\n    tool_args_schema: type[BaseModel] = None,\n    tool_args_type: Dict[str, str] = {},\n    summarize_docs: Optional[bool] = None,\n    summarize_llm_name: Optional[str] = None,\n    fixed_filter: str = \"\",\n    lambda_val: Union[List[float], float] = 0.005,\n    semantics: Union[List[str] | str] = \"default\",\n    custom_dimensions: Union[List[Dict], Dict] = {},\n    offset: int = 0,\n    n_sentences_before: int = 2,\n    n_sentences_after: int = 2,\n    reranker: str = \"slingshot\",\n    rerank_k: int = 50,\n    rerank_limit: Optional[int] = None,\n    rerank_cutoff: Optional[float] = None,\n    mmr_diversity_bias: float = 0.2,\n    udf_expression: str = None,\n    rerank_chain: List[Dict] = None,\n    return_direct: bool = False,\n    save_history: bool = True,\n    verbose: bool = False,\n    vectara_base_url: str = \"https://api.vectara.io\",\n    vectara_verify_ssl: bool = True,\n    vhc_eligible: bool = True,\n) -&gt; VectaraTool:\n    \"\"\"\n    Creates a Vectara search/retrieval tool\n\n    Args:\n        tool_name (str): The name of the tool.\n        tool_description (str): The description of the tool.\n        tool_args_schema (BaseModel, optional): The schema for the tool arguments.\n        tool_args_type (Dict[str, dict], optional): attributes for each argument where they key is the field name\n            and the value is a dictionary with the following keys:\n            - 'type': the type of each filter attribute in Vectara (doc or part).\n            - 'is_list': whether the filterable attribute is a list.\n            - 'filter_name': the name of the filterable attribute in Vectara.\n        summarize_docs (bool, optional): Whether to summarize the retrieved documents.\n        summarize_llm_name (str, optional): The name of the LLM to use for summarization.\n        fixed_filter (str, optional): A fixed Vectara filter condition to apply to all queries.\n        lambda_val (Union[List[float] | float], optional): Lambda value (or list of values for each corpora)\n            for the Vectara query, when using hybrid search.\n        semantics (Union[List[str], str], optional): Indicates whether the query is intended as a query or response.\n            Include list if using multiple corpora specifying the query type for each corpus.\n        custom_dimensions (Union[List[Dict] | Dict], optional): Custom dimensions for the query (for each corpora).\n        offset (int, optional): Number of results to skip.\n        n_sentences_before (int, optional): Number of sentences before the matching document part.\n        n_sentences_after (int, optional): Number of sentences after the matching document part.\n        reranker (str, optional): The reranker mode.\n        rerank_k (int, optional): Number of top-k documents for reranking.\n        rerank_limit (int, optional): Maximum number of results to return after reranking.\n        rerank_cutoff (float, optional): Minimum score threshold for results to include after reranking.\n        mmr_diversity_bias (float, optional): MMR diversity bias.\n        udf_expression (str, optional): the user defined expression for reranking results.\n        rerank_chain (List[Dict], optional): A list of rerankers to be applied sequentially.\n            Each dictionary should specify the \"type\" of reranker (mmr, slingshot, udf)\n            and any other parameters (e.g. \"limit\" or \"cutoff\" for any type,\n            \"diversity_bias\" for mmr, and \"user_function\" for udf).\n            If using slingshot/multilingual_reranker_v1, it must be first in the list.\n        save_history (bool, optional): Whether to save the query in history.\n        return_direct (bool, optional): Whether the agent should return the tool's response directly.\n        verbose (bool, optional): Whether to print verbose output.\n        vectara_base_url (str, optional): The base URL for the Vectara API.\n        vectara_verify_ssl (bool, optional): Whether to verify SSL certificates for the Vectara API.\n\n    Returns:\n        VectaraTool: A VectaraTool object.\n    \"\"\"\n\n    vectara = VectaraIndex(\n        vectara_api_key=self.vectara_api_key,\n        vectara_corpus_key=self.vectara_corpus_key,\n        x_source_str=\"vectara-agentic\",\n        vectara_base_url=vectara_base_url,\n        vectara_verify_ssl=vectara_verify_ssl,\n    )\n    vectara.vectara_api_timeout = 10\n\n    # Dynamically generate the search function\n    def search_function(*args: Any, **kwargs: Any) -&gt; list[dict]:\n        \"\"\"\n        Dynamically generated function for semantic search Vectara.\n        \"\"\"\n        # Convert args to kwargs using the function signature\n        sig = inspect.signature(search_function)\n        bound_args = sig.bind_partial(*args, **kwargs)\n        bound_args.apply_defaults()\n        kwargs = bound_args.arguments\n\n        query = kwargs.pop(\"query\")\n        top_k = kwargs.pop(\"top_k\", 10)\n        summarize = (\n            kwargs.pop(\"summarize\", True)\n            if summarize_docs is None\n            else summarize_docs\n        )\n        try:\n            filter_string = build_filter_string(\n                kwargs, tool_args_type, fixed_filter\n            )\n        except ValueError as e:\n            msg = (\n                f\"Building filter string failed in search tool due to invalid input or configuration ({e}). \"\n                \"Please verify the input arguments and ensure they meet the expected format or conditions.\"\n            )\n            return [{\"text\": msg, \"metadata\": {\"args\": args, \"kwargs\": kwargs}}]\n\n        vectara_retriever = vectara.as_retriever(\n            summary_enabled=False,\n            similarity_top_k=top_k,\n            reranker=reranker,\n            rerank_k=(\n                rerank_k\n                if rerank_k * self.num_corpora &lt;= 100\n                else int(100 / self.num_corpora)\n            ),\n            rerank_limit=rerank_limit,\n            rerank_cutoff=rerank_cutoff,\n            mmr_diversity_bias=mmr_diversity_bias,\n            udf_expression=udf_expression,\n            rerank_chain=rerank_chain,\n            lambda_val=lambda_val,\n            semantics=semantics,\n            custom_dimensions=custom_dimensions,\n            offset=offset,\n            filter=filter_string,\n            n_sentences_before=n_sentences_before,\n            n_sentences_after=n_sentences_after,\n            save_history=save_history,\n            x_source_str=\"vectara-agentic\",\n            verbose=verbose,\n        )\n        response = _retrieve_with_retry(vectara_retriever, query)\n\n        if len(response) == 0:\n            msg = \"Vectara Tool failed to retrieve any results for the query.\"\n            return [{\"text\": msg, \"metadata\": {\"args\": args, \"kwargs\": kwargs}}]\n        unique_ids = set()\n        docs = []\n        doc_matches = {}\n        for doc in response:\n            if doc.id_ in unique_ids:\n                doc_matches[doc.id_].append(doc.node.get_content())\n                continue\n            unique_ids.add(doc.id_)\n            doc_matches[doc.id_] = [doc.node.get_content()]\n            docs.append((doc.id_, doc.metadata))\n\n        res = []\n        if summarize:\n            summaries_dict = asyncio.run(\n                summarize_documents(\n                    corpus_key=self.vectara_corpus_key,\n                    api_key=self.vectara_api_key,\n                    llm_name=summarize_llm_name,\n                    doc_ids=list(unique_ids),\n                )\n            )\n        else:\n            summaries_dict = {}\n\n        for doc_id, metadata in docs:\n            res.append(\n                {\n                    \"text\": summaries_dict.get(doc_id, \"\") if summarize else \"\",\n                    \"metadata\": {\n                        \"document_id\": doc_id,\n                        \"metadata\": metadata,\n                        \"matching_text\": doc_matches[doc_id],\n                    },\n                }\n            )\n\n        # Create human-readable output using sequential format\n        def format_search_results(results):\n            if not results:\n                return \"No search results found\"\n\n            # Create a sequential view for human reading\n            formatted_results = []\n            for i, result in enumerate(results, 1):\n                result_str = f\"**Result #{i}**\\n\"\n                result_str += f\"Document ID: {result['metadata']['document_id']}\\n\"\n                if summarize and result[\"text\"]:\n                    result_str += f\"Summary: {result['text']}\\n\"\n\n                # Add all matching text if available\n                matches = result[\"metadata\"][\"matching_text\"]\n                if matches:\n                    result_str += \"\".join(\n                        f\"Match #{inx} Text: {match}\\n\"\n                        for inx, match in enumerate(matches, 1)\n                    )\n                formatted_results.append(result_str)\n            return \"\\n\".join(formatted_results)\n\n        return create_human_readable_output(res, format_search_results)\n\n    class SearchToolBaseParams(BaseModel):\n        \"\"\"Model for the base parameters of the search tool.\"\"\"\n\n        query: str = Field(\n            ...,\n            description=\"The search query to perform, in the form of a question.\",\n        )\n        top_k: int = Field(\n            default=10, description=\"The number of top documents to retrieve.\"\n        )\n        summarize: bool = Field(\n            True,\n            description=\"Whether to summarize the retrieved documents.\",\n        )\n\n    class SearchToolBaseParamsWithoutSummarize(BaseModel):\n        \"\"\"Model for the base parameters of the search tool.\"\"\"\n\n        query: str = Field(\n            ...,\n            description=\"The search query to perform, in the form of a question.\",\n        )\n        top_k: int = Field(\n            default=10, description=\"The number of top documents to retrieve.\"\n        )\n\n    search_tool_extra_desc = (\n        tool_description\n        + \"\\n\"\n        + \"Use this tool to search for relevant documents, not to ask questions.\"\n    )\n\n    tool = create_tool_from_dynamic_function(\n        search_function,\n        tool_name,\n        search_tool_extra_desc,\n        (\n            SearchToolBaseParams\n            if summarize_docs is None\n            else SearchToolBaseParamsWithoutSummarize\n        ),\n        tool_args_schema,\n        return_direct=return_direct,\n        vhc_eligible=vhc_eligible,\n    )\n    return tool\n</code></pre>"},{"location":"api/#vectara_agentic.create_app","title":"<code>create_app(agent, config)</code>","text":"<p>Create and configure the FastAPI app.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>The agent instance to handle chat/completion.</p> required <code>config</code> <code>AgentConfig</code> <p>Configuration containing the API key.</p> required <p>Returns:</p> Name Type Description <code>FastAPI</code> <code>FastAPI</code> <p>Configured FastAPI application.</p> Source code in <code>vectara_agentic/agent_endpoint.py</code> <pre><code>def create_app(agent: Agent, config: AgentConfig) -&gt; FastAPI:\n    \"\"\"\n    Create and configure the FastAPI app.\n\n    Args:\n        agent (Agent): The agent instance to handle chat/completion.\n        config (AgentConfig): Configuration containing the API key.\n\n    Returns:\n        FastAPI: Configured FastAPI application.\n    \"\"\"\n    app = FastAPI()\n    logger = logging.getLogger(\"uvicorn.error\")\n    logging.basicConfig(level=logging.INFO)\n\n    api_key_header = APIKeyHeader(name=\"X-API-Key\")\n\n    async def _verify_api_key(api_key: str = Depends(api_key_header)):\n        \"\"\"\n        Dependency that verifies the X-API-Key header.\n\n        Raises:\n            HTTPException(403): If the provided key does not match.\n\n        Returns:\n            bool: True if key is valid.\n        \"\"\"\n        if api_key != config.endpoint_api_key:\n            raise HTTPException(status_code=403, detail=\"Unauthorized\")\n        return True\n\n    @app.get(\n        \"/chat\", summary=\"Chat with the agent\", dependencies=[Depends(_verify_api_key)]\n    )\n    async def chat(message: str):\n        \"\"\"\n        Handle GET /chat requests.\n\n        Args:\n            message (str): The user's message to the agent.\n\n        Returns:\n            dict: Contains the agent's response under 'response'.\n\n        Raises:\n            HTTPException(400): If message is empty.\n            HTTPException(500): On internal errors.\n        \"\"\"\n        if not message:\n            raise HTTPException(status_code=400, detail=\"No message provided\")\n        try:\n            res = agent.chat(message)\n            return {\"response\": res}\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=\"Internal server error\") from e\n\n    @app.post(\n        \"/v1/completions\",\n        response_model=CompletionResponse,\n        dependencies=[Depends(_verify_api_key)],\n    )\n    async def completions(req: CompletionRequest):\n        \"\"\"\n        Handle POST /v1/completions requests.\n\n        Args:\n            req (CompletionRequest): The completion request payload.\n\n        Returns:\n            CompletionResponse: The generated completion and usage stats.\n\n        Raises:\n            HTTPException(400): If prompt is missing.\n            HTTPException(500): On internal errors.\n        \"\"\"\n        if not req.prompt:\n            raise HTTPException(status_code=400, detail=\"`prompt` is required\")\n        raw = req.prompt if isinstance(req.prompt, str) else req.prompt[0]\n        try:\n            start = time.time()\n            text = agent.chat(raw)\n            logger.info(f\"Agent returned in {time.time()-start:.2f}s\")\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=\"Internal server error\") from e\n\n        p_tokens = len(raw.split())\n        c_tokens = len(text.split())\n\n        return CompletionResponse(\n            id=f\"cmpl-{uuid.uuid4()}\",\n            object=\"text_completion\",\n            created=int(time.time()),\n            model=req.model,\n            choices=[Choice(text=text, index=0, logprobs=None, finish_reason=\"stop\")],\n            usage=CompletionUsage(\n                prompt_tokens=p_tokens,\n                completion_tokens=c_tokens,\n                total_tokens=p_tokens + c_tokens,\n            ),\n        )\n\n    @app.post(\n        \"/v1/chat\",\n        response_model=ChatCompletionResponse,\n        dependencies=[Depends(_verify_api_key)],\n    )\n    async def chat_completion(req: ChatCompletionRequest):\n        if not req.messages:\n            raise HTTPException(status_code=400, detail=\"`messages` is required\")\n\n        # concatenate all user messages into a single prompt\n        raw = \" \".join(m.content for m in req.messages if m.role == \"user\")\n\n        try:\n            start = time.time()\n            text = agent.chat(raw)\n            logger.info(f\"Agent returned in {time.time()-start:.2f}s\")\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=\"Internal server error\") from e\n\n        p_tokens = len(raw.split())\n        c_tokens = len(text.split())\n\n        return ChatCompletionResponse(\n            id=f\"chatcmpl-{uuid.uuid4()}\",\n            object=\"chat.completion\",\n            created=int(time.time()),\n            model=req.model,\n            choices=[\n                ChatCompletionChoice(\n                    index=0,\n                    message=ChatMessage(role=\"assistant\", content=text),\n                    finish_reason=\"stop\",\n                )\n            ],\n            usage=CompletionUsage(\n                prompt_tokens=p_tokens,\n                completion_tokens=c_tokens,\n                total_tokens=p_tokens + c_tokens,\n            ),\n        )\n\n    return app\n</code></pre>"},{"location":"api/#vectara_agentic.start_app","title":"<code>start_app(agent, host='0.0.0.0', port=8000)</code>","text":"<p>Launch the FastAPI application using Uvicorn.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>The agent instance for request handling.</p> required <code>host</code> <code>str</code> <p>Host interface. Defaults to \"0.0.0.0\".</p> <code>'0.0.0.0'</code> <code>port</code> <code>int</code> <p>Port number. Defaults to 8000.</p> <code>8000</code> Source code in <code>vectara_agentic/agent_endpoint.py</code> <pre><code>def start_app(agent: Agent, host=\"0.0.0.0\", port=8000):\n    \"\"\"\n    Launch the FastAPI application using Uvicorn.\n\n    Args:\n        agent (Agent): The agent instance for request handling.\n        host (str, optional): Host interface. Defaults to \"0.0.0.0\".\n        port (int, optional): Port number. Defaults to 8000.\n    \"\"\"\n    app = create_app(agent, config=AgentConfig())\n    uvicorn.run(app, host=host, port=port)\n</code></pre>"},{"location":"endpoint/","title":"API Endpoint","text":"<p>It's super easy to host your vectara-agentic assistant or agent behind an API endpoint:</p> <p><code>vectara-agentic</code> can be easily hosted locally or on a remote machine behind an API endpoint, by following these steps:</p> <p>1. Setup your API key Ensure that you have your API key set up as an environment variable:</p> <pre><code>export VECTARA_AGENTIC_API_KEY=&lt;YOUR-ENDPOINT-API-KEY&gt;\n</code></pre> <p>2. Start the API Server Initialize the agent and start the FastAPI server by following this example:</p> <pre><code>from agent import Agent\nfrom agent_endpoint import start_app\nagent = Agent(...)      # Initialize your agent with appropriate parameters\nstart_app(agent)\n</code></pre> <p>You can customize the host and port by passing them as arguments to start_app().</p> <p>For example:</p> <pre><code>start_app(agent, host=\"0.0.0.0\", port=8000)\n</code></pre> <p>3. Access the API Endpoint Once the server is running, you can interact with it using curl or any HTTP client. For example:</p> <pre><code>curl -G \"http://&lt;remote-server-ip&gt;:8000/chat\" \\\n--data-urlencode \"message=What is Vectara?\" \\\n-H \"X-API-Key: &lt;YOUR-API-KEY&gt;\"\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>You can install vectara-agentic using pip as follows:</p> <pre><code>pip install vectara-agentic\n</code></pre>"},{"location":"tools/","title":"Tools","text":"<p><code>vectara-agentic</code> provides a set of pre-built tools that you can use out-of-the-box for various purposes.</p>"},{"location":"tools/#standard-tools","title":"Standard Tools","text":"<p>Basic tools for general purposes:</p> <ul> <li>summarize_text: Summarizes text from a specific perspective or expertise level</li> <li>rephrase_text: Rephrases text according to specified instructions (e.g., for a 5-year-old or in formal tone)</li> </ul>"},{"location":"tools/#finance-tools","title":"Finance Tools","text":"<p><code>vectara-agentic</code> includes a few financial tools you can use right away in your agent, based on the LlamaIndex YahooFinanceToolSpec:</p> <ul> <li>balance_sheet: Returns a company's balance sheet</li> <li>income_statement: Returns a company's income statement</li> <li>cash_flow: Returns a company's cash flow statement</li> <li>stock_news: Returns latest news about a company</li> <li>stock_basic_info: Returns basic company information including price</li> <li>stock_analyst_recommendations: Returns analyst recommendations for a company</li> </ul>"},{"location":"tools/#legal-tools","title":"Legal Tools","text":"<p>vectara-agentic includes a few tools for the legal space:</p> <ul> <li>summarize_legal_text: Summarizes legal documents</li> <li>critique_as_judge: Critiques legal text from an expert judge's perspective</li> </ul>"},{"location":"tools/#guardrail-tools","title":"Guardrail Tools","text":"<p>The guardrail tools help you AI assistant or agent to avoid certain topics or responses that are prohibited by your organization or by law.</p> <p>The <code>get_bad_topics</code> tool returns a list of topics that are prohibited (politics, religion, violence, hate speech, adult content, illegal activities). The agent prompt has special instructions to call this tool if it exists, and avoid these topics.</p> <p>If you want to create your own set of topics, you can define a new tool by the same name (<code>get_bad_topics</code>) that returns a list of different topics, and the agent will use that list to avoid these topics.</p>"},{"location":"tools/#database-tools","title":"Database Tools","text":"<p>Database tools are quite useful if your agent requires access to a combination of RAG tools along with analytics capabilities. For example, consider the EV-assistant demo, providing answers about electric vehicles.</p> <p>We have provided this assistant with the following tools:</p> <ol> <li><code>ask_vehicles</code>: A Vectara RAG tool that answers general questions     about electric vehicles.</li> <li><code>ask_policies</code>: A Vectara RAG tool that answers questions about     electric vehicle policies.</li> <li>The <code>database_tools</code> that can help the agent answer analytics     queries based on three datasets: EV population data, EV population     size history by county, and EV title and registration activity.</li> </ol> <p>With the <code>ask_vehicles</code> and <code>ask_policies</code> tools, the ev-assistant can answer questions based on text, and it will use the database tools to answer analytical questions, based on the data.</p> <p>Here is an example for instantiating the database tools:</p> <pre><code># For a single database\ndatabase_tools = ToolsFactory().database_tools(\n    sql_database=your_database_object,\n    tool_name_prefix=\"ev\"\n)\n</code></pre> <p>This creates five tools:</p> <ol> <li><code>ev_list_tables</code>: A tool that lists the tables in the database.</li> <li><code>ev_describe_tables</code>: A tool that describes the schema of a table.</li> <li><code>ev_load_data</code>: A tool that loads data from a table.</li> <li><code>ev_load_sample_data</code> tool which provides a sample of the data from a table.</li> <li><code>ev_load_unique_values</code> tool which provides unique values for a set of columns in a table.</li> </ol> <p>Together, these 5 tools provide a comprehensive set of capabilities for an agent to interact with a database. </p> <p>For example, an agent can use the <code>ev_list_tables</code> tool to get a list of tables in the database, and then use the <code>ev_describe_tables</code> tool to get the schema of a specific table. It will use the <code>ev_load_sample_data</code> to get a sample of the data in the table, or the <code>ev_load_unique_values</code> to explore the type of values valid for a column. Finally, the agent can use the <code>ev_load_data</code> tool to load the data into the agent's memory.</p> <p>Multiple databases</p> <p>In the case of EV-assistant, we use only a single database with 4 tables, and <code>tool_name_prefix=\"ev\"</code></p> <p>If your use-case includes multiple databases, you can define multiple database tools: each with a different database connection and a different <code>tool_name_prefix</code>.</p>"},{"location":"tools/#other-tools","title":"Other Tools","text":"<p>In addition to the tools above, vectara-agentic also supports these additional tools from the LlamaIndex Tools hub:</p> <ol> <li><code>arxiv</code>: A tool that queries the arXiv repository of papers.</li> <li><code>tavily_research</code>: A tool that queries the web using Tavily.</li> <li><code>kuzu</code>: A tool that queries the Kuzu graph database.</li> <li><code>waii</code>: A tool for querying databases with natural language.</li> <li><code>exa</code>: A tool that uses EXA.AI search.</li> <li><code>brave_search</code>: A tool that uses Brave Search.</li> <li><code>bing_search</code>: A tool that uses Bing Search.</li> <li><code>wikipedia</code>: A tool that searches content from Wikipedia pages.</li> <li><code>neo4j</code>: A tool that queries a Neo4J graph database.</li> <li><code>google</code>: A set of tools that interact with Google services,     including Gmail, Google Calendar, and Google Search.</li> <li><code>slack</code>: A tool that interacts with Slack.</li> <li><code>salesforce</code>: A tool that queries Salesforce.</li> </ol>"},{"location":"tools/#human-readable-tool-output","title":"Human-Readable Tool Output","text":"<p>Tools can return outputs that provide both raw data (for programmatic use) and human-readable formatted output (for display to users or when computing Factual Consistency Score). This feature allows tools to define their own presentation layer while maintaining access to the underlying data structure.</p>"},{"location":"tools/#using-create_human_readable_output","title":"Using create_human_readable_output","text":"<p>The simplest way to create human-readable output is using the <code>create_human_readable_output</code> utility:</p> <pre><code>from vectara_agentic.tool_utils import create_human_readable_output\nfrom vectara_agentic.tools import ToolsFactory\n\ndef my_tool(query: str):\n    \"\"\"Example tool that returns structured data.\"\"\"\n    raw_data = {\n        \"query\": query,\n        \"results\": [\n            {\"id\": 1, \"title\": \"Result 1\", \"score\": 0.95},\n            {\"id\": 2, \"title\": \"Result 2\", \"score\": 0.87}\n        ]\n    }\n\n    # Define custom formatting function\n    def format_results(data):\n        formatted = f\"Query: {data['query']}\\n\\n\"\n        formatted += \"Results:\\n\"\n        for result in data['results']:\n            formatted += f\"- {result['title']} (score: {result['score']})\\n\"\n        return formatted\n\n    # Return human-readable output\n    return create_human_readable_output(raw_data, format_results)\n\n# Create tool\nfactory = ToolsFactory()\ntool = factory.create_tool(my_tool)\n</code></pre>"},{"location":"tools/#using-built-in-formatters","title":"Using Built-in Formatters","text":"<p>Several built-in formatters are available for common data types:</p> <pre><code>from vectara_agentic.tool_utils import (\n    create_human_readable_output, \n    format_as_table, \n    format_as_json, \n    format_as_markdown_list\n)\n\ndef data_tool():\n    \"\"\"Tool that returns tabular data.\"\"\"\n    data = [\n        {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"},\n        {\"name\": \"Bob\", \"age\": 25, \"city\": \"Boston\"}\n    ]\n\n    # Use built-in table formatter\n    return create_human_readable_output(data, format_as_table)\n</code></pre>"},{"location":"tools/#examples-in-the-codebase","title":"Examples in the Codebase","text":"<ul> <li>RAG Tool: Formats citations.</li> <li>Search Tool: Displays results in sequential format with summaries and sample matches</li> </ul> <p>This pattern provides flexibility for tools to define their own presentation layer while maintaining access to the underlying data structure.</p>"},{"location":"tools/#vhc-eligibility","title":"VHC Eligibility","text":"<p><code>VHC</code> or Vectara Hallucination Corrector refers to a specialized model from Vectara for detecting and correcting hallucinations. VHC eligibility controls which tools contribute context to VHC processing. This feature ensures that only tools that output factual information that is used as context to generate a response are considered when evaluating whether a response is hallucinated.</p>"},{"location":"tools/#understanding-vhc-eligibility","title":"Understanding VHC Eligibility","text":"<p>VHC-eligible tools (<code>vhc_eligible=True</code>, default) are those that provide factual information:</p> <ul> <li>RAG tools (<code>create_rag_tool</code>)</li> <li>Search tools (<code>create_search_tool</code>) </li> <li>Data retrieval tools (API calls, database queries)</li> <li>Information lookup tools</li> </ul> <p>Non-VHC-eligible tools (<code>vhc_eligible=False</code>) are utility tools that process or transform content:</p> <ul> <li>Text summarization tools</li> <li>Text rephrasing tools</li> <li>Content formatting tools</li> <li>Validation tools</li> <li>Navigation tools (URL generation)</li> </ul>"},{"location":"tools/#built-in-tool-vhc-eligibility","title":"Built-in Tool VHC Eligibility","text":"Tool Category VHC Eligible Examples Vectara RAG/Search \u2705 Yes All RAG and search tools Standard Tools \u274c No <code>summarize_text</code>, <code>rephrase_text</code>, <code>critique_text</code> Guardrail Tools \u274c No <code>get_bad_topics</code> Database Tools \u2705 Yes All database tools (provide factual data)"},{"location":"tools/#setting-vhc-eligibility","title":"Setting VHC Eligibility","text":""},{"location":"tools/#for-vectara-tools","title":"For Vectara Tools","text":"<pre><code>from vectara_agentic.tools import VectaraToolFactory\n\nvec_factory = VectaraToolFactory()\n\n# RAG tool (VHC-eligible by default)\nrag_tool = vec_factory.create_rag_tool(\n    tool_name=\"ask_documents\",\n    tool_description=\"Query documents for information\",\n    vhc_eligible=True  # Explicit, but this is the default\n)\n\n# Search tool marked as non-VHC-eligible (uncommon)\nsearch_tool = vec_factory.create_search_tool(\n    tool_name=\"list_documents\", \n    tool_description=\"List matching documents\",\n    vhc_eligible=False  # Override default for special cases\n)\n</code></pre>"},{"location":"tools/#for-custom-tools","title":"For Custom Tools","text":"<pre><code>from vectara_agentic.tools import ToolsFactory\n\nfactory = ToolsFactory()\n\n# Data retrieval tool - should participate in VHC\ndef get_stock_price(ticker: str) -&gt; float:\n    \"\"\"Get current stock price for a ticker.\"\"\"\n    # API call to get real data\n    return 150.25\n\nstock_tool = factory.create_tool(get_stock_price, vhc_eligible=True)\n\n# Utility tool - should not participate in VHC  \ndef format_currency(amount: float) -&gt; str:\n    \"\"\"Format amount as currency string.\"\"\"\n    return f\"${amount:,.2f}\"\n\nformat_tool = factory.create_tool(format_currency, vhc_eligible=False)\n</code></pre>"},{"location":"tools/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Default Behavior: Most tools should use the default <code>vhc_eligible=True</code> unless they're pure utility functions</p> </li> <li> <p>Utility Tools: Mark tools as <code>vhc_eligible=False</code> if they:</p> </li> <li> <p>Transform or reformat existing data</p> </li> <li>Perform validation or checks</li> <li>Generate URLs or links</li> <li> <p>Provide metadata about other tools</p> </li> <li> <p>Content Tools: Keep <code>vhc_eligible=True</code> (default) for tools that:</p> </li> <li> <p>Retrieve data from APIs or databases</p> </li> <li>Search or query information sources</li> <li> <p>Return factual content</p> </li> <li> <p>Consistency: Use consistent VHC eligibility within tool categories to ensure predictable VHC behavior</p> </li> </ol>"},{"location":"usage/","title":"Usage","text":"<p>Let's walk through a complete example of creating an AI assistant using vectara-agentic. We will build a finance assistant that can answer questions about the annual financial reports for Apple Computer, Google, Amazon, Snowflake, Atlassian, Tesla, Nvidia, Microsoft, Advanced Micro Devices, Intel, and Netflix between the years 2020 and 2024.</p>"},{"location":"usage/#import-dependencies","title":"Import Dependencies","text":"<p>First, we must import some libraries and define some constants for our demo.</p> <pre><code>import os\nfrom dotenv import load_dotenv\nimport streamlit as st\nimport pandas as pd\nimport requests\nfrom pydantic import Field\n\nload_dotenv(override=True)\n</code></pre> <p>We then use the <code>load_dotenv</code> function to load our environment variables from a <code>.env</code> file.</p>"},{"location":"usage/#create-tools","title":"Create Tools","text":"<p>Next, we will create the tools for our agent.</p> <p>There are three categories of tools you can use with vectara-agentic:</p> <ol> <li>A query tool that connects to Vectara to ask a question about data     in a Vectara corpus.</li> <li>Pre-built tools that are available out of the box, or ready to use     tools from the LlamaIndex Tools     Hub.</li> <li>Any other tool that you want to make for your agent, based on custom     code in Python.</li> </ol> <p>Vectara RAG Query Tool Let's see how to create a Vectara RAG tool. In order to use this tool, you need to create a corpus and API key with a Vectara account. In this example, we will create the <code>ask_transcripts</code> tool, which can be used to perform RAG queries on analyst call transcripts. You can see this tool in use with our Finance Assistant demo.</p> <pre><code>from pydantic import BaseModel\n\n# define the arguments schema for the tool\nclass QueryTranscriptsArgs(BaseModel):\n    year: int = Field(..., description=f\"The year. An integer between {min(years)} and {max(years)}.\")\n    ticker: str = Field(..., description=f\"The company ticker. Must be a valid ticket symbol from the list {tickers.keys()}.\")\n</code></pre> <p>Note that:</p> <ul> <li>The arguments for this tool are defined using Python's <code>pydantic</code> package with the <code>Field</code> class. By defining the tool in this   way, we provide a good description for each argument so that the agent LLM can easily understand the tool's functionality    and how to use it properly. Each argument corresponds to a metadata attribute for documents in the Vectara corpus.   The agent will provide these arguments based on the user's query to filter the matching search results returned by the RAG tool.</li> <li>The <code>query</code> argument is added automatically to the RAG tool, so you don't need to specify it here.</li> </ul> <p>You can also define an argument to support optional conditional arguments, for example:</p> <pre><code>from pydantic import BaseModel\nfrom typing import Optional\n\n# define the arguments schema for the tool\nclass QueryTranscriptsArgs(BaseModel):\n    year: Optional[int | str] = Field(\n        description=f\"The year this query relates to. An integer between {min(years)} and {max(years)} or a string specifying a condition on the year\",\n        examples=[2020, '&gt;2021', '&lt;2023', '&gt;=2021', '&lt;=2023', '[2021, 2023]', '[2021, 2023)']\n    )\n    ticker: str = Field(..., description=f\"The company ticker. Must be a valid ticket symbol from the list {tickers.keys()}.\")\n</code></pre> <p>With this change for the <code>year</code> argument, we are telling the agent that both an int value (e.g. 2022) or a string value (e.g. '&gt;2022' or '&lt;2022') are valid inputs for this argument. You can also use range filters (e.g. '[2021, 2023]') to specify a range of years. If a string value is provided, <code>vectara-agentic</code> knows how to parse it properly in the backend and set a metadata filter with the right condition for Vectara.</p> <p>Now to create the actual tool, we use the <code>create_rag_tool()</code> method from the <code>VectaraToolFactory</code> class as follows:</p> <pre><code>from vectara_agentic.tools import VectaraToolFactory\n\nvec_factory = VectaraToolFactory(vectara_api_key=vectara_api_key,\n                                 vectara_corpus_key=vectara_corpus_key)\n\nask_transcripts = vec_factory.create_rag_tool(\n    tool_name = \"ask_transcripts\",\n    tool_description = \"\"\"\n    Given a company name and year,\n    returns a response (str) to a user question about a company, based on analyst call transcripts about the company's financial reports for that year.\n    You can ask this tool any question about the company including risks, opportunities, financial performance, competitors and more.\n    Make sure to provide the a valid company ticker and year.\n    \"\"\",\n    tool_args_schema = QueryTranscriptsArgs,\n    tool_args_type = {\n      \"year\": \"doc\",\n      \"ticker\": \"doc\"\n    },\n    reranker = \"chain\", rerank_k = 100,\n    rerank_chain = [\n      {\n        \"type\": \"slingshot\"\n      },\n      {\n        \"type\": \"userfn\",\n        \"user_function\": \"knee()\"\n      }\n      {\n        \"type\": \"mmr\",\n        \"diversity_bias\": 0.1\n      }\n    ],\n    n_sentences_before = 2, n_sentences_after = 2, lambda_val = 0.005,\n    summary_num_results = 10,\n    vectara_summarizer = 'vectara-summary-ext-24-05-med-omni',\n    include_citations = False,\n    fcs_threshold = 0.2,\n    vhc_eligible = True\n)\n</code></pre> <p>In the code above, we did the following:</p> <ul> <li>First, we initialized the <code>VectaraToolFactory</code> with the Vectara     corpus key and API key. If you don't want to explicitly pass in     these arguments, you can specify them in your environment as     <code>VECTARA_CORPUS_KEY</code> and <code>VECTARA_API_KEY</code>. Additionally, you can     also create a single <code>VectaraToolFactory</code> that queries multiple     corpora. This may be helpful if you have related information across     multiple corpora in Vectara. To do this, create a query API key on     the     Authorization     page and give it to access to all the corpora you want for this     query tool. When specifying your environment variables, set     <code>VECTARA_CORPUS_KEY</code> to a list of corpus keys separated by commas     (e.g. <code>corpus_key1,corpus_key2,corpus_key3</code>).</li> <li>Then we called <code>create_rag_tool()</code>, specifying the tool name,     description and schema for the tool, followed by various optional     parameters to control the Vectara RAG query tool. Notice that we     also specified the type of each additional argument in the schema with the <code>tool_args_type</code> parameter.     The type of each argument can be <code>\"doc\"</code> or <code>\"part\"</code>, corresponding     to whether the metadata argument is document metadata or part     metadata in the Vectara corpus. See this     page     on metadata for more information.</li> </ul> <p>One important parameter to point out is <code>fcs_threshold</code>. This allows you to specify a minimum factual consistency score (between 0 and 1) for the response to be considered a \"good\" response. If the generated response has an <code>FCS</code> below this threshold, the agent will not use the generated summary (considering it a hallucination). You can think of this as a hallucination guardrail. The higher you set <code>fcs_threshold</code>, the stricter your guardrail will be.</p> <p>If your agent continuously rejects all of the generated responses, consider lowering the threshold.</p> <p>Another important parameter is <code>reranker</code>. In this example, we are using a chain reranker, which chains together multiple reranking methods to achieve better control over the reranking and combines the strengths of various reranking methods. In the example above, we use the multilingual (or slingshot) reranker followed by a user-defined function (the knee reranker), and finally the MMR reranker with a diversity bias of 0.1. You can also supply other parameters to each reranker, such as a <code>cutoff</code> parameter, which removes documents that have scores below this threshold value after applying the given reranker. Lastly, you can add a user defined function reranker as the last reranker in the chain to specify a customized expression to rerank results in a way that is relevant to your specific application. If you want to learn more about reranking tips and best practices, check out our blog posts on user defined functions and knee reranking as well as this example notebook on user defined functions for some guidance and inspiration.</p> <p>That's it: now the <code>ask_transcripts</code> tool is ready to be added to the agent.</p> <p>Notes:</p> <ul> <li>You can use the <code>VectaraToolFactory</code> to generate more than one RAG tool with different parameters, depending on your needs.</li> <li><code>create_rag_tool</code> and <code>create_search_tool</code> both support the <code>vectara_base_url</code>    argument. If specified, it allows you to specify a different base URL for Vectara,   for example when you have an on-premise installation.</li> <li>If you want to specify a Certificate Authority for a local installation,   you can set <code>\"export REQUESTS_CA_BUNDLE=/path/to/custom_ca_bundle.pem\"</code>in your environment.</li> </ul> <p>Vectara Search Tool In most cases, you will likely want to use the Vectara RAG query tool, which generates a summary to return to the agent along with the source text and documents used to generate that summary.</p> <p>In some applications, you may want the tool to only retrieve the actual text/documents that best match the query rather than summarizing all of the results. For example, you may ask your agent \"How many documents mention information about tax laws and regulations?\". The agent will be able to get a list of documents from your Vectara corpus and analyze the results to answer your question.</p> <p>You can also get a summary of each retrieved document by specifying <code>summarize_docs=True</code> when creating your search function.</p> <p>Metadata Filtering In most cases, you will want to use the <code>tool_args_schema</code> to define the metadata fields used in your Vectara RAG or Search tool. Defining your parameters in this way allows the agent to interpret the user query and determine if any of these filters should be applied on that particular query.</p> <p>In some instances you may want to have a metadata filter that applies in every call to a Vectara RAG or search tool. For example, you may want to enforce that the oldest possible search results are from 2022. In this case, you can use the <code>fixed_filter</code> parameter to the <code>create_rag_tool()</code> or <code>create_search_tool()</code> functions.</p> <p>In our example where we want all results to be from 2022 and later, we would specify <code>fixed_filter = \"doc.year &gt;= 2022\"</code>.</p> <p>Make sure that these fields are defined as filter attributes in your Vectara corpus. See this page for more information about metadata and filter attributes.</p> <p>Additional Tools To generate non-RAG tools, you can use the <code>ToolsFactory</code> class, which provides some out-of-the-box tools that you might find helpful when building your agents, as well as an easy way to create custom tools.</p> <p>Currently, we have a few tool groups you may want to consider using:</p> <ul> <li><code>standard_tools()</code>: These are basic tools that can be helpful, and     include the <code>summarize_text</code> tool and <code>rephrase_text</code> tool.</li> <li><code>finance_tools()</code>: includes a set of financial query tools based on     Yahoo! finance.</li> <li><code>legal_tools()</code>: These tools are designed to help with legal     queries, and include <code>critique_as_judge</code> and <code>summarize_legal_text</code>.</li> <li><code>database_tools()</code>: tools to explore SQL databases and make queries     based on user prompts.</li> <li><code>guardrail_tools()</code>: These tools are designed to help the agent     avoid certain topics that may be inappropriate or controversial in its responses.</li> </ul> <p>For example, to get access to all the legal tools, you can use the following:</p> <pre><code>from vectara_agentic.tools import ToolsFactory\n\nlegal_tools = ToolsFactory().legal_tools()\n</code></pre> <p>For more details about these and other tools, see Tools.</p> <p>Create your own tool You can also create your own tool directly by defining a Python function:</p> <pre><code>import numpy as np\n\ndef earnings_per_share(\n  net_income: float = Field(description=\"the net income for the company\"),\n  number_of_shares: float = Field(description=\"the number of oustanding shares\"),\n) -&gt; float:\n    \"\"\"\n    This tool returns the EPS (earnings per share).\n    \"\"\"\n    return np.round(net_income / number_of_shares,4)\n\nmy_tool = tools_factory.create_tool(earnings_per_share)\n</code></pre> <p>A few important things to note:</p> <ol> <li>A tool may accept any type of argument (e.g. float, int) and return     any type of value (e.g. float). The <code>create_tool()</code> method will     handle the conversion of the arguments and response into strings     (which is type the agent expects).</li> <li>It is important to define a clear and concise docstring for your     tool. This will help the agent understand what the tool does and how     to use it.</li> </ol> <p>Here are some functions we will define for our finance assistant example:</p> <pre><code>tickers = {\n  \"AAPL\": \"Apple Computer\", \n  \"GOOG\": \"Google\", \n  \"AMZN\": \"Amazon\",\n  \"SNOW\": \"Snowflake\",\n  \"TEAM\": \"Atlassian\",\n  \"TSLA\": \"Tesla\",\n  \"NVDA\": \"Nvidia\",\n  \"MSFT\": \"Microsoft\",\n  \"AMD\": \"Advanced Micro Devices\",\n  \"INTC\": \"Intel\",\n  \"NFLX\": \"Netflix\",\n}\nyears = [2020, 2021, 2022, 2023, 2024]\n\ndef get_company_info() -&gt; list[str]:\n\"\"\"\nReturns a dictionary of companies you can query about. Always check this before using any other tool.\nThe output is a dictionary of valid ticker symbols mapped to company names.\nYou can use this to identify the companies you can query about, and their ticker information.\n\"\"\"\nreturn tickers\n\ndef get_valid_years() -&gt; list[str]:\n\"\"\"\nReturns a list of the years for which financial reports are available.\nAlways check this before using any other tool.\n\"\"\"\nreturn years\n\n# Tool to get the income statement for a given company and year using the FMP API\ndef get_income_statement(\nticker=Field(description=\"the ticker symbol of the company.\"),\nyear=Field(description=\"the year for which to get the income statement.\"),\n) -&gt; str:\n\"\"\"\nGet the income statement for a given company and year using the FMP (https://financialmodelingprep.com) API.\nReturns a dictionary with the income statement data. All data is in USD, but you can convert it to more compact form like K, M, B.\n\"\"\"\nfmp_api_key = os.environ.get(\"FMP_API_KEY\", None)\nif fmp_api_key is None:\n   return \"FMP_API_KEY environment variable not set. This tool does not work.\"\nurl = f\"https://financialmodelingprep.com/api/v3/income-statement/{ticker}?apikey={fmp_api_key}\"\nresponse = requests.get(url)\nif response.status_code == 200:\n   data = response.json()\n   income_statement = pd.DataFrame(data)\n   income_statement[\"date\"] = pd.to_datetime(income_statement[\"date\"])\n   income_statement_specific_year = income_statement[\n     income_statement[\"date\"].dt.year == int(year)\n   ]\n   values_dict = income_statement_specific_year.to_dict(orient=\"records\")[0]\n   return f\"Financial results: {', '.join([f'{key}: {value}' for key, value in values_dict.items() if key not in ['date', 'cik', 'link', 'finalLink']])}\"\nelse:\n   return \"FMP API returned error. This tool does not work.\"\n</code></pre> <p>The <code>get_income_statement()</code> tool utilizes the FMP API to get the income statement for a given company and year. Notice how the tool description is structured. We describe each of the expected arguments to the function using pydantic's <code>Field</code> class. The function description only describes to the agent what the function does and how the agent should use the tool. This function definition follows best practices for defining tools. You should make this description detailed enough so that your agent knows when to use each of your tools.</p> <p>You can define your tool as an individual Python function (as shown above) or as a method in a Python class. It may be helpful to define all of your tools (Vectara tools, other pre-built tools, and your custom tools) in a single AgentTools class. Please note that you cannot define a tool as a function within another tool. Each tool must be a separate Python function.</p> <p>Your tools should also handle any exceptions gracefully by returning an <code>Exception</code> or a string describing the failure. The agent can interpret that string and then decide how to deal with the failure (either calling another tool to accomplish the task or telling the user that their request was unable to be processed).</p> <p>Finally, notice that we have used snake_case for all of our function names. While this is not required, it's a best practice that we recommend for you to follow.</p> <p>VHC Eligibility for Custom Tools</p> <p>When creating custom tools, you should consider whether they should participate in VHC (Vectara Hallucination Correction) analysis. To learn more about this feature, reference the VHC Eligibility section.</p> <p>For example, the <code>get_financial_data()</code> tool defined below should use VHC because it provides factual financial information about a stock. On the other hand, <code>format_financial_report()</code> should not use VHC because it is simply used to create a structured output from another tool.</p> <pre><code>def get_financial_data(ticker: str) -&gt; dict:\n    \"\"\"Retrieve financial data for a company.\"\"\"\n    # API call to get real financial data\n    return {\"revenue\": 1000000, \"profit\": 50000}\n\ndef format_financial_report(data: dict) -&gt; str:\n    \"\"\"Format financial data into a readable report.\"\"\"\n    return f\"Revenue: ${data['revenue']:,}, Profit: ${data['profit']:,}\"\n\ndata_tool = tools_factory.create_tool(get_financial_data, vhc_eligible=True)\nformat_tool = tools_factory.create_tool(format_financial_report, vhc_eligible=False)\n</code></pre> <p>Computing Vectara Hallucination Correction (VHC)</p> <p>After your agent generates a response, you can compute VHC to analyze and correct any detected hallucinations:</p> <pre><code># Chat with the agent first\nresponse = agent.chat(\"What was Apple's revenue in 2022?\")\nprint(response.response)\n\n# Compute VHC analysis\nvhc_result = agent.compute_vhc()\n\n# Access results\nif vhc_result[\"corrected_text\"]:\n    print(\"Original response:\", response.response)\n    print(\"Corrected response:\", vhc_result[\"corrected_text\"])\n    print(\"Detected corrections:\", vhc_result[\"corrections\"])\nelse:\n    print(\"No corrections needed or VHC not available\")\n</code></pre> <p>For async applications, use the async version:</p> <pre><code># Async chat and VHC computation\nresponse = await agent.achat(\"What was Apple's revenue in 2022?\")\nvhc_result = await agent.acompute_vhc()\n</code></pre> <p>VHC Requirements: - Requires a valid <code>VECTARA_API_KEY</code> environment variable - Only VHC-eligible tools contribute factual content for the analysis - Results are cached to avoid redundant computation for the same query/response pair</p>"},{"location":"usage/#initialize-the-agent","title":"Initialize The Agent","text":"<p>Now that we have our tools, let's create the agent, using the following arguments:</p> <ol> <li><code>tools: list[FunctionTool]</code>: A list of tools that the agent will use     to interact with information and apply actions. For any tools you     create yourself, make sure to pass them to the <code>create_tool()</code>     method of your <code>ToolsFactory</code> object.</li> <li><code>topic: str = \"general\"</code>: This is simply a string (should be a noun)     that is used to identify the agent's area of expertise. For our     example we set this to <code>financial analyst</code>.</li> <li><code>custom_instructions: str = \"\"</code>: This is a set of instructions that     the agent will follow. These instructions should not tell the agent     what your tools do (that's what the tool descriptions are for) but     rather any particular behavior you want your LLM to have, such as     how to present the information it receives from the tools to the     user.</li> <li><code>agent_config: Optional[AgentConfig] = None</code>: the agent configuration     See below for more details. If unspecified, defaults are used.</li> <li><code>fallback_agent_config: Optional[AgentConfig] = None</code>: configuration     for a fallback_agent. If specified, this will get activated if the     main agent API is not responding (e.g. when inference enpoint is down).     If unspecified, no fallback agent is assumed.</li> <li><code>agent_progress_callback: Optional[Callable[[AgentStatusType, dict, str], None]] = None</code>:     This is an optional callback function that will be called on every     agent step (see below)</li> <li><code>query_logging_callback: Optional[Callable[[str, str], None]] = None</code>:     This is an optional callback function that will be called at the end     of response generation, with the query and response strings.</li> <li><code>validate_tools: bool = False</code>: whether to validate tool inconsistency      with instructions.</li> </ol> <p>Every agent has its own default set of instructions that it follows to interpret users' messages and use the necessary tools to complete its task. However, we can (and often should) define custom instructions (via the <code>custom_instructions</code> argument) for our AI assistant. Here are some guidelines to follow when creating your instructions:</p> <ul> <li>Write precise and clear instructions without overcomplicating the     agent.</li> <li>Consider edge cases and unusual or atypical scenarios.</li> <li>Be cautious to not over-specify behavior based on your primary use     case as this may limit the agent's ability to behave properly in     other situations.</li> </ul> <p>Here are the instructions we are using for our financial AI assistant:</p> <pre><code>financial_assistant_instructions = \"\"\"\n  - You are a helpful financial assistant, with expertise in financial reporting, in conversation with a user.\n  - Never discuss politics, and always respond politely.\n  - Respond in a compact format by using appropriate units of measure (e.g., K for thousands, M for millions, B for billions).\n  - Do not report the same number twice (e.g. $100K and 100,000 USD).\n  - Always check the get_company_info and get_valid_years tools to validate company and year are valid.\n  - When querying a tool for a numeric value or KPI, use a concise and non-ambiguous description of what you are looking for.\n  - If you calculate a metric, make sure you have all the necessary information to complete the calculation. Don't guess.\n\"\"\"\n</code></pre> <p>Notice how these instructions are different from the tool function descriptions. These instructions are general rules that the agent should follow. At times, these instructions may refer to specific tools, but in general, the agent should be able to decide for itself what tools it should call. This is what makes agents very powerful and makes our job as coders much simpler.</p> <p>agent_progress_callback callback The <code>agent_progress_callback</code> is an optional <code>Callable</code> function that can serve a variety of purposes for your assistant. It is a callback function that is managed by the agent, and it will be called anytime the agent is updated, such as when calling a tool, or when receiving a response from a tool. This works with both regular chat methods (<code>chat()</code>, <code>achat()</code>) and  streaming methods (<code>stream_chat()</code>, <code>astream_chat()</code>).</p> <p>In our example, we will use it to log the actions of our agent so users can see the steps the agent is taking as it answers their questions. Since our assistant is using streamlit to display the results, we will append the log messages to the session state.</p> <pre><code>from vectara_agentic.agent import AgentStatusType\n\ndef agent_progress_callback(status_type: AgentStatusType, msg: dict, event_id: str):\n  output = f\"{status_type.value} - {msg}\"\n  st.session_state.log_messages.append(output)\n</code></pre> <p>Note: version 0.3.0 introduces a breaking change in <code>agent_progress_callback</code>,  instead of the previous <code>msg</code> argument that was a string, it now returns a dictionary that provides more detailed and easier to handle information.</p> <p>agent_config The <code>agent_config</code> argument is an optional object that you can use to explicitly specify the configuration of your agent, including the following:</p> <ul> <li><code>agent_type</code>: the agent type. Valid values are <code>FUNCTION_CALLING</code> or <code>REACT</code> (default: <code>FUNCTION_CALLING</code>).</li> <li><code>main_llm_provider</code> and <code>tool_llm_provider</code>: the LLM provider for main agent and for the tools. Valid values are <code>OPENAI</code>, <code>ANTHROPIC</code>, <code>TOGETHER</code>, <code>GROQ</code>, <code>COHERE</code>, <code>BEDROCK</code>, <code>GEMINI</code> (default: <code>OPENAI</code>).</li> </ul> <p>Note: Fireworks AI support has been removed. If you were using Fireworks, please migrate to one of the supported providers listed above. - <code>main_llm_model_name</code> and <code>tool_llm_model_name</code>: agent model name for agent and tools (default depends on provider: OpenAI uses gpt-4.1-mini, Gemini uses gemini-2.5-flash-lite). - <code>observer</code>: the observer type; should be <code>ARIZE_PHOENIX</code> or if undefined no observation framework will be used. - <code>endpoint_api_key</code>: a secret key if using the API endpoint option (defaults to <code>dev-api-key</code>)</p> <p>By default, each of these parameters will be read from your environment, but you can also explicitly define them with the <code>AgentConfig</code> class.</p> <p>For example, here is how we can define an <code>AgentConfig</code> object to create a <code>ReAct</code> agent using <code>OPENAI</code> as the LLM for the agent and <code>Cohere</code> as the LLM for the agent's tools:</p> <pre><code>from vectara_agentic.agent_config import AgentConfig\n\nconfig = AgentConfig(\n  agent_type=\"REACT\",\n  main_llm_provider=\"OPENAI\",\n  tool_llm_provider=\"COHERE\"\n)\n</code></pre> <p>Creating the agent Here is how we will instantiate our finance assistant:</p> <pre><code>from vectara_agentic import Agent\n\nagent = Agent(\n     tools=[\n         tools_factory.create_tool(get_company_info, vhc_eligible=False),\n         tools_factory.create_tool(get_valid_years, vhc_eligible=False),\n         tools_factory.create_tool(get_income_statement, vhc_eligible=True)\n     ] +\n     tools_factory.standard_tools() +\n     tools_factory.financial_tools() +\n     tools_factory.guardrail_tools() +\n     [ask_transcripts],\n     topic=\"10-K annual financial reports\",\n     custom_instructions=financial_assistant_instructions,\n     agent_progress_callback=agent_progress_callback\n)\n</code></pre> <p>Notice that when we call the <code>create_tool()</code> method, we can specify:</p> <ul> <li><code>tool_type</code>: Either <code>\"query\"</code> (default) or <code>\"action\"</code></li> <li><code>vhc_eligible</code>: Whether the tool should participate in VHC analysis (default <code>True</code>)</li> </ul> <p>In our example, we explicitly set <code>vhc_eligible=False</code> for utility tools like <code>get_company_info</code> and <code>get_valid_years</code> since they provide metadata rather than factual content for responses. The <code>get_income_statement</code> tool is marked <code>vhc_eligible=True</code> since it provides actual financial data.</p>"},{"location":"usage/#chat-with-your-assistant","title":"Chat with your Assistant","text":"<p>Once you have created your agent, using it is quite simple. All you have to do is call its <code>chat()</code> method, which prompts your agent to answer the user's query using its available set of tools. It's that easy.</p> <pre><code>query = \"Which 3 companies had the highest revenue in 2022, and how did they do in 2021?\"\nprint(str(agent.chat(query)))\n</code></pre> <p>The agent returns the response:</p> <p>The three companies with the highest revenue in 2022 were:</p> <ol> <li>Amazon (AMZN): $513.98B</li> <li>Apple (AAPL): $394.33B</li> <li>Google (GOOG): $282.84B</li> </ol> <p>Their revenues in 2021 were:</p> <ol> <li>Amazon (AMZN): $469.82B</li> <li>Apple (AAPL): $365.82B</li> <li>Google (GOOG): $257.64B</li> </ol> <p>The <code>chat()</code> function returns an <code>AgentResponse</code> object, which includes the agent's generated response text and a list of <code>ToolOutput</code> objects. The agent's response text can easily be retrieved <code>response</code> member (or simply by using <code>str()</code>). The tool information can be extracted with the <code>sources</code> member of the <code>AgentResponse</code> class and will return a list of tool outputs, including the name of each tool that was called and the output from that tool that was given to the agent.</p> <p>To make a full Streamlit app, there is some extra code that is necessary to configure the demo layout. You can check out the full code and demo for this app on Hugging Face.</p>"},{"location":"usage/#chat-interaction-methods","title":"Chat Interaction Methods","text":"<p><code>vectara-agentic</code> provides multiple ways to interact with your agent depending on your needs:</p>"},{"location":"usage/#standard-chat-methods","title":"Standard Chat Methods","text":"<p>Synchronous Chat</p> <pre><code># Simple chat that blocks until complete\nresponse = agent.chat(\"What was Apple's revenue in 2022?\")\nprint(response.response)\n</code></pre> <p>Asynchronous Chat</p> <pre><code># Non-blocking chat for async applications\nresponse = await agent.achat(\"What was Apple's revenue in 2022?\")  \nprint(response.response)\n</code></pre> <p>Both methods return an <code>AgentResponse</code> object with the complete response text and metadata.</p>"},{"location":"usage/#streaming-chat-methods","title":"Streaming Chat Methods","text":"<p>For better user experience with long responses, use streaming methods that provide real-time updates:</p> <p>Synchronous Streaming</p> <pre><code>stream_response = agent.stream_chat(\"Compare Apple and Google's revenue growth over 5 years\")\nasync for chunk in stream_response.async_response_gen():\n    print(chunk, end=\"\", flush=True)\n\n# Get final response with metadata after streaming\nfinal_response = stream_response.get_response()\nprint(f\"\\nSources used: {len(final_response.sources)}\")\n</code></pre> <p>Asynchronous Streaming</p> <pre><code>stream_response = await agent.astream_chat(\"Analyze market trends for tech companies\")\n\n# Process chunks asynchronously\nasync for chunk in stream_response.async_response_gen():\n    # Update async UI components\n    await update_ui_async(chunk)\n\n# Get final response asynchronously\nfinal_response = await stream_response.aget_response() \nprint(f\"Analysis complete: {final_response.response}\")\n</code></pre>"},{"location":"usage/#tool-call-progress-tracking","title":"Tool Call Progress Tracking","text":"<p>When agents use tools (like RAG retrieval, calculations, or external APIs), you can track these operations in real-time using <code>agent_progress_callback</code> with both streaming and non-streaming methods:</p> <pre><code>from vectara_agentic import AgentStatusType\n\ndef tool_progress_tracker(status_type, msg, event_id):\n    \"\"\"Track tool calls and outputs during streaming\"\"\"\n    if status_type == AgentStatusType.TOOL_CALL:\n        print(f\"\ud83d\udd27 Calling tool: {msg['tool_name']}\")\n        print(f\"   Arguments: {msg['arguments']}\")\n    elif status_type == AgentStatusType.TOOL_OUTPUT:\n        print(f\"\ud83d\udcca Tool '{msg['tool_name']}' completed\")\n        print(f\"   Result: {msg['content'][:100]}...\")\n\n# Create agent with progress tracking\nagent = Agent(\n    tools=[vectara_tools, calculation_tools],\n    agent_progress_callback=tool_progress_tracker\n)\n\n# With streaming - see tool calls AND streaming response\nstream_response = await agent.astream_chat(\n    \"Analyze Apple's financial performance and calculate growth rates\"\n)\n\n# With regular chat - see tool calls, then get final response  \nresponse = await agent.achat(\n    \"Analyze Apple's financial performance and calculate growth rates\"\n)\n\n# You'll see tool calls as they happen in both cases:\n# \ud83d\udd27 Calling tool: query_financial_data\n#    Arguments: {\"query\": \"Apple financial performance\"}\n# \ud83d\udcca Tool 'query_financial_data' completed\n#    Result: Apple Inc. reported revenue of $394.33 billion...\n# \ud83d\udd27 Calling tool: calculate_growth_rate\n#    Arguments: {\"current\": 394.33, \"previous\": 365.82}\n\nasync for chunk in stream_response.async_response_gen():\n    print(chunk, end=\"\", flush=True)  # Response text streams here\n\nfinal_response = await stream_response.aget_response()\n</code></pre>"},{"location":"usage/#agentstreamingresponse-features","title":"AgentStreamingResponse Features","text":"<p>The <code>AgentStreamingResponse</code> object provides several useful capabilities:</p> <ol> <li>Real-time streaming: Get response chunks as they're generated</li> <li>Tool call visibility: Combined with progress callbacks, see tool usage in real-time</li> <li>Final response access: Retrieve the complete response and metadata after streaming</li> <li>Flexible consumption: Choose between automatic printing or manual processing</li> <li>Async/sync compatibility: Works in both synchronous and asynchronous contexts</li> </ol>"},{"location":"usage/#when-to-use-each-method","title":"When to Use Each Method","text":"<ul> <li>Use <code>chat()</code> or <code>achat()</code> for:</li> <li>Simple, short queries</li> <li>Batch processing scenarios</li> <li> <p>When you only need the final result</p> </li> <li> <p>Use <code>stream_chat()</code> or <code>astream_chat()</code> for:</p> </li> <li>Long-running queries that generate substantial responses</li> <li>Interactive UIs where users want to see progress</li> <li>Web applications with real-time updates</li> <li>Better perceived performance and user engagement</li> </ul>"},{"location":"usage/#combining-streaming-with-callbacks","title":"Combining Streaming with Callbacks","text":"<p>You can use both streaming and progress callbacks together for comprehensive monitoring:</p> <pre><code>def progress_callback(status_type, msg, event_id):\n    print(f\"[{status_type.value}] {msg}\")\n\nagent = Agent(\n    tools=[...],\n    topic=\"financial reports\", \n    custom_instructions=\"...\",\n    agent_progress_callback=progress_callback  # Track tool calls\n)\n\n# This will show both tool progress AND streaming response\nstream_response = agent.stream_chat(\"What were Apple's key financial metrics in 2022?\")\nasync for chunk in stream_response.async_response_gen():\n    print(chunk, end=\"\", flush=True)\n</code></pre> <p>This approach gives you visibility into both the agent's internal tool usage and the streaming response generation.</p>"},{"location":"usage/#practical-examples-callbacks-vs-streaming","title":"Practical Examples: Callbacks vs Streaming","text":"<p>Here are real-world scenarios showing when to use callbacks, streaming, or both:</p>"},{"location":"usage/#example-1-simple-chat-application","title":"Example 1: Simple Chat Application","text":"<p>Callback-Only Approach (good for debugging/logging):</p> <pre><code>def simple_progress_callback(status_type, msg, event_id):\n    if status_type == AgentStatusType.TOOL_CALL:\n        print(f\"\ud83d\udd27 Using tool: {msg['tool_name']}\")\n    elif status_type == AgentStatusType.TOOL_OUTPUT:\n        print(f\"\ud83d\udcca Tool result ready\")\n\nagent = Agent(\n    tools=[ask_finance],\n    topic=\"financial reports\",\n    agent_progress_callback=simple_progress_callback\n)\n\n# User sees tool usage but waits for complete response\nresponse = agent.chat(\"What was Apple's revenue growth in 2022?\")\nprint(f\"Final answer: {response.response}\")\n</code></pre> <p>Streaming-Only Approach (best for user experience):</p> <pre><code>agent = Agent(tools=[ask_finance], topic=\"financial reports\")\n\n# User sees response as it's generated\nstream_response = agent.stream_chat(\"What was Apple's revenue growth in 2022?\")\nasync for chunk in stream_response.async_response_gen():\n    print(chunk, end=\"\", flush=True)\n</code></pre> <p>Combined Approach (best of both worlds):</p> <pre><code>def detailed_progress_callback(status_type, msg, event_id):\n    if status_type == AgentStatusType.TOOL_CALL:\n        print(f\"\ud83d\udd0d Searching for: {msg.get('arguments', '')}\")\n\nagent = Agent(\n    tools=[ask_finance],\n    topic=\"financial reports\", \n    agent_progress_callback=detailed_progress_callback\n)\n\n# User sees both tool progress AND streaming response\nstream_response = agent.stream_chat(\"What was Apple's revenue growth in 2022?\")\nasync for chunk in stream_response.async_response_gen():\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"usage/#example-2-web-application-with-real-time-updates","title":"Example 2: Web Application with Real-time Updates","text":"<p>FastAPI with Server-Sent Events:</p> <pre><code>from fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\n\napp = FastAPI()\n\n@app.get(\"/chat/stream\")\nasync def stream_chat_endpoint(query: str):\n    async def generate_response():\n        stream_response = await agent.astream_chat(query)\n\n        async for chunk in stream_response.async_response_gen():\n            # Send each chunk as SSE\n            yield f\"data: {json.dumps({'chunk': chunk})}\\n\\n\"\n\n        # Send final metadata when complete\n        final_response = await stream_response.aget_response()\n        yield f\"data: {json.dumps({'done': True, 'sources': len(final_response.sources)})}\\n\\n\"\n\n    return StreamingResponse(generate_response(), media_type=\"text/plain\")\n</code></pre> <p>Streamlit with Progress Updates:</p> <pre><code>import streamlit as st\n\ndef streamlit_progress_callback(status_type, msg, event_id):\n    if status_type == AgentStatusType.TOOL_CALL:\n        st.info(f\"Using tool: {msg['tool_name']}\")\n\n# Show progress in sidebar\nwith st.sidebar:\n    st.header(\"Agent Activity\")\n\nagent = Agent(\n    tools=[ask_finance],\n    topic=\"financial reports\",\n    agent_progress_callback=streamlit_progress_callback\n)\n\nif user_query := st.chat_input(\"Ask about financial data\"):\n    with st.chat_message(\"assistant\"):\n        # Stream response directly to chat\n        stream_response = agent.stream_chat(user_query)\n        response_placeholder = st.empty()\n\n        full_response = \"\"\n        async for chunk in stream_response.async_response_gen():\n            full_response += chunk\n            response_placeholder.markdown(full_response + \"\u258c\")\n\n        response_placeholder.markdown(full_response)\n</code></pre>"},{"location":"usage/#example-3-batch-processing-vs-interactive","title":"Example 3: Batch Processing vs Interactive","text":"<p>Batch Processing (callbacks for monitoring):</p> <pre><code>def batch_progress_callback(status_type, msg, event_id):\n    # Log to file for batch processing audit\n    logging.info(f\"{status_type.value}: {msg}\")\n\nquestions = [\n    \"What was Apple's 2022 revenue?\",\n    \"How did Google perform in 2022?\", \n    \"Compare Amazon's growth to Microsoft's\"\n]\n\nagent = Agent(\n    tools=[ask_finance],\n    topic=\"financial reports\",\n    agent_progress_callback=batch_progress_callback\n)\n\nresults = []\nfor question in questions:\n    response = agent.chat(question)  # No streaming needed\n    results.append({\"question\": question, \"answer\": response.response})\n</code></pre> <p>Interactive Session (streaming for engagement):</p> <pre><code>import time\n\nagent = Agent(tools=[ask_finance], topic=\"financial reports\")\n\nprint(\"Financial Assistant (type 'quit' to exit)\")\nwhile True:\n    user_input = input(\"\\n\ud83d\udcac Ask me about financial data: \")\n    if user_input.lower() == 'quit':\n        break\n\n    print(\"\\n\ud83e\udd16 Assistant:\")\n    stream_response = agent.stream_chat(user_input)\n    async for chunk in stream_response.async_response_gen():\n    print(chunk, end=\"\", flush=True)\n    print(\"\\n\" + \"\u2500\" * 50)\n</code></pre>"},{"location":"usage/#key-decision-guidelines","title":"Key Decision Guidelines","text":"Scenario Callbacks Streaming Both Debug/Development \u2705 Essential \u274c Optional \u2705 Recommended Batch Processing \u2705 For logging \u274c Unnecessary \u2753 Maybe Interactive UI \u2753 Background \u2705 Essential \u2705 Ideal Web APIs \u2753 For logging \u2705 For SSE \u2705 Best UX Long Responses \u2753 Background \u2705 Essential \u2705 Optimal Short Responses \u2705 Sufficient \u2753 Overkill \u2753 Optional"},{"location":"usage/#using-workflows","title":"Using Workflows","text":"<p>vectara-agentic now supports custom workflows via the <code>run()</code> method, enabling you to define multi-step interactions with validated inputs and outputs. To learn more about workflows read the documentation from LlamaIndex.</p>"},{"location":"usage/#defining-a-custom-workflow","title":"Defining a Custom Workflow","text":"<p>To create a workflow, subclass the Workflow class from <code>llama_index.core.workflow</code> and define two Pydantic models: <code>InputsModel</code> and <code>`OutputsModel</code>.  For example:</p> <pre><code>from pydantic import BaseModel\nfrom llama_index.core.workflow import (\n    StartEvent,StopEvent, Workflow, step,\n)\n\nclass MyWorkflow(Workflow):\n    class InputsModel(BaseModel):\n        query: str\n\n    class OutputsModel(BaseModel):\n        answer: str\n\n    @step\n    async def my_step(self, ev: StartEvent) -&gt; StopEvent:\n        # do something here\n        return StopEvent(result=\"Hello, world!\")\n</code></pre> <p>When the <code>run()</code> method in vectara-agentic is invoked, it calls the workflow with the following variables in the <code>StartEvent</code>:</p> <ul> <li><code>agent</code>: the agent object used to call <code>run()</code> (self)</li> <li><code>tools</code>: the tools provided to the agent. Those can be used as needed in the flow.</li> <li><code>llm</code>: a pointer to a LlamaIndex LLM, so it can be used in the workflow. For example, one of the steps may call <code>llm.acomplete(prompt)</code></li> <li><code>verbose</code>: controls whether extra debug information is displayed</li> <li><code>inputs</code>: this is the actual inputs to the workflow provided by the call to <code>run()</code> and must be of type <code>InputsModel</code></li> </ul> <p>If you want to use <code>agent</code>, <code>tools</code>, <code>llm</code> or <code>verbose</code> in other events (that are not <code>StartEvent</code>), you can store them in the <code>Context</code> of the Workflow as follows:</p> <pre><code>await ctx.set(\"agent\", ev.agent)\n</code></pre> <p>and then in any other event you can pull that agent object with</p> <pre><code>agent = await ctx.get(\"agent\")\n</code></pre> <p>Similarly you can reuse the <code>llm</code>, <code>tools</code> or <code>verbose</code> arguments within other nodes in the workflow.</p>"},{"location":"usage/#integrating-the-workflow-with-your-agent","title":"Integrating the Workflow with Your Agent","text":"<p>When instantiating your agent, pass your workflow class to the <code>workflow_cls</code> parameter (and optionally set a workflow timeout):</p> <pre><code>agent = Agent(\n    tools=[...],  # your list of tools\n    topic=\"10-K annual financial reports\",\n    custom_instructions=financial_assistant_instructions,\n    agent_progress_callback=agent_progress_callback,\n    workflow_cls=FinanceWorkflow,   # Provide your workflow class here\n    workflow_timeout=120            # Optional timeout in seconds\n)\n</code></pre>"},{"location":"usage/#running-the-workflow","title":"Running the Workflow","text":"<p>To run the workflow, create an instance of your workflow's <code>InputsModel</code> with the required parameters and call the agent's <code>run()</code> method. For example:</p> <pre><code># Create input for the workflow\nworkflow_inputs = FinanceWorkflow.InputsModel(query=\"What were the revenue trends for Apple?\", analysis_depth=3)\n\n# Execute the workflow asynchronously (ensure you're in an async context or use asyncio.run)\nworkflow_output = asyncio.run(agent.run(workflow_inputs))\n\n# Access the final answer from the output model\nprint(workflow_output.answer)\n</code></pre> <p>The <code>run()</code> method executes your workflow\u2019s logic, validates the output against the <code>OutputsModel</code>, and returns a structured result.</p>"},{"location":"usage/#using-subquestionqueryworkflow-and-sequentialsubquestionsworkflow","title":"Using SubQuestionQueryWorkflow and SequentialSubQuestionsWorkflow","text":"<p>vectara-agentic already includes two useful workflows you can use right away (they are also useful as advanced examples) These workflows are called <code>SubQuestionQueryWorkflow</code> and <code>SequentialSubQuestionsWorkflow</code>, and they work by breaking a complex query into sub-queries and then executing each sub-query with the agent until it reaches a good response.</p> <p>The difference is that <code>SubQuestionQueryWorkflow</code> will create a series of independent sub-queries and execute them in parallel while <code>SequentialSubQuestionsWorkflow</code> will create a sequence of dependent sub-queries and answer them sequentially, providing the answer from the previous question as context to the next question in the sequence.</p>"},{"location":"usage/#additional-information","title":"Additional Information","text":"<p>Agent Information The <code>Agent</code> class defines a few helpful methods to help you understand the internals of your application.</p> <ol> <li>The <code>report()</code> method prints out the agent object's type (FUNCTION_CALLING or REACT), the tools, and the LLMs used for the main     agent and tool calling.</li> <li>The agent provides access to session information including conversation history     and tool usage patterns.</li> </ol> <p>If you have any other information that you would like to be accessible to users, feel free to make a suggestion on our community server.</p> <p>Observability You can also setup full observability for your vectara-agentic assistant or agent using Arize Phoenix. This allows you to view LLM prompt inputs and outputs, the latency of each task and subtask, and many of the individual function calls performed by the LLM, as well as VHC corrections for each response.</p> <p>To set up observability for your app, follow these steps:</p> <ol> <li>Set <code>os[\"VECTARA_AGENTIC_OBSERVER_TYPE\"] = \"ARIZE_PHOENIX\"</code> or     specify <code>observer = \"ARIZE_PHOENIX\"</code> in your <code>AgentConfig</code>.</li> <li>Connect to a local phoenix server:<ol> <li>If you have a local phoenix server that you've run using e.g.     <code>python -m phoenix.server.main serve</code>, vectara-agentic will send     all traces to it automatically.</li> <li>If not, vectara-agentic will run a local instance during the     agent's lifecycle, and will close it when finished.</li> <li>In both cases, traces will be sent to the local instance, and     you can see the dashboard at http://localhost:6006.</li> </ol> </li> <li>Alternatively, you can connect to a Phoenix instance hosted on     Arize.<ol> <li>Go to https://app.phoenix.arize.com, and set up an account if     you don't have one.</li> <li>Create an API key and put it in the <code>PHOENIX_API_KEY</code> variable.     This variable indicates you want to use the hosted version.</li> <li>To view the traces go to https://app.phoenix.arize.com.</li> </ol> </li> </ol> <p>In addition to the raw traces, vectara-agentic also records <code>FCS</code> values into Arize for every Vectara RAG call (note: RAG tools still use FCS internally). You can see those results in the <code>Feedback</code> column of the arize UI.</p> <p>Query Callback You can define a callback function to log query/response pairs in your agent. This function should be specified in the <code>query_logging_callback</code> argument when you create your agent and should take in two string arguments. The first argument passed to this function will be the user query and the second will be the agent's response.</p> <p>If defined, this function is called every time the agent receives a query and generates a response.</p>"},{"location":"usage/#using-a-private-llm","title":"Using a Private LLM","text":"<p>vectara-agentic offers a wide variety of LLM options from several providers to use for the main agent and for tool calling. However, in some instances, you may be interested in using your own LLM hosted locally at your company.</p> <p>If you would like the main agent LLM to be a custom LLM, specify <code>VECTARA_AGENTIC_MAIN_LLM_PROVIDER=\"PRIVATE\"</code> in your environment or <code>main_llm_provider=\"PRIVATE\"</code> in your <code>AgentConfig</code> object and <code>VECTARA_AGENTIC_MAIN_MODEL_NAME</code> (or <code>main_llm_model_name</code> in <code>AgentConfig</code>) as the model name of your LLM.</p> <p>If you would like the tool calling LLM to be a custom LLM, specify <code>VECTARA_AGENTIC_TOOL_LLM_PROVIDER=\"PRIVATE\"</code> in your environment or <code>tool_llm_provider=\"PRIVATE\"</code> in your <code>AgentConfig</code> object and <code>VECTARA_AGENTIC_TOOL_MODEL_NAME</code> (or <code>tool_llm_model_name</code> in <code>AgentConfig</code>) as the model name of your LLM.</p> <p>Additionally, you should specify <code>VECTARA_AGENTIC_PRIVATE_LLM_API_BASE</code> in your environment (or <code>private_llm_api_base</code> in the <code>AgentConfig</code>) as the API endpoint url for your private LLM and <code>VECTARA_AGENTIC_PRIVATE_API_KEY</code> (or <code>private_llm_api_key</code>) as the API key to your LLM.</p>"}]}