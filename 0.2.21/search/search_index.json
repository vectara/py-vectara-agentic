{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":""},{"location":"#introduction","title":"Introduction","text":"<p>What is Agentic RAG?</p> <p>Agentic RAG combines retrieval-augmented-generation (RAG) with autonomous agents. While standard RAG retrieves relevant facts and generates responses, Agentic RAG uses an LLM to \"manage\" the process through reasoning, planning, and tool usage.</p> <p>With vanilla RAG, Vectara receives a user query, retrieves the most relevant facts from your data, and uses an LLM to generate the most accurate response based on those facts. (Unfamiliar with RAG? Check out this page to learn more!)</p> <p>Agentic RAG leverages an LLM to \"manage\" the process of answering the user query via reasoning, planning, and a provided set of \"tools\". Since a \"manager\" LLM-powered agent is in charge, it is smart enough to analyze the user query and properly call tools to obtain a comprehensive response to a complex user query.</p> <p>For example:</p> <ul> <li>The agent can rephrase the user query to fit a certain style, role,     or persona.</li> <li>The agent can break the query down into multiple (simpler)     sub-queries and call the RAG query tool for each sub-query, then     combine the responses to come up with a comprehensive response.</li> <li>The agent can identify filtering criteria in the user query and use     them to filter the results from the RAG query tool.</li> </ul> <p>The main tool used in vectara-agentic is the <code>Vectara RAG query tool</code>, which queries a Vectara corpus and returns the most relevant response. By using a RAG-based agent, you mitigate some of the issues with pure LLMs, particularly hallucinations and explainability.</p> <p>Another important tool that can be used to query a Vectara corpus is the <code>Vectara search tool</code>, which queries a Vectara corpus for the most relevant search results and documents that match a query.</p> <p>Additional tools give your application superpowers to retrieve up-to-date information, access enterprise specific data via APIs, make SQL queries to a database, or even perform actions such as creating a calendar event or sending an email.</p> <p>Let\\'s demonstrate the advantage of Agentic RAG via a simple example.</p> <p>Imagine that you have ingested into Vectara all your Google Drive files, JIRA tickets, and product documentation. You build an Agentic RAG application using these tools:</p> <ol> <li>A JIRA RAG query tool</li> <li>A Google Drive RAG query tool</li> <li>A product docs RAG query tool</li> <li>A tool that can issue SQL queries against an internal database     containing customer support data</li> </ol> <p>Consider the query: \"What is the top issue reported by customers in the last 3 months? Who is working to solve it?\"</p> <p>A standard RAG pipeline would try to match this entire query to the most relevant facts in your data, and generate a response. It may fail to distinguish the query as two separate questions, and given the complexity, may fail to produce a good response.</p> <p>An Agentic RAG assistant would recognize the complexity of the user query, and decide to act in two steps. First it will form a query with its SQL tool to identify the top issue reported by customers in the last 3 months, and then it will call the JIRA tool to identify who is working on that issue from the first query.</p> <p>What is vectara-agentic?</p> <p>Vectara-agentic is a Python package for building Agentic RAG applications powered by Vectara. It: - Provides a simple API to define tools, including Vectara RAG tool and Vectara search tooo. - Includes pre-built tools for various domains (legal, finance, etc). - Integrates with multiple LLM providers (OpenAI, Anthropic, Gemini, Together.AI, Cohere, GROQ, Fireworks AI). - Supports advanced workflows for complex queries.</p>"},{"location":"#agent-architecture","title":"Agent Architecture","text":"<p>Vectara-agentic follows a typical agentic RAG architecture. It consists of the following components:</p> <ul> <li>One or more RAG tools for making queries to corpora in Vectara.</li> <li>A set of additional tools that the agent can use to retrieve     information, process data, or perform actions.</li> <li>A central LLM, or agent (based on <code>ReAct</code>, <code>OpenAI</code>, <code>LATS</code>, or     <code>LLMCompiler</code> agent type) that manages the process of interpreting     the user query, creating and executing a plan to collect information     needed to respond to that query, and crafting a final response.</li> </ul>"},{"location":"#basic-example","title":"Basic Example","text":"<p>Here's a simple example creating an agent with a single RAG tool:</p> <pre><code>from vectara_agentic.agent import Agent\nfrom vectara_agentic.tools import VectaraToolFactory\nfrom pydantic import Field, BaseModel\n\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\napi_key = str(os.environ['VECTARA_API_KEY'])\ncorpus_key = str(os.environ['VECTARA_CORPUS_KEY'])\n\nvec_factory = VectaraToolFactory(\n  vectara_api_key = api_key, \n  vectara_corpus_key = corpus_key\n)\n\nask_pet_policy_tool = vec_factory.create_rag_tool(\n    tool_name = \"ask_pet_policy\",\n    tool_description = \"Responds to questions about Vectara's pet policy.\",\n    summary_num_results = 10,\n    n_sentences_before = 3,\n    n_sentences_after = 3,\n    mmr_diversity_bias = 0.1,\n    include_citations = False\n)\n\nagent = Agent(\n    tools = [ask_pet_policy_tool],\n    topic = \"Vectara Pet Policy\"\n)\n\nagent.chat(\"What is Vectara's pet policy?\")\n</code></pre> <p>When we run this code, we get the following response:</p> <p>Vectara\\'s pet policy does not allow common household pets like cats and dogs on their campuses. Instead, they welcome a select group of exotic creatures that reflect their innovative spirit and core values. Additionally, birds are not only permitted but encouraged in their workspace as part of their unique approach.</p> <p>In the above code, we defined a single RAG tool (ask_pet_policy_tool) for our <code>Agent</code> class, and then created an AI assistant with this tool.</p> <p>The above code demonstrates the typical flow for instantiating your <code>Agent</code> object when you are defining more than one tool, only in this case it only used a single tool. Since making a simple assistant like this with just one RAG tool is a common need, we have provided a single function that does all of this at once called <code>from_corpus()</code>.</p> <p>Here\\'s how you can create a simple assistant that uses a single RAG tool for asking questions about Medicare:</p> <pre><code>agent = Agent.from_corpus(\n  vectara_corpus_key=corpus_key,\n  vectara_api_key=api_key,\n  data_description=\"medical plan benefits and pricing\",\n  assistant_specialty=\"Medicare\",\n  tool_name=\"ask_medicare\",\n)\n</code></pre>"},{"location":"#try-it-yourself","title":"Try it Yourself","text":"<ol> <li>Create a Vectara account</li> <li>Set up environment variables:</li> </ol> <p>Vectara Corpus: - <code>VECTARA_CORPUS_KEY</code>: Your corpus key - <code>VECTARA_API_KEY</code>: Your API key</p> <p><code>VECTARA_CORPUS_KEY</code>: The corpus key for the corpus that contains the Vectara pet policy. You can download the Pet Policy PDF file and add it to a new or existing Vectara corpus.</p> <p><code>VECTARA_API_KEY</code>: An API key that can perform queries on this corpus.</p> <p>Agent type, LLMs and model names:</p> <p><code>VECTARA_AGENTIC_AGENT_TYPE</code>: Agent type, either OPENAI (default), REACT, FUNCTION_CALLING, LATS or LLMCOMPILER  (make sure you have an OpenAI API key if you use the OpenAI agent).</p> <p><code>VECTARA_AGENTIC_MAIN_LLM_PROVIDER</code>: The LLM used for the agent, either OPENAI (default), ANTHROPIC, GEMINI, TOGETHER, COHERE, BEDROCK, GROQ, or FIREWORKS. Note that to use the OPENAI agent type, you must use OPENAI as the main LLM provider.</p> <p><code>VECTARA_AGENTIC_TOOL_LLM_PROVIDER</code>: The LLM used for the agent tools, either OPENAI (default), ANTHROPIC, GEMINI, TOGETHER, COHERE, BEDROCK, GROQ, or FIREWORKS.</p> <p><code>OPENAI_API_KEY</code>, <code>ANTHROPIC_API_KEY</code>, <code>GOOGLE_API_KEY</code>, <code>TOGETHER_API_KEY</code>, <code>COHERE_API_KEY</code>, <code>BEDROCK_API_KEY</code>, <code>GROQ_API_KEY</code>, or <code>FIREWORKS_API_KEY</code>: Your API key for the agent or tool LLM, if you choose to use these services.</p> <p>With any LLM provider choice, you can also specify the model type to use via these environment variables:</p> <p><code>VECTARA_AGENTIC_MAIN_MODEL_NAME</code>: specifies the model name for the main LLM provider.</p> <p><code>VECTARA_AGENTIC_TOOL_MODEL_NAME</code>: specifies the model name for the tool LLM provider.</p> <p>Defaults:</p> <ol> <li>For <code>OPENAI</code>, the default is <code>gpt-4.1</code>.</li> <li>For <code>ANTHROPIC</code>, the default is <code>claude-sonnet-4-20250514</code>.</li> <li>For <code>GEMINI</code>, the default is <code>gemini-2.0-flash</code>.</li> <li>For <code>TOGETHER.AI</code>, the default is <code>Qwen/Qwen2.5-72B-Instruct-Turbo</code>.</li> <li>For <code>COHERE</code>, the default is <code>command-a-03-2025</code>.</li> <li>For <code>BEDROCK</code>, the default is <code>us.anthropic.claude-sonnet-4-20250514-v1:0</code>.</li> <li>For <code>GROQ</code>, the default is <code>deepseek-r1-distill-llama-70b</code>.</li> <li>For <code>FIREWORKS</code>, the default is <code>accounts/fireworks/models/firefunction-v2</code>.</li> </ol>"},{"location":"api/","title":"API Reference","text":"<p>vectara_agentic package.</p>"},{"location":"api/#vectara_agentic.Agent","title":"<code>Agent</code>","text":"<p>Agent class for handling different types of agents and their interactions.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>class Agent:\n    \"\"\"\n    Agent class for handling different types of agents and their interactions.\n    \"\"\"\n\n    def __init__(\n        self,\n        tools: List[FunctionTool],\n        topic: str = \"general\",\n        custom_instructions: str = \"\",\n        general_instructions: str = GENERAL_INSTRUCTIONS,\n        verbose: bool = True,\n        use_structured_planning: bool = False,\n        update_func: Optional[Callable[[AgentStatusType, str], None]] = None,\n        agent_progress_callback: Optional[\n            Callable[[AgentStatusType, str], None]\n        ] = None,\n        query_logging_callback: Optional[Callable[[str, str], None]] = None,\n        agent_config: Optional[AgentConfig] = None,\n        fallback_agent_config: Optional[AgentConfig] = None,\n        chat_history: Optional[list[Tuple[str, str]]] = None,\n        validate_tools: bool = False,\n        workflow_cls: Optional[Workflow] = None,\n        workflow_timeout: int = 120,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the agent with the specified type, tools, topic, and system message.\n\n        Args:\n\n            tools (list[FunctionTool]): A list of tools to be used by the agent.\n            topic (str, optional): The topic for the agent. Defaults to 'general'.\n            custom_instructions (str, optional): Custom instructions for the agent. Defaults to ''.\n            general_instructions (str, optional): General instructions for the agent.\n                The Agent has a default set of instructions that are crafted to help it operate effectively.\n                This allows you to customize the agent's behavior and personality, but use with caution.\n            verbose (bool, optional): Whether the agent should print its steps. Defaults to True.\n            use_structured_planning (bool, optional)\n                Whether or not we want to wrap the agent with LlamaIndex StructuredPlannerAgent.\n            agent_progress_callback (Callable): A callback function the code calls on any agent updates.\n                update_func (Callable): old name for agent_progress_callback. Will be deprecated in future.\n            query_logging_callback (Callable): A callback function the code calls upon completion of a query\n            agent_config (AgentConfig, optional): The configuration of the agent.\n                Defaults to AgentConfig(), which reads from environment variables.\n            fallback_agent_config (AgentConfig, optional): The fallback configuration of the agent.\n                This config is used when the main agent config fails multiple times.\n            chat_history (Tuple[str, str], optional): A list of user/agent chat pairs to initialize the agent memory.\n            validate_tools (bool, optional): Whether to validate tool inconsistency with instructions.\n                Defaults to False.\n            workflow_cls (Workflow, optional): The workflow class to be used with run(). Defaults to None.\n            workflow_timeout (int, optional): The timeout for the workflow in seconds. Defaults to 120.\n        \"\"\"\n        self.agent_config = agent_config or AgentConfig()\n        self.agent_config_type = AgentConfigType.DEFAULT\n        self.tools = tools\n        if not any(tool.metadata.name == \"get_current_date\" for tool in self.tools):\n            self.tools += [ToolsFactory().create_tool(get_current_date)]\n        self.agent_type = self.agent_config.agent_type\n        self.use_structured_planning = use_structured_planning\n        self.llm = get_llm(LLMRole.MAIN, config=self.agent_config)\n        self._custom_instructions = custom_instructions\n        self._general_instructions = general_instructions\n        self._topic = topic\n        self.agent_progress_callback = (\n            agent_progress_callback if agent_progress_callback else update_func\n        )\n        self.query_logging_callback = query_logging_callback\n\n        self.workflow_cls = workflow_cls\n        self.workflow_timeout = workflow_timeout\n\n        # Sanitize tools for Gemini if needed\n        if self.agent_config.main_llm_provider == ModelProvider.GEMINI:\n            self.tools = self._sanitize_tools_for_gemini(self.tools)\n\n        # Validate tools\n        # Check for:\n        # 1. multiple copies of the same tool\n        # 2. Instructions for using tools that do not exist\n        tool_names = [tool.metadata.name for tool in self.tools]\n        duplicates = [tool for tool, count in Counter(tool_names).items() if count &gt; 1]\n        if duplicates:\n            raise ValueError(f\"Duplicate tools detected: {', '.join(duplicates)}\")\n\n        if validate_tools:\n            prompt = f\"\"\"\n            You are provided these tools:\n            &lt;tools&gt;{','.join(tool_names)}&lt;/tools&gt;\n            And these instructions:\n            &lt;instructions&gt;\n            {self._custom_instructions}\n            &lt;/instructions&gt;\n            Your task is to identify invalid tools.\n            A tool is invalid if it is mentioned in the instructions but not in the tools list.\n            A tool's name must have at least two characters.\n            Your response should be a comma-separated list of the invalid tools.\n            If no invalid tools exist, respond with \"&lt;OKAY&gt;\" (and nothing else).\n            \"\"\"\n            llm = get_llm(LLMRole.MAIN, config=self.agent_config)\n            bad_tools_str = llm.complete(prompt).text.strip('\\n')\n            if bad_tools_str and bad_tools_str != \"&lt;OKAY&gt;\":\n                bad_tools = [tool.strip() for tool in bad_tools_str.split(\",\")]\n                numbered = \", \".join(\n                    f\"({i}) {tool}\" for i, tool in enumerate(bad_tools, 1)\n                )\n                raise ValueError(\n                    f\"The Agent custom instructions mention these invalid tools: {numbered}\"\n                )\n\n        # Create token counters for the main and tool LLMs\n        main_tok = get_tokenizer_for_model(role=LLMRole.MAIN)\n        self.main_token_counter = (\n            TokenCountingHandler(tokenizer=main_tok) if main_tok else None\n        )\n        tool_tok = get_tokenizer_for_model(role=LLMRole.TOOL)\n        self.tool_token_counter = (\n            TokenCountingHandler(tokenizer=tool_tok) if tool_tok else None\n        )\n\n        # Setup callback manager\n        callbacks: list[BaseCallbackHandler] = [\n            AgentCallbackHandler(self.agent_progress_callback)\n        ]\n        if self.main_token_counter:\n            callbacks.append(self.main_token_counter)\n        if self.tool_token_counter:\n            callbacks.append(self.tool_token_counter)\n        callback_manager = CallbackManager(callbacks)  # type: ignore\n        self.verbose = verbose\n\n        if chat_history:\n            msg_history = []\n            for text_pairs in chat_history:\n                msg_history.append(\n                    ChatMessage.from_str(content=text_pairs[0], role=MessageRole.USER)\n                )\n                msg_history.append(\n                    ChatMessage.from_str(\n                        content=text_pairs[1], role=MessageRole.ASSISTANT\n                    )\n                )\n            self.memory = ChatMemoryBuffer.from_defaults(\n                token_limit=128000, chat_history=msg_history\n            )\n        else:\n            self.memory = ChatMemoryBuffer.from_defaults(token_limit=128000)\n\n        # Set up main agent and fallback agent\n        self.agent = self._create_agent(self.agent_config, callback_manager)\n        self.fallback_agent_config = fallback_agent_config\n        if self.fallback_agent_config:\n            self.fallback_agent = self._create_agent(\n                self.fallback_agent_config, callback_manager\n            )\n        else:\n            self.fallback_agent_config = None\n\n        # Setup observability\n        try:\n            self.observability_enabled = setup_observer(self.agent_config, self.verbose)\n        except Exception as e:\n            print(f\"Failed to set up observer ({e}), ignoring\")\n            self.observability_enabled = False\n\n    def _sanitize_tools_for_gemini(\n        self, tools: list[FunctionTool]\n    ) -&gt; list[FunctionTool]:\n        \"\"\"\n        Strip all default values from:\n        - tool.fn\n        - tool.async_fn\n        - tool.metadata.fn_schema\n        so Gemini sees *only* required parameters, no defaults.\n        \"\"\"\n        for tool in tools:\n            # 1) strip defaults off the actual callables\n            for func in (tool.fn, tool.async_fn):\n                if not func:\n                    continue\n                orig_sig = inspect.signature(func)\n                new_params = [\n                    p.replace(default=Parameter.empty)\n                    for p in orig_sig.parameters.values()\n                ]\n                new_sig = Signature(\n                    new_params, return_annotation=orig_sig.return_annotation\n                )\n                if ismethod(func):\n                    func.__func__.__signature__ = new_sig\n                else:\n                    func.__signature__ = new_sig\n\n            # 2) rebuild the Pydantic schema so that *every* field is required\n            schema_cls = getattr(tool.metadata, \"fn_schema\", None)\n            if schema_cls and hasattr(schema_cls, \"model_fields\"):\n                # collect (name \u2192 (type, Field(...))) for all fields\n                new_fields: dict[str, tuple[type, Any]] = {}\n                for name, mf in schema_cls.model_fields.items():\n                    typ = mf.annotation\n                    desc = getattr(mf, \"description\", \"\")\n                    # force required (no default) with Field(...)\n                    new_fields[name] = (typ, Field(..., description=desc))\n\n                # make a brand-new schema class where every field is required\n                no_default_schema = create_model(\n                    f\"{schema_cls.__name__}\",  # new class name\n                    **new_fields,  # type: ignore\n                )\n\n                # give it a clean __signature__ so inspect.signature sees no defaults\n                params = [\n                    Parameter(n, Parameter.POSITIONAL_OR_KEYWORD, annotation=typ)\n                    for n, (typ, _) in new_fields.items()\n                ]\n                no_default_schema.__signature__ = Signature(params)\n\n                # swap it back onto the tool\n                tool.metadata.fn_schema = no_default_schema\n\n        return tools\n\n    def _create_agent(\n        self, config: AgentConfig, llm_callback_manager: CallbackManager\n    ) -&gt; Union[BaseAgent, AgentRunner]:\n        \"\"\"\n        Creates the agent based on the configuration object.\n\n        Args:\n\n            config: The configuration of the agent.\n            llm_callback_manager: The callback manager for the agent's llm.\n\n        Returns:\n            Union[BaseAgent, AgentRunner]: The configured agent object.\n        \"\"\"\n        agent_type = config.agent_type\n        llm = get_llm(LLMRole.MAIN, config=config)\n        llm.callback_manager = llm_callback_manager\n\n        if agent_type == AgentType.FUNCTION_CALLING:\n            if config.tool_llm_provider == ModelProvider.OPENAI:\n                raise ValueError(\n                    \"Vectara-agentic: Function calling agent type is not supported with the OpenAI LLM.\"\n                )\n            prompt = _get_prompt(\n                GENERAL_PROMPT_TEMPLATE,\n                self._general_instructions,\n                self._topic,\n                self._custom_instructions,\n            )\n            agent = FunctionCallingAgent.from_tools(\n                tools=self.tools,\n                llm=llm,\n                memory=self.memory,\n                verbose=self.verbose,\n                max_function_calls=config.max_reasoning_steps,\n                callback_manager=llm_callback_manager,\n                system_prompt=prompt,\n                allow_parallel_tool_calls=True,\n            )\n        elif agent_type == AgentType.REACT:\n            prompt = _get_prompt(\n                REACT_PROMPT_TEMPLATE,\n                self._general_instructions,\n                self._topic,\n                self._custom_instructions,\n            )\n            agent = ReActAgent.from_tools(\n                tools=self.tools,\n                llm=llm,\n                memory=self.memory,\n                verbose=self.verbose,\n                react_chat_formatter=ReActChatFormatter(system_header=prompt),\n                max_iterations=config.max_reasoning_steps,\n                callable_manager=llm_callback_manager,\n            )\n        elif agent_type == AgentType.OPENAI:\n            if config.tool_llm_provider != ModelProvider.OPENAI:\n                raise ValueError(\n                    \"Vectara-agentic: OPENAI agent type requires the OpenAI LLM.\"\n                )\n            prompt = _get_prompt(\n                GENERAL_PROMPT_TEMPLATE,\n                self._general_instructions,\n                self._topic,\n                self._custom_instructions,\n            )\n            agent = OpenAIAgent.from_tools(\n                tools=self.tools,\n                llm=llm,\n                memory=self.memory,\n                verbose=self.verbose,\n                callback_manager=llm_callback_manager,\n                max_function_calls=config.max_reasoning_steps,\n                system_prompt=prompt,\n            )\n        elif agent_type == AgentType.LLMCOMPILER:\n            agent_worker = LLMCompilerAgentWorker.from_tools(\n                tools=self.tools,\n                llm=llm,\n                verbose=self.verbose,\n                callback_manager=llm_callback_manager,\n            )\n            agent_worker.system_prompt = _get_prompt(\n                prompt_template=_get_llm_compiler_prompt(\n                    prompt=agent_worker.system_prompt,\n                    general_instructions=self._general_instructions,\n                    topic=self._topic,\n                    custom_instructions=self._custom_instructions,\n                ),\n                general_instructions=self._general_instructions,\n                topic=self._topic,\n                custom_instructions=self._custom_instructions,\n            )\n            agent_worker.system_prompt_replan = _get_prompt(\n                prompt_template=_get_llm_compiler_prompt(\n                    prompt=agent_worker.system_prompt_replan,\n                    general_instructions=GENERAL_INSTRUCTIONS,\n                    topic=self._topic,\n                    custom_instructions=self._custom_instructions,\n                ),\n                general_instructions=GENERAL_INSTRUCTIONS,\n                topic=self._topic,\n                custom_instructions=self._custom_instructions,\n            )\n            agent = agent_worker.as_agent()\n        elif agent_type == AgentType.LATS:\n            agent_worker = LATSAgentWorker.from_tools(\n                tools=self.tools,\n                llm=llm,\n                num_expansions=3,\n                max_rollouts=-1,\n                verbose=self.verbose,\n                callback_manager=llm_callback_manager,\n            )\n            prompt = _get_prompt(\n                REACT_PROMPT_TEMPLATE,\n                self._general_instructions,\n                self._topic,\n                self._custom_instructions,\n            )\n            agent_worker.chat_formatter = ReActChatFormatter(system_header=prompt)\n            agent = agent_worker.as_agent()\n        else:\n            raise ValueError(f\"Unknown agent type: {agent_type}\")\n\n        # Set up structured planner if needed\n        if self.use_structured_planning or self.agent_type in [\n            AgentType.LLMCOMPILER,\n            AgentType.LATS,\n        ]:\n            planner_llm = get_llm(LLMRole.TOOL, config=config)\n            agent = StructuredPlannerAgent(\n                agent_worker=agent.agent_worker,\n                tools=self.tools,\n                llm=planner_llm,\n                memory=self.memory,\n                verbose=self.verbose,\n                initial_plan_prompt=STRUCTURED_PLANNER_INITIAL_PLAN_PROMPT,\n                plan_refine_prompt=STRUCTURED_PLANNER_PLAN_REFINE_PROMPT,\n            )\n\n        return agent\n\n    def clear_memory(self) -&gt; None:\n        \"\"\"\n        Clear the agent's memory.\n        \"\"\"\n        if self.agent_config_type == AgentConfigType.DEFAULT:\n            self.agent.memory.reset()\n        elif (\n            self.agent_config_type == AgentConfigType.FALLBACK\n            and self.fallback_agent_config\n        ):\n            self.fallback_agent.memory.reset()\n        else:\n            raise ValueError(f\"Invalid agent config type {self.agent_config_type}\")\n\n    def __eq__(self, other):\n        if not isinstance(other, Agent):\n            print(\n                f\"Comparison failed: other is not an instance of Agent. (self: {type(self)}, other: {type(other)})\"\n            )\n            return False\n\n        # Compare agent_type\n        if self.agent_config.agent_type != other.agent_config.agent_type:\n            print(\n                f\"Comparison failed: agent_type differs. (self.agent_config.agent_type: {self.agent_config.agent_type},\"\n                f\" other.agent_config.agent_type: {other.agent_config.agent_type})\"\n            )\n            return False\n\n        # Compare tools\n        if self.tools != other.tools:\n            print(\n                \"Comparison failed: tools differ.\"\n                f\"(self.tools: {[t.metadata.name for t in self.tools]}, \"\n                f\"other.tools: {[t.metadata.name for t in other.tools]})\"\n            )\n            return False\n\n        # Compare topic\n        if self._topic != other._topic:\n            print(\n                f\"Comparison failed: topic differs. (self.topic: {self._topic}, other.topic: {other._topic})\"\n            )\n            return False\n\n        # Compare custom_instructions\n        if self._custom_instructions != other._custom_instructions:\n            print(\n                \"Comparison failed: custom_instructions differ. (self.custom_instructions: \"\n                f\"{self._custom_instructions}, other.custom_instructions: {other._custom_instructions})\"\n            )\n            return False\n\n        # Compare verbose\n        if self.verbose != other.verbose:\n            print(\n                f\"Comparison failed: verbose differs. (self.verbose: {self.verbose}, other.verbose: {other.verbose})\"\n            )\n            return False\n\n        # Compare agent memory\n        if self.agent.memory.chat_store != other.agent.memory.chat_store:\n            print(\n                f\"Comparison failed: agent memory differs. (self.agent: {repr(self.agent.memory.chat_store)}, \"\n                f\"other.agent: {repr(other.agent.memory.chat_store)})\"\n            )\n            return False\n\n        # If all comparisons pass\n        print(\"All comparisons passed. Objects are equal.\")\n        return True\n\n    @classmethod\n    def from_tools(\n        cls,\n        tools: List[FunctionTool],\n        topic: str = \"general\",\n        custom_instructions: str = \"\",\n        verbose: bool = True,\n        update_func: Optional[Callable[[AgentStatusType, str], None]] = None,\n        agent_progress_callback: Optional[\n            Callable[[AgentStatusType, str], None]\n        ] = None,\n        query_logging_callback: Optional[Callable[[str, str], None]] = None,\n        agent_config: AgentConfig = AgentConfig(),\n        validate_tools: bool = False,\n        fallback_agent_config: Optional[AgentConfig] = None,\n        chat_history: Optional[list[Tuple[str, str]]] = None,\n        workflow_cls: Optional[Workflow] = None,\n        workflow_timeout: int = 120,\n    ) -&gt; \"Agent\":\n        \"\"\"\n        Create an agent from tools, agent type, and language model.\n\n        Args:\n\n            tools (list[FunctionTool]): A list of tools to be used by the agent.\n            topic (str, optional): The topic for the agent. Defaults to 'general'.\n            custom_instructions (str, optional): custom instructions for the agent. Defaults to ''.\n            verbose (bool, optional): Whether the agent should print its steps. Defaults to True.\n            agent_progress_callback (Callable): A callback function the code calls on any agent updates.\n                update_func (Callable): old name for agent_progress_callback. Will be deprecated in future.\n            query_logging_callback (Callable): A callback function the code calls upon completion of a query\n            agent_config (AgentConfig, optional): The configuration of the agent.\n            fallback_agent_config (AgentConfig, optional): The fallback configuration of the agent.\n            chat_history (Tuple[str, str], optional): A list of user/agent chat pairs to initialize the agent memory.\n            validate_tools (bool, optional): Whether to validate tool inconsistency with instructions.\n                Defaults to False.\n            workflow_cls (Workflow, optional): The workflow class to be used with run(). Defaults to None.\n            workflow_timeout (int, optional): The timeout for the workflow in seconds. Defaults to 120.\n\n        Returns:\n            Agent: An instance of the Agent class.\n        \"\"\"\n        return cls(\n            tools=tools,\n            topic=topic,\n            custom_instructions=custom_instructions,\n            verbose=verbose,\n            agent_progress_callback=agent_progress_callback,\n            query_logging_callback=query_logging_callback,\n            update_func=update_func,\n            agent_config=agent_config,\n            chat_history=chat_history,\n            validate_tools=validate_tools,\n            fallback_agent_config=fallback_agent_config,\n            workflow_cls=workflow_cls,\n            workflow_timeout=workflow_timeout,\n        )\n\n    @classmethod\n    def from_corpus(\n        cls,\n        tool_name: str,\n        data_description: str,\n        assistant_specialty: str,\n        general_instructions: str = GENERAL_INSTRUCTIONS,\n        vectara_corpus_key: str = str(os.environ.get(\"VECTARA_CORPUS_KEY\", \"\")),\n        vectara_api_key: str = str(os.environ.get(\"VECTARA_API_KEY\", \"\")),\n        agent_progress_callback: Optional[\n            Callable[[AgentStatusType, str], None]\n        ] = None,\n        query_logging_callback: Optional[Callable[[str, str], None]] = None,\n        agent_config: AgentConfig = AgentConfig(),\n        fallback_agent_config: Optional[AgentConfig] = None,\n        chat_history: Optional[list[Tuple[str, str]]] = None,\n        verbose: bool = False,\n        vectara_filter_fields: list[dict] = [],\n        vectara_offset: int = 0,\n        vectara_lambda_val: float = 0.005,\n        vectara_semantics: str = \"default\",\n        vectara_custom_dimensions: Dict = {},\n        vectara_reranker: str = \"slingshot\",\n        vectara_rerank_k: int = 50,\n        vectara_rerank_limit: Optional[int] = None,\n        vectara_rerank_cutoff: Optional[float] = None,\n        vectara_diversity_bias: float = 0.2,\n        vectara_udf_expression: Optional[str] = None,\n        vectara_rerank_chain: Optional[List[Dict]] = None,\n        vectara_n_sentences_before: int = 2,\n        vectara_n_sentences_after: int = 2,\n        vectara_summary_num_results: int = 10,\n        vectara_summarizer: str = \"vectara-summary-ext-24-05-med-omni\",\n        vectara_summary_response_language: str = \"eng\",\n        vectara_summary_prompt_text: Optional[str] = None,\n        vectara_max_response_chars: Optional[int] = None,\n        vectara_max_tokens: Optional[int] = None,\n        vectara_temperature: Optional[float] = None,\n        vectara_frequency_penalty: Optional[float] = None,\n        vectara_presence_penalty: Optional[float] = None,\n        vectara_save_history: bool = True,\n        return_direct: bool = False,\n    ) -&gt; \"Agent\":\n        \"\"\"\n        Create an agent from a single Vectara corpus\n\n        Args:\n            tool_name (str): The name of Vectara tool used by the agent\n            vectara_corpus_key (str): The Vectara corpus key (or comma separated list of keys).\n            vectara_api_key (str): The Vectara API key.\n            agent_progress_callback (Callable): A callback function the code calls on any agent updates.\n            query_logging_callback (Callable): A callback function the code calls upon completion of a query\n            agent_config (AgentConfig, optional): The configuration of the agent.\n            fallback_agent_config (AgentConfig, optional): The fallback configuration of the agent.\n            chat_history (Tuple[str, str], optional): A list of user/agent chat pairs to initialize the agent memory.\n            data_description (str): The description of the data.\n            assistant_specialty (str): The specialty of the assistant.\n            general_instructions (str, optional): General instructions for the agent.\n                The Agent has a default set of instructions that are crafted to help it operate effectively.\n                This allows you to customize the agent's behavior and personality, but use with caution.\n            verbose (bool, optional): Whether to print verbose output.\n            vectara_filter_fields (List[dict], optional): The filterable attributes\n                (each dict maps field name to Tuple[type, description]).\n            vectara_offset (int, optional): Number of results to skip.\n            vectara_lambda_val (float, optional): Lambda value for Vectara hybrid search.\n            vectara_semantics: (str, optional): Indicates whether the query is intended as a query or response.\n            vectara_custom_dimensions: (Dict, optional): Custom dimensions for the query.\n            vectara_reranker (str, optional): The Vectara reranker name (default \"slingshot\")\n            vectara_rerank_k (int, optional): The number of results to use with reranking.\n            vectara_rerank_limit: (int, optional): The maximum number of results to return after reranking.\n            vectara_rerank_cutoff: (float, optional): The minimum score threshold for results to include after\n                reranking.\n            vectara_diversity_bias (float, optional): The MMR diversity bias.\n            vectara_udf_expression (str, optional): The user defined expression for reranking results.\n            vectara_rerank_chain (List[Dict], optional): A list of Vectara rerankers to be applied sequentially.\n            vectara_n_sentences_before (int, optional): The number of sentences before the matching text\n            vectara_n_sentences_after (int, optional): The number of sentences after the matching text.\n            vectara_summary_num_results (int, optional): The number of results to use in summarization.\n            vectara_summarizer (str, optional): The Vectara summarizer name.\n            vectara_summary_response_language (str, optional): The response language for the Vectara summary.\n            vectara_summary_prompt_text (str, optional): The custom prompt, using appropriate prompt variables and\n                functions.\n            vectara_max_response_chars (int, optional): The desired maximum number of characters for the generated\n                summary.\n            vectara_max_tokens (int, optional): The maximum number of tokens to be returned by the LLM.\n            vectara_temperature (float, optional): The sampling temperature; higher values lead to more randomness.\n            vectara_frequency_penalty (float, optional): How much to penalize repeating tokens in the response,\n                higher values reducing likelihood of repeating the same line.\n            vectara_presence_penalty (float, optional): How much to penalize repeating tokens in the response,\n                higher values increasing the diversity of topics.\n            vectara_save_history (bool, optional): Whether to save the query in history.\n            return_direct (bool, optional): Whether the agent should return the tool's response directly.\n\n        Returns:\n            Agent: An instance of the Agent class.\n        \"\"\"\n        vec_factory = VectaraToolFactory(\n            vectara_api_key=vectara_api_key,\n            vectara_corpus_key=vectara_corpus_key,\n        )\n        field_definitions = {}\n        field_definitions[\"query\"] = (str, Field(description=\"The user query\"))  # type: ignore\n        for field in vectara_filter_fields:\n            field_definitions[field[\"name\"]] = (\n                eval(field[\"type\"]),\n                Field(description=field[\"description\"]),\n            )  # type: ignore\n        query_args = create_model(\"QueryArgs\", **field_definitions)  # type: ignore\n\n        # tool name must be valid Python function name\n        if tool_name:\n            tool_name = re.sub(r\"[^A-Za-z0-9_]\", \"_\", tool_name)\n\n        vectara_tool = vec_factory.create_rag_tool(\n            tool_name=tool_name or f\"vectara_{vectara_corpus_key}\",\n            tool_description=f\"\"\"\n            Given a user query,\n            returns a response (str) to a user question about {data_description}.\n            \"\"\",\n            tool_args_schema=query_args,\n            reranker=vectara_reranker,\n            rerank_k=vectara_rerank_k,\n            rerank_limit=vectara_rerank_limit,\n            rerank_cutoff=vectara_rerank_cutoff,\n            mmr_diversity_bias=vectara_diversity_bias,\n            udf_expression=vectara_udf_expression,\n            rerank_chain=vectara_rerank_chain,\n            n_sentences_before=vectara_n_sentences_before,\n            n_sentences_after=vectara_n_sentences_after,\n            offset=vectara_offset,\n            lambda_val=vectara_lambda_val,\n            semantics=vectara_semantics,\n            custom_dimensions=vectara_custom_dimensions,\n            summary_num_results=vectara_summary_num_results,\n            vectara_summarizer=vectara_summarizer,\n            summary_response_lang=vectara_summary_response_language,\n            vectara_prompt_text=vectara_summary_prompt_text,\n            max_response_chars=vectara_max_response_chars,\n            max_tokens=vectara_max_tokens,\n            temperature=vectara_temperature,\n            frequency_penalty=vectara_frequency_penalty,\n            presence_penalty=vectara_presence_penalty,\n            save_history=vectara_save_history,\n            include_citations=True,\n            verbose=verbose,\n            return_direct=return_direct,\n        )\n\n        assistant_instructions = f\"\"\"\n        - You are a helpful {assistant_specialty} assistant.\n        - You can answer questions about {data_description}.\n        - Never discuss politics, and always respond politely.\n        \"\"\"\n\n        return cls(\n            tools=[vectara_tool],\n            topic=assistant_specialty,\n            custom_instructions=assistant_instructions,\n            general_instructions=general_instructions,\n            verbose=verbose,\n            agent_progress_callback=agent_progress_callback,\n            query_logging_callback=query_logging_callback,\n            agent_config=agent_config,\n            fallback_agent_config=fallback_agent_config,\n            chat_history=chat_history,\n        )\n\n    def _switch_agent_config(self) -&gt; None:\n        \"\"\" \"\n        Switch the configuration type of the agent.\n        This function is called automatically to switch the agent configuration if the current configuration fails.\n        \"\"\"\n        if self.agent_config_type == AgentConfigType.DEFAULT:\n            self.agent_config_type = AgentConfigType.FALLBACK\n        else:\n            self.agent_config_type = AgentConfigType.DEFAULT\n\n    def report(self, detailed: bool = False) -&gt; None:\n        \"\"\"\n        Get a report from the agent.\n\n        Args:\n            detailed (bool, optional): Whether to include detailed information. Defaults to False.\n\n        Returns:\n            str: The report from the agent.\n        \"\"\"\n        print(\"Vectara agentic Report:\")\n        print(f\"Agent Type = {self.agent_config.agent_type}\")\n        print(f\"Topic = {self._topic}\")\n        print(\"Tools:\")\n        for tool in self.tools:\n            if hasattr(tool, \"metadata\"):\n                if detailed:\n                    print(f\"- {tool.metadata.description}\")\n                else:\n                    print(f\"- {tool.metadata.name}\")\n            else:\n                print(\"- tool without metadata\")\n        print(\n            f\"Agent LLM = {get_llm(LLMRole.MAIN, config=self.agent_config).metadata.model_name}\"\n        )\n        print(\n            f\"Tool LLM = {get_llm(LLMRole.TOOL, config=self.agent_config).metadata.model_name}\"\n        )\n\n    def token_counts(self) -&gt; dict:\n        \"\"\"\n        Get the token counts for the agent and tools.\n\n        Returns:\n            dict: The token counts for the agent and tools.\n        \"\"\"\n        return {\n            \"main token count\": (\n                self.main_token_counter.total_llm_token_count\n                if self.main_token_counter\n                else -1\n            ),\n            \"tool token count\": (\n                self.tool_token_counter.total_llm_token_count\n                if self.tool_token_counter\n                else -1\n            ),\n        }\n\n    def _get_current_agent(self):\n        return (\n            self.agent\n            if self.agent_config_type == AgentConfigType.DEFAULT\n            else self.fallback_agent\n        )\n\n    def _get_current_agent_type(self):\n        return (\n            self.agent_config.agent_type\n            if self.agent_config_type == AgentConfigType.DEFAULT\n            else self.fallback_agent_config.agent_type\n        )\n\n    async def _aformat_for_lats(self, prompt, agent_response):\n        llm_prompt = f\"\"\"\n        Given the question '{prompt}', and agent response '{agent_response.response}',\n        Please provide a well formatted final response to the query.\n        final response:\n        \"\"\"\n        agent_type = self._get_current_agent_type()\n        if agent_type != AgentType.LATS:\n            return\n\n        agent = self._get_current_agent()\n        agent_response.response = str(agent.llm.acomplete(llm_prompt))\n\n    def chat(self, prompt: str) -&gt; AgentResponse:  # type: ignore\n        \"\"\"\n        Interact with the agent using a chat prompt.\n\n        Args:\n            prompt (str): The chat prompt.\n\n        Returns:\n            AgentResponse: The response from the agent.\n        \"\"\"\n        return asyncio.run(self.achat(prompt))\n\n    async def achat(self, prompt: str) -&gt; AgentResponse:  # type: ignore\n        \"\"\"\n        Interact with the agent using a chat prompt.\n\n        Args:\n            prompt (str): The chat prompt.\n\n        Returns:\n            AgentResponse: The response from the agent.\n        \"\"\"\n        max_attempts = 4 if self.fallback_agent_config else 2\n        attempt = 0\n        orig_llm = self.llm.metadata.model_name\n        last_error = None\n        while attempt &lt; max_attempts:\n            try:\n                current_agent = self._get_current_agent()\n                agent_response = await current_agent.achat(prompt)\n                await self._aformat_for_lats(prompt, agent_response)\n                if self.observability_enabled:\n                    eval_fcs()\n                if self.query_logging_callback:\n                    self.query_logging_callback(prompt, agent_response.response)\n                return agent_response\n\n            except Exception as e:\n                last_error = e\n                if attempt &gt;= 2:\n                    if self.verbose:\n                        print(\n                            f\"LLM call failed on attempt {attempt}. Switching agent configuration.\"\n                        )\n                    self._switch_agent_config()\n                time.sleep(1)\n                attempt += 1\n\n        return AgentResponse(\n            response=(\n                f\"For {orig_llm} LLM - failure can't be resolved after \"\n                f\"{max_attempts} attempts ({last_error}.\"\n            )\n        )\n\n    def stream_chat(self, prompt: str) -&gt; AgentStreamingResponse:  # type: ignore\n        \"\"\"\n        Interact with the agent using a chat prompt with streaming.\n        Args:\n            prompt (str): The chat prompt.\n        Returns:\n            AgentStreamingResponse: The streaming response from the agent.\n        \"\"\"\n        return asyncio.run(self.astream_chat(prompt))\n\n    async def astream_chat(self, prompt: str) -&gt; AgentStreamingResponse:  # type: ignore\n        \"\"\"\n        Interact with the agent using a chat prompt asynchronously with streaming.\n        Args:\n            prompt (str): The chat prompt.\n        Returns:\n            AgentStreamingResponse: The streaming response from the agent.\n        \"\"\"\n        max_attempts = 4 if self.fallback_agent_config else 2\n        attempt = 0\n        orig_llm = self.llm.metadata.model_name\n        while attempt &lt; max_attempts:\n            try:\n                current_agent = self._get_current_agent()\n                agent_response = await current_agent.astream_chat(prompt)\n                original_async_response_gen = agent_response.async_response_gen\n\n                # Define a wrapper to preserve streaming behavior while executing post-stream logic.\n                async def _stream_response_wrapper():\n                    async for token in original_async_response_gen():\n                        yield token  # Yield tokens as they are generated\n                    # Post-streaming additional logic:\n                    await self._aformat_for_lats(prompt, agent_response)\n                    if self.query_logging_callback:\n                        self.query_logging_callback(prompt, agent_response.response)\n                    if self.observability_enabled:\n                        eval_fcs()\n\n                agent_response.async_response_gen = (\n                    _stream_response_wrapper  # Override the generator\n                )\n                return agent_response\n\n            except Exception as e:\n                last_error = e\n                if attempt &gt;= 2:\n                    if self.verbose:\n                        print(\n                            f\"LLM call failed on attempt {attempt}. Switching agent configuration.\"\n                        )\n                    self._switch_agent_config()\n                time.sleep(1)\n                attempt += 1\n\n        return AgentStreamingResponse(\n            response=(\n                f\"For {orig_llm} LLM - failure can't be resolved after \"\n                f\"{max_attempts} attempts ({last_error}).\"\n            )\n        )\n\n    #\n    # run() method for running a workflow\n    # workflow will always get these arguments in the StartEvent: agent, tools, llm, verbose\n    # the inputs argument comes from the call to run()\n    #\n    async def run(\n        self,\n        inputs: Any,\n        verbose: bool = False,\n    ) -&gt; Any:\n        \"\"\"\n        Run a workflow using the agent.\n        workflow class must be provided in the agent constructor.\n        Args:\n            inputs (Any): The inputs to the workflow.\n            verbose (bool, optional): Whether to print verbose output. Defaults to False.\n        Returns:\n            Any: The output or context of the workflow.\n        \"\"\"\n        # Create workflow\n        if self.workflow_cls:\n            workflow = self.workflow_cls(timeout=self.workflow_timeout, verbose=verbose)\n        else:\n            raise ValueError(\"Workflow is not defined.\")\n\n        # Validate inputs is in the form of workflow.InputsModel\n        if not isinstance(inputs, self.workflow_cls.InputsModel):\n            raise ValueError(f\"Inputs must be an instance of {workflow.InputsModel}.\")\n\n        workflow_context = Context(workflow=workflow)\n        try:\n            # run workflow\n            result = await workflow.run(\n                ctx=workflow_context,\n                agent=self,\n                tools=self.tools,\n                llm=self.llm,\n                verbose=verbose,\n                inputs=inputs,\n            )\n\n            # return output in the form of workflow.OutputsModel(BaseModel)\n            try:\n                output = workflow.OutputsModel.model_validate(result)\n            except ValidationError as e:\n                raise ValueError(f\"Failed to map workflow output to model: {e}\") from e\n\n        except Exception as e:\n            outputs_model_on_fail_cls = getattr(workflow.__class__, \"OutputModelOnFail\", None)\n            if outputs_model_on_fail_cls:\n                model_fields = outputs_model_on_fail_cls.model_fields\n                input_dict = {\n                    key: await workflow_context.get(key, None)\n                    for key in model_fields\n                }\n\n                # return output in the form of workflow.OutputModelOnFail(BaseModel)\n                output = outputs_model_on_fail_cls.model_validate(input_dict)\n            else:\n                print(f\"Vectara Agentic: Workflow failed with unexpected error: {e}\")\n                raise type(e)(str(e)).with_traceback(e.__traceback__)\n\n        return output\n\n    #\n    # Serialization methods\n    #\n    def dumps(self) -&gt; str:\n        \"\"\"Serialize the Agent instance to a JSON string.\"\"\"\n        return json.dumps(self.to_dict())\n\n    @classmethod\n    def loads(cls, data: str) -&gt; \"Agent\":\n        \"\"\"Create an Agent instance from a JSON string.\"\"\"\n        return cls.from_dict(json.loads(data))\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Serialize the Agent instance to a dictionary.\"\"\"\n        tool_info = []\n        for tool in self.tools:\n            if hasattr(tool.metadata, \"fn_schema\"):\n                fn_schema_cls = tool.metadata.fn_schema\n                fn_schema_serialized = {\n                    \"schema\": (\n                        fn_schema_cls.model_json_schema()\n                        if fn_schema_cls and hasattr(fn_schema_cls, \"model_json_schema\")\n                        else None\n                    ),\n                    \"metadata\": {\n                        \"module\": fn_schema_cls.__module__ if fn_schema_cls else None,\n                        \"class\": fn_schema_cls.__name__ if fn_schema_cls else None,\n                    },\n                }\n            else:\n                fn_schema_serialized = None\n\n            tool_dict = {\n                \"tool_type\": tool.metadata.tool_type.value,\n                \"name\": tool.metadata.name,\n                \"description\": tool.metadata.description,\n                \"fn\": (\n                    pickle.dumps(getattr(tool, \"fn\", None)).decode(\"latin-1\")\n                    if getattr(tool, \"fn\", None)\n                    else None\n                ),\n                \"async_fn\": (\n                    pickle.dumps(getattr(tool, \"async_fn\", None)).decode(\"latin-1\")\n                    if getattr(tool, \"async_fn\", None)\n                    else None\n                ),\n                \"fn_schema\": fn_schema_serialized,\n            }\n            tool_info.append(tool_dict)\n\n        return {\n            \"agent_type\": self.agent_config.agent_type.value,\n            \"memory\": pickle.dumps(self.agent.memory).decode(\"latin-1\"),\n            \"tools\": tool_info,\n            \"topic\": self._topic,\n            \"custom_instructions\": self._custom_instructions,\n            \"verbose\": self.verbose,\n            \"agent_config\": self.agent_config.to_dict(),\n            \"fallback_agent\": (\n                self.fallback_agent_config.to_dict()\n                if self.fallback_agent_config\n                else None\n            ),\n            \"workflow_cls\": self.workflow_cls if self.workflow_cls else None,\n        }\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; \"Agent\":\n        \"\"\"Create an Agent instance from a dictionary.\"\"\"\n        agent_config = AgentConfig.from_dict(data[\"agent_config\"])\n        fallback_agent_config = (\n            AgentConfig.from_dict(data[\"fallback_agent_config\"])\n            if data.get(\"fallback_agent_config\")\n            else None\n        )\n        tools: list[FunctionTool] = []\n\n        for tool_data in data[\"tools\"]:\n            query_args_model = None\n            if tool_data.get(\"fn_schema\"):\n                schema_info = tool_data[\"fn_schema\"]\n                try:\n                    module_name = schema_info[\"metadata\"][\"module\"]\n                    class_name = schema_info[\"metadata\"][\"class\"]\n                    mod = importlib.import_module(module_name)\n                    candidate_cls = getattr(mod, class_name)\n                    if inspect.isclass(candidate_cls) and issubclass(\n                        candidate_cls, BaseModel\n                    ):\n                        query_args_model = candidate_cls\n                    else:\n                        # It's not the Pydantic model class we expected (e.g., it's the function itself)\n                        # Force fallback to JSON schema reconstruction by raising an error.\n                        raise ImportError(\n                            f\"Retrieved '{class_name}' from '{module_name}' is not a Pydantic BaseModel class. \"\n                            \"Falling back to JSON schema reconstruction.\"\n                        )\n                except Exception:\n                    # Fallback: rebuild using the JSON schema\n                    field_definitions = {}\n                    json_schema_to_rebuild = schema_info.get(\"schema\")\n                    if json_schema_to_rebuild and isinstance(\n                        json_schema_to_rebuild, dict\n                    ):\n                        for field, values in json_schema_to_rebuild.get(\n                            \"properties\", {}\n                        ).items():\n                            field_type = get_field_type(values)\n                            field_description = values.get(\n                                \"description\"\n                            )  # Defaults to None\n                            if \"default\" in values:\n                                field_definitions[field] = (\n                                    field_type,\n                                    Field(\n                                        description=field_description,\n                                        default=values[\"default\"],\n                                    ),\n                                )\n                            else:\n                                field_definitions[field] = (\n                                    field_type,\n                                    Field(description=field_description),\n                                )\n                        query_args_model = create_model(\n                            json_schema_to_rebuild.get(\n                                \"title\", f\"{tool_data['name']}_QueryArgs\"\n                            ),\n                            **field_definitions,\n                        )\n                    else:  # If schema part is missing or not a dict, create a default empty model\n                        query_args_model = create_model(\n                            f\"{tool_data['name']}_QueryArgs\"\n                        )\n\n            # If fn_schema was not in tool_data or reconstruction failed badly, default to empty pydantic model\n            if query_args_model is None:\n                query_args_model = create_model(f\"{tool_data['name']}_QueryArgs\")\n\n            fn = (\n                pickle.loads(tool_data[\"fn\"].encode(\"latin-1\"))\n                if tool_data[\"fn\"]\n                else None\n            )\n            async_fn = (\n                pickle.loads(tool_data[\"async_fn\"].encode(\"latin-1\"))\n                if tool_data[\"async_fn\"]\n                else None\n            )\n\n            tool = VectaraTool.from_defaults(\n                name=tool_data[\"name\"],\n                description=tool_data[\"description\"],\n                fn=fn,\n                async_fn=async_fn,\n                fn_schema=query_args_model,  # Re-assign the recreated dynamic model\n                tool_type=ToolType(tool_data[\"tool_type\"]),\n            )\n            tools.append(tool)\n\n        agent = cls(\n            tools=tools,\n            agent_config=agent_config,\n            topic=data[\"topic\"],\n            custom_instructions=data[\"custom_instructions\"],\n            verbose=data[\"verbose\"],\n            fallback_agent_config=fallback_agent_config,\n            workflow_cls=data[\"workflow_cls\"],\n        )\n        memory = (\n            pickle.loads(data[\"memory\"].encode(\"latin-1\"))\n            if data.get(\"memory\")\n            else None\n        )\n        if memory:\n            agent.agent.memory = memory\n        return agent\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.__init__","title":"<code>__init__(tools, topic='general', custom_instructions='', general_instructions=GENERAL_INSTRUCTIONS, verbose=True, use_structured_planning=False, update_func=None, agent_progress_callback=None, query_logging_callback=None, agent_config=None, fallback_agent_config=None, chat_history=None, validate_tools=False, workflow_cls=None, workflow_timeout=120)</code>","text":"<p>Initialize the agent with the specified type, tools, topic, and system message.</p> <p>Args:</p> <pre><code>tools (list[FunctionTool]): A list of tools to be used by the agent.\ntopic (str, optional): The topic for the agent. Defaults to 'general'.\ncustom_instructions (str, optional): Custom instructions for the agent. Defaults to ''.\ngeneral_instructions (str, optional): General instructions for the agent.\n    The Agent has a default set of instructions that are crafted to help it operate effectively.\n    This allows you to customize the agent's behavior and personality, but use with caution.\nverbose (bool, optional): Whether the agent should print its steps. Defaults to True.\nuse_structured_planning (bool, optional)\n    Whether or not we want to wrap the agent with LlamaIndex StructuredPlannerAgent.\nagent_progress_callback (Callable): A callback function the code calls on any agent updates.\n    update_func (Callable): old name for agent_progress_callback. Will be deprecated in future.\nquery_logging_callback (Callable): A callback function the code calls upon completion of a query\nagent_config (AgentConfig, optional): The configuration of the agent.\n    Defaults to AgentConfig(), which reads from environment variables.\nfallback_agent_config (AgentConfig, optional): The fallback configuration of the agent.\n    This config is used when the main agent config fails multiple times.\nchat_history (Tuple[str, str], optional): A list of user/agent chat pairs to initialize the agent memory.\nvalidate_tools (bool, optional): Whether to validate tool inconsistency with instructions.\n    Defaults to False.\nworkflow_cls (Workflow, optional): The workflow class to be used with run(). Defaults to None.\nworkflow_timeout (int, optional): The timeout for the workflow in seconds. Defaults to 120.\n</code></pre> Source code in <code>vectara_agentic/agent.py</code> <pre><code>def __init__(\n    self,\n    tools: List[FunctionTool],\n    topic: str = \"general\",\n    custom_instructions: str = \"\",\n    general_instructions: str = GENERAL_INSTRUCTIONS,\n    verbose: bool = True,\n    use_structured_planning: bool = False,\n    update_func: Optional[Callable[[AgentStatusType, str], None]] = None,\n    agent_progress_callback: Optional[\n        Callable[[AgentStatusType, str], None]\n    ] = None,\n    query_logging_callback: Optional[Callable[[str, str], None]] = None,\n    agent_config: Optional[AgentConfig] = None,\n    fallback_agent_config: Optional[AgentConfig] = None,\n    chat_history: Optional[list[Tuple[str, str]]] = None,\n    validate_tools: bool = False,\n    workflow_cls: Optional[Workflow] = None,\n    workflow_timeout: int = 120,\n) -&gt; None:\n    \"\"\"\n    Initialize the agent with the specified type, tools, topic, and system message.\n\n    Args:\n\n        tools (list[FunctionTool]): A list of tools to be used by the agent.\n        topic (str, optional): The topic for the agent. Defaults to 'general'.\n        custom_instructions (str, optional): Custom instructions for the agent. Defaults to ''.\n        general_instructions (str, optional): General instructions for the agent.\n            The Agent has a default set of instructions that are crafted to help it operate effectively.\n            This allows you to customize the agent's behavior and personality, but use with caution.\n        verbose (bool, optional): Whether the agent should print its steps. Defaults to True.\n        use_structured_planning (bool, optional)\n            Whether or not we want to wrap the agent with LlamaIndex StructuredPlannerAgent.\n        agent_progress_callback (Callable): A callback function the code calls on any agent updates.\n            update_func (Callable): old name for agent_progress_callback. Will be deprecated in future.\n        query_logging_callback (Callable): A callback function the code calls upon completion of a query\n        agent_config (AgentConfig, optional): The configuration of the agent.\n            Defaults to AgentConfig(), which reads from environment variables.\n        fallback_agent_config (AgentConfig, optional): The fallback configuration of the agent.\n            This config is used when the main agent config fails multiple times.\n        chat_history (Tuple[str, str], optional): A list of user/agent chat pairs to initialize the agent memory.\n        validate_tools (bool, optional): Whether to validate tool inconsistency with instructions.\n            Defaults to False.\n        workflow_cls (Workflow, optional): The workflow class to be used with run(). Defaults to None.\n        workflow_timeout (int, optional): The timeout for the workflow in seconds. Defaults to 120.\n    \"\"\"\n    self.agent_config = agent_config or AgentConfig()\n    self.agent_config_type = AgentConfigType.DEFAULT\n    self.tools = tools\n    if not any(tool.metadata.name == \"get_current_date\" for tool in self.tools):\n        self.tools += [ToolsFactory().create_tool(get_current_date)]\n    self.agent_type = self.agent_config.agent_type\n    self.use_structured_planning = use_structured_planning\n    self.llm = get_llm(LLMRole.MAIN, config=self.agent_config)\n    self._custom_instructions = custom_instructions\n    self._general_instructions = general_instructions\n    self._topic = topic\n    self.agent_progress_callback = (\n        agent_progress_callback if agent_progress_callback else update_func\n    )\n    self.query_logging_callback = query_logging_callback\n\n    self.workflow_cls = workflow_cls\n    self.workflow_timeout = workflow_timeout\n\n    # Sanitize tools for Gemini if needed\n    if self.agent_config.main_llm_provider == ModelProvider.GEMINI:\n        self.tools = self._sanitize_tools_for_gemini(self.tools)\n\n    # Validate tools\n    # Check for:\n    # 1. multiple copies of the same tool\n    # 2. Instructions for using tools that do not exist\n    tool_names = [tool.metadata.name for tool in self.tools]\n    duplicates = [tool for tool, count in Counter(tool_names).items() if count &gt; 1]\n    if duplicates:\n        raise ValueError(f\"Duplicate tools detected: {', '.join(duplicates)}\")\n\n    if validate_tools:\n        prompt = f\"\"\"\n        You are provided these tools:\n        &lt;tools&gt;{','.join(tool_names)}&lt;/tools&gt;\n        And these instructions:\n        &lt;instructions&gt;\n        {self._custom_instructions}\n        &lt;/instructions&gt;\n        Your task is to identify invalid tools.\n        A tool is invalid if it is mentioned in the instructions but not in the tools list.\n        A tool's name must have at least two characters.\n        Your response should be a comma-separated list of the invalid tools.\n        If no invalid tools exist, respond with \"&lt;OKAY&gt;\" (and nothing else).\n        \"\"\"\n        llm = get_llm(LLMRole.MAIN, config=self.agent_config)\n        bad_tools_str = llm.complete(prompt).text.strip('\\n')\n        if bad_tools_str and bad_tools_str != \"&lt;OKAY&gt;\":\n            bad_tools = [tool.strip() for tool in bad_tools_str.split(\",\")]\n            numbered = \", \".join(\n                f\"({i}) {tool}\" for i, tool in enumerate(bad_tools, 1)\n            )\n            raise ValueError(\n                f\"The Agent custom instructions mention these invalid tools: {numbered}\"\n            )\n\n    # Create token counters for the main and tool LLMs\n    main_tok = get_tokenizer_for_model(role=LLMRole.MAIN)\n    self.main_token_counter = (\n        TokenCountingHandler(tokenizer=main_tok) if main_tok else None\n    )\n    tool_tok = get_tokenizer_for_model(role=LLMRole.TOOL)\n    self.tool_token_counter = (\n        TokenCountingHandler(tokenizer=tool_tok) if tool_tok else None\n    )\n\n    # Setup callback manager\n    callbacks: list[BaseCallbackHandler] = [\n        AgentCallbackHandler(self.agent_progress_callback)\n    ]\n    if self.main_token_counter:\n        callbacks.append(self.main_token_counter)\n    if self.tool_token_counter:\n        callbacks.append(self.tool_token_counter)\n    callback_manager = CallbackManager(callbacks)  # type: ignore\n    self.verbose = verbose\n\n    if chat_history:\n        msg_history = []\n        for text_pairs in chat_history:\n            msg_history.append(\n                ChatMessage.from_str(content=text_pairs[0], role=MessageRole.USER)\n            )\n            msg_history.append(\n                ChatMessage.from_str(\n                    content=text_pairs[1], role=MessageRole.ASSISTANT\n                )\n            )\n        self.memory = ChatMemoryBuffer.from_defaults(\n            token_limit=128000, chat_history=msg_history\n        )\n    else:\n        self.memory = ChatMemoryBuffer.from_defaults(token_limit=128000)\n\n    # Set up main agent and fallback agent\n    self.agent = self._create_agent(self.agent_config, callback_manager)\n    self.fallback_agent_config = fallback_agent_config\n    if self.fallback_agent_config:\n        self.fallback_agent = self._create_agent(\n            self.fallback_agent_config, callback_manager\n        )\n    else:\n        self.fallback_agent_config = None\n\n    # Setup observability\n    try:\n        self.observability_enabled = setup_observer(self.agent_config, self.verbose)\n    except Exception as e:\n        print(f\"Failed to set up observer ({e}), ignoring\")\n        self.observability_enabled = False\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.achat","title":"<code>achat(prompt)</code>  <code>async</code>","text":"<p>Interact with the agent using a chat prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The chat prompt.</p> required <p>Returns:</p> Name Type Description <code>AgentResponse</code> <code>AgentResponse</code> <p>The response from the agent.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>async def achat(self, prompt: str) -&gt; AgentResponse:  # type: ignore\n    \"\"\"\n    Interact with the agent using a chat prompt.\n\n    Args:\n        prompt (str): The chat prompt.\n\n    Returns:\n        AgentResponse: The response from the agent.\n    \"\"\"\n    max_attempts = 4 if self.fallback_agent_config else 2\n    attempt = 0\n    orig_llm = self.llm.metadata.model_name\n    last_error = None\n    while attempt &lt; max_attempts:\n        try:\n            current_agent = self._get_current_agent()\n            agent_response = await current_agent.achat(prompt)\n            await self._aformat_for_lats(prompt, agent_response)\n            if self.observability_enabled:\n                eval_fcs()\n            if self.query_logging_callback:\n                self.query_logging_callback(prompt, agent_response.response)\n            return agent_response\n\n        except Exception as e:\n            last_error = e\n            if attempt &gt;= 2:\n                if self.verbose:\n                    print(\n                        f\"LLM call failed on attempt {attempt}. Switching agent configuration.\"\n                    )\n                self._switch_agent_config()\n            time.sleep(1)\n            attempt += 1\n\n    return AgentResponse(\n        response=(\n            f\"For {orig_llm} LLM - failure can't be resolved after \"\n            f\"{max_attempts} attempts ({last_error}.\"\n        )\n    )\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.astream_chat","title":"<code>astream_chat(prompt)</code>  <code>async</code>","text":"<p>Interact with the agent using a chat prompt asynchronously with streaming. Args:     prompt (str): The chat prompt. Returns:     AgentStreamingResponse: The streaming response from the agent.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>async def astream_chat(self, prompt: str) -&gt; AgentStreamingResponse:  # type: ignore\n    \"\"\"\n    Interact with the agent using a chat prompt asynchronously with streaming.\n    Args:\n        prompt (str): The chat prompt.\n    Returns:\n        AgentStreamingResponse: The streaming response from the agent.\n    \"\"\"\n    max_attempts = 4 if self.fallback_agent_config else 2\n    attempt = 0\n    orig_llm = self.llm.metadata.model_name\n    while attempt &lt; max_attempts:\n        try:\n            current_agent = self._get_current_agent()\n            agent_response = await current_agent.astream_chat(prompt)\n            original_async_response_gen = agent_response.async_response_gen\n\n            # Define a wrapper to preserve streaming behavior while executing post-stream logic.\n            async def _stream_response_wrapper():\n                async for token in original_async_response_gen():\n                    yield token  # Yield tokens as they are generated\n                # Post-streaming additional logic:\n                await self._aformat_for_lats(prompt, agent_response)\n                if self.query_logging_callback:\n                    self.query_logging_callback(prompt, agent_response.response)\n                if self.observability_enabled:\n                    eval_fcs()\n\n            agent_response.async_response_gen = (\n                _stream_response_wrapper  # Override the generator\n            )\n            return agent_response\n\n        except Exception as e:\n            last_error = e\n            if attempt &gt;= 2:\n                if self.verbose:\n                    print(\n                        f\"LLM call failed on attempt {attempt}. Switching agent configuration.\"\n                    )\n                self._switch_agent_config()\n            time.sleep(1)\n            attempt += 1\n\n    return AgentStreamingResponse(\n        response=(\n            f\"For {orig_llm} LLM - failure can't be resolved after \"\n            f\"{max_attempts} attempts ({last_error}).\"\n        )\n    )\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.chat","title":"<code>chat(prompt)</code>","text":"<p>Interact with the agent using a chat prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The chat prompt.</p> required <p>Returns:</p> Name Type Description <code>AgentResponse</code> <code>AgentResponse</code> <p>The response from the agent.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>def chat(self, prompt: str) -&gt; AgentResponse:  # type: ignore\n    \"\"\"\n    Interact with the agent using a chat prompt.\n\n    Args:\n        prompt (str): The chat prompt.\n\n    Returns:\n        AgentResponse: The response from the agent.\n    \"\"\"\n    return asyncio.run(self.achat(prompt))\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.clear_memory","title":"<code>clear_memory()</code>","text":"<p>Clear the agent's memory.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>def clear_memory(self) -&gt; None:\n    \"\"\"\n    Clear the agent's memory.\n    \"\"\"\n    if self.agent_config_type == AgentConfigType.DEFAULT:\n        self.agent.memory.reset()\n    elif (\n        self.agent_config_type == AgentConfigType.FALLBACK\n        and self.fallback_agent_config\n    ):\n        self.fallback_agent.memory.reset()\n    else:\n        raise ValueError(f\"Invalid agent config type {self.agent_config_type}\")\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.dumps","title":"<code>dumps()</code>","text":"<p>Serialize the Agent instance to a JSON string.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>def dumps(self) -&gt; str:\n    \"\"\"Serialize the Agent instance to a JSON string.\"\"\"\n    return json.dumps(self.to_dict())\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.from_corpus","title":"<code>from_corpus(tool_name, data_description, assistant_specialty, general_instructions=GENERAL_INSTRUCTIONS, vectara_corpus_key=str(os.environ.get('VECTARA_CORPUS_KEY', '')), vectara_api_key=str(os.environ.get('VECTARA_API_KEY', '')), agent_progress_callback=None, query_logging_callback=None, agent_config=AgentConfig(), fallback_agent_config=None, chat_history=None, verbose=False, vectara_filter_fields=[], vectara_offset=0, vectara_lambda_val=0.005, vectara_semantics='default', vectara_custom_dimensions={}, vectara_reranker='slingshot', vectara_rerank_k=50, vectara_rerank_limit=None, vectara_rerank_cutoff=None, vectara_diversity_bias=0.2, vectara_udf_expression=None, vectara_rerank_chain=None, vectara_n_sentences_before=2, vectara_n_sentences_after=2, vectara_summary_num_results=10, vectara_summarizer='vectara-summary-ext-24-05-med-omni', vectara_summary_response_language='eng', vectara_summary_prompt_text=None, vectara_max_response_chars=None, vectara_max_tokens=None, vectara_temperature=None, vectara_frequency_penalty=None, vectara_presence_penalty=None, vectara_save_history=True, return_direct=False)</code>  <code>classmethod</code>","text":"<p>Create an agent from a single Vectara corpus</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>The name of Vectara tool used by the agent</p> required <code>vectara_corpus_key</code> <code>str</code> <p>The Vectara corpus key (or comma separated list of keys).</p> <code>str(get('VECTARA_CORPUS_KEY', ''))</code> <code>vectara_api_key</code> <code>str</code> <p>The Vectara API key.</p> <code>str(get('VECTARA_API_KEY', ''))</code> <code>agent_progress_callback</code> <code>Callable</code> <p>A callback function the code calls on any agent updates.</p> <code>None</code> <code>query_logging_callback</code> <code>Callable</code> <p>A callback function the code calls upon completion of a query</p> <code>None</code> <code>agent_config</code> <code>AgentConfig</code> <p>The configuration of the agent.</p> <code>AgentConfig()</code> <code>fallback_agent_config</code> <code>AgentConfig</code> <p>The fallback configuration of the agent.</p> <code>None</code> <code>chat_history</code> <code>Tuple[str, str]</code> <p>A list of user/agent chat pairs to initialize the agent memory.</p> <code>None</code> <code>data_description</code> <code>str</code> <p>The description of the data.</p> required <code>assistant_specialty</code> <code>str</code> <p>The specialty of the assistant.</p> required <code>general_instructions</code> <code>str</code> <p>General instructions for the agent. The Agent has a default set of instructions that are crafted to help it operate effectively. This allows you to customize the agent's behavior and personality, but use with caution.</p> <code>GENERAL_INSTRUCTIONS</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose output.</p> <code>False</code> <code>vectara_filter_fields</code> <code>List[dict]</code> <p>The filterable attributes (each dict maps field name to Tuple[type, description]).</p> <code>[]</code> <code>vectara_offset</code> <code>int</code> <p>Number of results to skip.</p> <code>0</code> <code>vectara_lambda_val</code> <code>float</code> <p>Lambda value for Vectara hybrid search.</p> <code>0.005</code> <code>vectara_semantics</code> <code>str</code> <p>(str, optional): Indicates whether the query is intended as a query or response.</p> <code>'default'</code> <code>vectara_custom_dimensions</code> <code>Dict</code> <p>(Dict, optional): Custom dimensions for the query.</p> <code>{}</code> <code>vectara_reranker</code> <code>str</code> <p>The Vectara reranker name (default \"slingshot\")</p> <code>'slingshot'</code> <code>vectara_rerank_k</code> <code>int</code> <p>The number of results to use with reranking.</p> <code>50</code> <code>vectara_rerank_limit</code> <code>Optional[int]</code> <p>(int, optional): The maximum number of results to return after reranking.</p> <code>None</code> <code>vectara_rerank_cutoff</code> <code>Optional[float]</code> <p>(float, optional): The minimum score threshold for results to include after reranking.</p> <code>None</code> <code>vectara_diversity_bias</code> <code>float</code> <p>The MMR diversity bias.</p> <code>0.2</code> <code>vectara_udf_expression</code> <code>str</code> <p>The user defined expression for reranking results.</p> <code>None</code> <code>vectara_rerank_chain</code> <code>List[Dict]</code> <p>A list of Vectara rerankers to be applied sequentially.</p> <code>None</code> <code>vectara_n_sentences_before</code> <code>int</code> <p>The number of sentences before the matching text</p> <code>2</code> <code>vectara_n_sentences_after</code> <code>int</code> <p>The number of sentences after the matching text.</p> <code>2</code> <code>vectara_summary_num_results</code> <code>int</code> <p>The number of results to use in summarization.</p> <code>10</code> <code>vectara_summarizer</code> <code>str</code> <p>The Vectara summarizer name.</p> <code>'vectara-summary-ext-24-05-med-omni'</code> <code>vectara_summary_response_language</code> <code>str</code> <p>The response language for the Vectara summary.</p> <code>'eng'</code> <code>vectara_summary_prompt_text</code> <code>str</code> <p>The custom prompt, using appropriate prompt variables and functions.</p> <code>None</code> <code>vectara_max_response_chars</code> <code>int</code> <p>The desired maximum number of characters for the generated summary.</p> <code>None</code> <code>vectara_max_tokens</code> <code>int</code> <p>The maximum number of tokens to be returned by the LLM.</p> <code>None</code> <code>vectara_temperature</code> <code>float</code> <p>The sampling temperature; higher values lead to more randomness.</p> <code>None</code> <code>vectara_frequency_penalty</code> <code>float</code> <p>How much to penalize repeating tokens in the response, higher values reducing likelihood of repeating the same line.</p> <code>None</code> <code>vectara_presence_penalty</code> <code>float</code> <p>How much to penalize repeating tokens in the response, higher values increasing the diversity of topics.</p> <code>None</code> <code>vectara_save_history</code> <code>bool</code> <p>Whether to save the query in history.</p> <code>True</code> <code>return_direct</code> <code>bool</code> <p>Whether the agent should return the tool's response directly.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Agent</code> <code>Agent</code> <p>An instance of the Agent class.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>@classmethod\ndef from_corpus(\n    cls,\n    tool_name: str,\n    data_description: str,\n    assistant_specialty: str,\n    general_instructions: str = GENERAL_INSTRUCTIONS,\n    vectara_corpus_key: str = str(os.environ.get(\"VECTARA_CORPUS_KEY\", \"\")),\n    vectara_api_key: str = str(os.environ.get(\"VECTARA_API_KEY\", \"\")),\n    agent_progress_callback: Optional[\n        Callable[[AgentStatusType, str], None]\n    ] = None,\n    query_logging_callback: Optional[Callable[[str, str], None]] = None,\n    agent_config: AgentConfig = AgentConfig(),\n    fallback_agent_config: Optional[AgentConfig] = None,\n    chat_history: Optional[list[Tuple[str, str]]] = None,\n    verbose: bool = False,\n    vectara_filter_fields: list[dict] = [],\n    vectara_offset: int = 0,\n    vectara_lambda_val: float = 0.005,\n    vectara_semantics: str = \"default\",\n    vectara_custom_dimensions: Dict = {},\n    vectara_reranker: str = \"slingshot\",\n    vectara_rerank_k: int = 50,\n    vectara_rerank_limit: Optional[int] = None,\n    vectara_rerank_cutoff: Optional[float] = None,\n    vectara_diversity_bias: float = 0.2,\n    vectara_udf_expression: Optional[str] = None,\n    vectara_rerank_chain: Optional[List[Dict]] = None,\n    vectara_n_sentences_before: int = 2,\n    vectara_n_sentences_after: int = 2,\n    vectara_summary_num_results: int = 10,\n    vectara_summarizer: str = \"vectara-summary-ext-24-05-med-omni\",\n    vectara_summary_response_language: str = \"eng\",\n    vectara_summary_prompt_text: Optional[str] = None,\n    vectara_max_response_chars: Optional[int] = None,\n    vectara_max_tokens: Optional[int] = None,\n    vectara_temperature: Optional[float] = None,\n    vectara_frequency_penalty: Optional[float] = None,\n    vectara_presence_penalty: Optional[float] = None,\n    vectara_save_history: bool = True,\n    return_direct: bool = False,\n) -&gt; \"Agent\":\n    \"\"\"\n    Create an agent from a single Vectara corpus\n\n    Args:\n        tool_name (str): The name of Vectara tool used by the agent\n        vectara_corpus_key (str): The Vectara corpus key (or comma separated list of keys).\n        vectara_api_key (str): The Vectara API key.\n        agent_progress_callback (Callable): A callback function the code calls on any agent updates.\n        query_logging_callback (Callable): A callback function the code calls upon completion of a query\n        agent_config (AgentConfig, optional): The configuration of the agent.\n        fallback_agent_config (AgentConfig, optional): The fallback configuration of the agent.\n        chat_history (Tuple[str, str], optional): A list of user/agent chat pairs to initialize the agent memory.\n        data_description (str): The description of the data.\n        assistant_specialty (str): The specialty of the assistant.\n        general_instructions (str, optional): General instructions for the agent.\n            The Agent has a default set of instructions that are crafted to help it operate effectively.\n            This allows you to customize the agent's behavior and personality, but use with caution.\n        verbose (bool, optional): Whether to print verbose output.\n        vectara_filter_fields (List[dict], optional): The filterable attributes\n            (each dict maps field name to Tuple[type, description]).\n        vectara_offset (int, optional): Number of results to skip.\n        vectara_lambda_val (float, optional): Lambda value for Vectara hybrid search.\n        vectara_semantics: (str, optional): Indicates whether the query is intended as a query or response.\n        vectara_custom_dimensions: (Dict, optional): Custom dimensions for the query.\n        vectara_reranker (str, optional): The Vectara reranker name (default \"slingshot\")\n        vectara_rerank_k (int, optional): The number of results to use with reranking.\n        vectara_rerank_limit: (int, optional): The maximum number of results to return after reranking.\n        vectara_rerank_cutoff: (float, optional): The minimum score threshold for results to include after\n            reranking.\n        vectara_diversity_bias (float, optional): The MMR diversity bias.\n        vectara_udf_expression (str, optional): The user defined expression for reranking results.\n        vectara_rerank_chain (List[Dict], optional): A list of Vectara rerankers to be applied sequentially.\n        vectara_n_sentences_before (int, optional): The number of sentences before the matching text\n        vectara_n_sentences_after (int, optional): The number of sentences after the matching text.\n        vectara_summary_num_results (int, optional): The number of results to use in summarization.\n        vectara_summarizer (str, optional): The Vectara summarizer name.\n        vectara_summary_response_language (str, optional): The response language for the Vectara summary.\n        vectara_summary_prompt_text (str, optional): The custom prompt, using appropriate prompt variables and\n            functions.\n        vectara_max_response_chars (int, optional): The desired maximum number of characters for the generated\n            summary.\n        vectara_max_tokens (int, optional): The maximum number of tokens to be returned by the LLM.\n        vectara_temperature (float, optional): The sampling temperature; higher values lead to more randomness.\n        vectara_frequency_penalty (float, optional): How much to penalize repeating tokens in the response,\n            higher values reducing likelihood of repeating the same line.\n        vectara_presence_penalty (float, optional): How much to penalize repeating tokens in the response,\n            higher values increasing the diversity of topics.\n        vectara_save_history (bool, optional): Whether to save the query in history.\n        return_direct (bool, optional): Whether the agent should return the tool's response directly.\n\n    Returns:\n        Agent: An instance of the Agent class.\n    \"\"\"\n    vec_factory = VectaraToolFactory(\n        vectara_api_key=vectara_api_key,\n        vectara_corpus_key=vectara_corpus_key,\n    )\n    field_definitions = {}\n    field_definitions[\"query\"] = (str, Field(description=\"The user query\"))  # type: ignore\n    for field in vectara_filter_fields:\n        field_definitions[field[\"name\"]] = (\n            eval(field[\"type\"]),\n            Field(description=field[\"description\"]),\n        )  # type: ignore\n    query_args = create_model(\"QueryArgs\", **field_definitions)  # type: ignore\n\n    # tool name must be valid Python function name\n    if tool_name:\n        tool_name = re.sub(r\"[^A-Za-z0-9_]\", \"_\", tool_name)\n\n    vectara_tool = vec_factory.create_rag_tool(\n        tool_name=tool_name or f\"vectara_{vectara_corpus_key}\",\n        tool_description=f\"\"\"\n        Given a user query,\n        returns a response (str) to a user question about {data_description}.\n        \"\"\",\n        tool_args_schema=query_args,\n        reranker=vectara_reranker,\n        rerank_k=vectara_rerank_k,\n        rerank_limit=vectara_rerank_limit,\n        rerank_cutoff=vectara_rerank_cutoff,\n        mmr_diversity_bias=vectara_diversity_bias,\n        udf_expression=vectara_udf_expression,\n        rerank_chain=vectara_rerank_chain,\n        n_sentences_before=vectara_n_sentences_before,\n        n_sentences_after=vectara_n_sentences_after,\n        offset=vectara_offset,\n        lambda_val=vectara_lambda_val,\n        semantics=vectara_semantics,\n        custom_dimensions=vectara_custom_dimensions,\n        summary_num_results=vectara_summary_num_results,\n        vectara_summarizer=vectara_summarizer,\n        summary_response_lang=vectara_summary_response_language,\n        vectara_prompt_text=vectara_summary_prompt_text,\n        max_response_chars=vectara_max_response_chars,\n        max_tokens=vectara_max_tokens,\n        temperature=vectara_temperature,\n        frequency_penalty=vectara_frequency_penalty,\n        presence_penalty=vectara_presence_penalty,\n        save_history=vectara_save_history,\n        include_citations=True,\n        verbose=verbose,\n        return_direct=return_direct,\n    )\n\n    assistant_instructions = f\"\"\"\n    - You are a helpful {assistant_specialty} assistant.\n    - You can answer questions about {data_description}.\n    - Never discuss politics, and always respond politely.\n    \"\"\"\n\n    return cls(\n        tools=[vectara_tool],\n        topic=assistant_specialty,\n        custom_instructions=assistant_instructions,\n        general_instructions=general_instructions,\n        verbose=verbose,\n        agent_progress_callback=agent_progress_callback,\n        query_logging_callback=query_logging_callback,\n        agent_config=agent_config,\n        fallback_agent_config=fallback_agent_config,\n        chat_history=chat_history,\n    )\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create an Agent instance from a dictionary.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; \"Agent\":\n    \"\"\"Create an Agent instance from a dictionary.\"\"\"\n    agent_config = AgentConfig.from_dict(data[\"agent_config\"])\n    fallback_agent_config = (\n        AgentConfig.from_dict(data[\"fallback_agent_config\"])\n        if data.get(\"fallback_agent_config\")\n        else None\n    )\n    tools: list[FunctionTool] = []\n\n    for tool_data in data[\"tools\"]:\n        query_args_model = None\n        if tool_data.get(\"fn_schema\"):\n            schema_info = tool_data[\"fn_schema\"]\n            try:\n                module_name = schema_info[\"metadata\"][\"module\"]\n                class_name = schema_info[\"metadata\"][\"class\"]\n                mod = importlib.import_module(module_name)\n                candidate_cls = getattr(mod, class_name)\n                if inspect.isclass(candidate_cls) and issubclass(\n                    candidate_cls, BaseModel\n                ):\n                    query_args_model = candidate_cls\n                else:\n                    # It's not the Pydantic model class we expected (e.g., it's the function itself)\n                    # Force fallback to JSON schema reconstruction by raising an error.\n                    raise ImportError(\n                        f\"Retrieved '{class_name}' from '{module_name}' is not a Pydantic BaseModel class. \"\n                        \"Falling back to JSON schema reconstruction.\"\n                    )\n            except Exception:\n                # Fallback: rebuild using the JSON schema\n                field_definitions = {}\n                json_schema_to_rebuild = schema_info.get(\"schema\")\n                if json_schema_to_rebuild and isinstance(\n                    json_schema_to_rebuild, dict\n                ):\n                    for field, values in json_schema_to_rebuild.get(\n                        \"properties\", {}\n                    ).items():\n                        field_type = get_field_type(values)\n                        field_description = values.get(\n                            \"description\"\n                        )  # Defaults to None\n                        if \"default\" in values:\n                            field_definitions[field] = (\n                                field_type,\n                                Field(\n                                    description=field_description,\n                                    default=values[\"default\"],\n                                ),\n                            )\n                        else:\n                            field_definitions[field] = (\n                                field_type,\n                                Field(description=field_description),\n                            )\n                    query_args_model = create_model(\n                        json_schema_to_rebuild.get(\n                            \"title\", f\"{tool_data['name']}_QueryArgs\"\n                        ),\n                        **field_definitions,\n                    )\n                else:  # If schema part is missing or not a dict, create a default empty model\n                    query_args_model = create_model(\n                        f\"{tool_data['name']}_QueryArgs\"\n                    )\n\n        # If fn_schema was not in tool_data or reconstruction failed badly, default to empty pydantic model\n        if query_args_model is None:\n            query_args_model = create_model(f\"{tool_data['name']}_QueryArgs\")\n\n        fn = (\n            pickle.loads(tool_data[\"fn\"].encode(\"latin-1\"))\n            if tool_data[\"fn\"]\n            else None\n        )\n        async_fn = (\n            pickle.loads(tool_data[\"async_fn\"].encode(\"latin-1\"))\n            if tool_data[\"async_fn\"]\n            else None\n        )\n\n        tool = VectaraTool.from_defaults(\n            name=tool_data[\"name\"],\n            description=tool_data[\"description\"],\n            fn=fn,\n            async_fn=async_fn,\n            fn_schema=query_args_model,  # Re-assign the recreated dynamic model\n            tool_type=ToolType(tool_data[\"tool_type\"]),\n        )\n        tools.append(tool)\n\n    agent = cls(\n        tools=tools,\n        agent_config=agent_config,\n        topic=data[\"topic\"],\n        custom_instructions=data[\"custom_instructions\"],\n        verbose=data[\"verbose\"],\n        fallback_agent_config=fallback_agent_config,\n        workflow_cls=data[\"workflow_cls\"],\n    )\n    memory = (\n        pickle.loads(data[\"memory\"].encode(\"latin-1\"))\n        if data.get(\"memory\")\n        else None\n    )\n    if memory:\n        agent.agent.memory = memory\n    return agent\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.from_tools","title":"<code>from_tools(tools, topic='general', custom_instructions='', verbose=True, update_func=None, agent_progress_callback=None, query_logging_callback=None, agent_config=AgentConfig(), validate_tools=False, fallback_agent_config=None, chat_history=None, workflow_cls=None, workflow_timeout=120)</code>  <code>classmethod</code>","text":"<p>Create an agent from tools, agent type, and language model.</p> <p>Args:</p> <pre><code>tools (list[FunctionTool]): A list of tools to be used by the agent.\ntopic (str, optional): The topic for the agent. Defaults to 'general'.\ncustom_instructions (str, optional): custom instructions for the agent. Defaults to ''.\nverbose (bool, optional): Whether the agent should print its steps. Defaults to True.\nagent_progress_callback (Callable): A callback function the code calls on any agent updates.\n    update_func (Callable): old name for agent_progress_callback. Will be deprecated in future.\nquery_logging_callback (Callable): A callback function the code calls upon completion of a query\nagent_config (AgentConfig, optional): The configuration of the agent.\nfallback_agent_config (AgentConfig, optional): The fallback configuration of the agent.\nchat_history (Tuple[str, str], optional): A list of user/agent chat pairs to initialize the agent memory.\nvalidate_tools (bool, optional): Whether to validate tool inconsistency with instructions.\n    Defaults to False.\nworkflow_cls (Workflow, optional): The workflow class to be used with run(). Defaults to None.\nworkflow_timeout (int, optional): The timeout for the workflow in seconds. Defaults to 120.\n</code></pre> <p>Returns:</p> Name Type Description <code>Agent</code> <code>Agent</code> <p>An instance of the Agent class.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>@classmethod\ndef from_tools(\n    cls,\n    tools: List[FunctionTool],\n    topic: str = \"general\",\n    custom_instructions: str = \"\",\n    verbose: bool = True,\n    update_func: Optional[Callable[[AgentStatusType, str], None]] = None,\n    agent_progress_callback: Optional[\n        Callable[[AgentStatusType, str], None]\n    ] = None,\n    query_logging_callback: Optional[Callable[[str, str], None]] = None,\n    agent_config: AgentConfig = AgentConfig(),\n    validate_tools: bool = False,\n    fallback_agent_config: Optional[AgentConfig] = None,\n    chat_history: Optional[list[Tuple[str, str]]] = None,\n    workflow_cls: Optional[Workflow] = None,\n    workflow_timeout: int = 120,\n) -&gt; \"Agent\":\n    \"\"\"\n    Create an agent from tools, agent type, and language model.\n\n    Args:\n\n        tools (list[FunctionTool]): A list of tools to be used by the agent.\n        topic (str, optional): The topic for the agent. Defaults to 'general'.\n        custom_instructions (str, optional): custom instructions for the agent. Defaults to ''.\n        verbose (bool, optional): Whether the agent should print its steps. Defaults to True.\n        agent_progress_callback (Callable): A callback function the code calls on any agent updates.\n            update_func (Callable): old name for agent_progress_callback. Will be deprecated in future.\n        query_logging_callback (Callable): A callback function the code calls upon completion of a query\n        agent_config (AgentConfig, optional): The configuration of the agent.\n        fallback_agent_config (AgentConfig, optional): The fallback configuration of the agent.\n        chat_history (Tuple[str, str], optional): A list of user/agent chat pairs to initialize the agent memory.\n        validate_tools (bool, optional): Whether to validate tool inconsistency with instructions.\n            Defaults to False.\n        workflow_cls (Workflow, optional): The workflow class to be used with run(). Defaults to None.\n        workflow_timeout (int, optional): The timeout for the workflow in seconds. Defaults to 120.\n\n    Returns:\n        Agent: An instance of the Agent class.\n    \"\"\"\n    return cls(\n        tools=tools,\n        topic=topic,\n        custom_instructions=custom_instructions,\n        verbose=verbose,\n        agent_progress_callback=agent_progress_callback,\n        query_logging_callback=query_logging_callback,\n        update_func=update_func,\n        agent_config=agent_config,\n        chat_history=chat_history,\n        validate_tools=validate_tools,\n        fallback_agent_config=fallback_agent_config,\n        workflow_cls=workflow_cls,\n        workflow_timeout=workflow_timeout,\n    )\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.loads","title":"<code>loads(data)</code>  <code>classmethod</code>","text":"<p>Create an Agent instance from a JSON string.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>@classmethod\ndef loads(cls, data: str) -&gt; \"Agent\":\n    \"\"\"Create an Agent instance from a JSON string.\"\"\"\n    return cls.from_dict(json.loads(data))\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.report","title":"<code>report(detailed=False)</code>","text":"<p>Get a report from the agent.</p> <p>Parameters:</p> Name Type Description Default <code>detailed</code> <code>bool</code> <p>Whether to include detailed information. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>None</code> <p>The report from the agent.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>def report(self, detailed: bool = False) -&gt; None:\n    \"\"\"\n    Get a report from the agent.\n\n    Args:\n        detailed (bool, optional): Whether to include detailed information. Defaults to False.\n\n    Returns:\n        str: The report from the agent.\n    \"\"\"\n    print(\"Vectara agentic Report:\")\n    print(f\"Agent Type = {self.agent_config.agent_type}\")\n    print(f\"Topic = {self._topic}\")\n    print(\"Tools:\")\n    for tool in self.tools:\n        if hasattr(tool, \"metadata\"):\n            if detailed:\n                print(f\"- {tool.metadata.description}\")\n            else:\n                print(f\"- {tool.metadata.name}\")\n        else:\n            print(\"- tool without metadata\")\n    print(\n        f\"Agent LLM = {get_llm(LLMRole.MAIN, config=self.agent_config).metadata.model_name}\"\n    )\n    print(\n        f\"Tool LLM = {get_llm(LLMRole.TOOL, config=self.agent_config).metadata.model_name}\"\n    )\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.run","title":"<code>run(inputs, verbose=False)</code>  <code>async</code>","text":"<p>Run a workflow using the agent. workflow class must be provided in the agent constructor. Args:     inputs (Any): The inputs to the workflow.     verbose (bool, optional): Whether to print verbose output. Defaults to False. Returns:     Any: The output or context of the workflow.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>async def run(\n    self,\n    inputs: Any,\n    verbose: bool = False,\n) -&gt; Any:\n    \"\"\"\n    Run a workflow using the agent.\n    workflow class must be provided in the agent constructor.\n    Args:\n        inputs (Any): The inputs to the workflow.\n        verbose (bool, optional): Whether to print verbose output. Defaults to False.\n    Returns:\n        Any: The output or context of the workflow.\n    \"\"\"\n    # Create workflow\n    if self.workflow_cls:\n        workflow = self.workflow_cls(timeout=self.workflow_timeout, verbose=verbose)\n    else:\n        raise ValueError(\"Workflow is not defined.\")\n\n    # Validate inputs is in the form of workflow.InputsModel\n    if not isinstance(inputs, self.workflow_cls.InputsModel):\n        raise ValueError(f\"Inputs must be an instance of {workflow.InputsModel}.\")\n\n    workflow_context = Context(workflow=workflow)\n    try:\n        # run workflow\n        result = await workflow.run(\n            ctx=workflow_context,\n            agent=self,\n            tools=self.tools,\n            llm=self.llm,\n            verbose=verbose,\n            inputs=inputs,\n        )\n\n        # return output in the form of workflow.OutputsModel(BaseModel)\n        try:\n            output = workflow.OutputsModel.model_validate(result)\n        except ValidationError as e:\n            raise ValueError(f\"Failed to map workflow output to model: {e}\") from e\n\n    except Exception as e:\n        outputs_model_on_fail_cls = getattr(workflow.__class__, \"OutputModelOnFail\", None)\n        if outputs_model_on_fail_cls:\n            model_fields = outputs_model_on_fail_cls.model_fields\n            input_dict = {\n                key: await workflow_context.get(key, None)\n                for key in model_fields\n            }\n\n            # return output in the form of workflow.OutputModelOnFail(BaseModel)\n            output = outputs_model_on_fail_cls.model_validate(input_dict)\n        else:\n            print(f\"Vectara Agentic: Workflow failed with unexpected error: {e}\")\n            raise type(e)(str(e)).with_traceback(e.__traceback__)\n\n    return output\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.stream_chat","title":"<code>stream_chat(prompt)</code>","text":"<p>Interact with the agent using a chat prompt with streaming. Args:     prompt (str): The chat prompt. Returns:     AgentStreamingResponse: The streaming response from the agent.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>def stream_chat(self, prompt: str) -&gt; AgentStreamingResponse:  # type: ignore\n    \"\"\"\n    Interact with the agent using a chat prompt with streaming.\n    Args:\n        prompt (str): The chat prompt.\n    Returns:\n        AgentStreamingResponse: The streaming response from the agent.\n    \"\"\"\n    return asyncio.run(self.astream_chat(prompt))\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.to_dict","title":"<code>to_dict()</code>","text":"<p>Serialize the Agent instance to a dictionary.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Serialize the Agent instance to a dictionary.\"\"\"\n    tool_info = []\n    for tool in self.tools:\n        if hasattr(tool.metadata, \"fn_schema\"):\n            fn_schema_cls = tool.metadata.fn_schema\n            fn_schema_serialized = {\n                \"schema\": (\n                    fn_schema_cls.model_json_schema()\n                    if fn_schema_cls and hasattr(fn_schema_cls, \"model_json_schema\")\n                    else None\n                ),\n                \"metadata\": {\n                    \"module\": fn_schema_cls.__module__ if fn_schema_cls else None,\n                    \"class\": fn_schema_cls.__name__ if fn_schema_cls else None,\n                },\n            }\n        else:\n            fn_schema_serialized = None\n\n        tool_dict = {\n            \"tool_type\": tool.metadata.tool_type.value,\n            \"name\": tool.metadata.name,\n            \"description\": tool.metadata.description,\n            \"fn\": (\n                pickle.dumps(getattr(tool, \"fn\", None)).decode(\"latin-1\")\n                if getattr(tool, \"fn\", None)\n                else None\n            ),\n            \"async_fn\": (\n                pickle.dumps(getattr(tool, \"async_fn\", None)).decode(\"latin-1\")\n                if getattr(tool, \"async_fn\", None)\n                else None\n            ),\n            \"fn_schema\": fn_schema_serialized,\n        }\n        tool_info.append(tool_dict)\n\n    return {\n        \"agent_type\": self.agent_config.agent_type.value,\n        \"memory\": pickle.dumps(self.agent.memory).decode(\"latin-1\"),\n        \"tools\": tool_info,\n        \"topic\": self._topic,\n        \"custom_instructions\": self._custom_instructions,\n        \"verbose\": self.verbose,\n        \"agent_config\": self.agent_config.to_dict(),\n        \"fallback_agent\": (\n            self.fallback_agent_config.to_dict()\n            if self.fallback_agent_config\n            else None\n        ),\n        \"workflow_cls\": self.workflow_cls if self.workflow_cls else None,\n    }\n</code></pre>"},{"location":"api/#vectara_agentic.Agent.token_counts","title":"<code>token_counts()</code>","text":"<p>Get the token counts for the agent and tools.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The token counts for the agent and tools.</p> Source code in <code>vectara_agentic/agent.py</code> <pre><code>def token_counts(self) -&gt; dict:\n    \"\"\"\n    Get the token counts for the agent and tools.\n\n    Returns:\n        dict: The token counts for the agent and tools.\n    \"\"\"\n    return {\n        \"main token count\": (\n            self.main_token_counter.total_llm_token_count\n            if self.main_token_counter\n            else -1\n        ),\n        \"tool token count\": (\n            self.tool_token_counter.total_llm_token_count\n            if self.tool_token_counter\n            else -1\n        ),\n    }\n</code></pre>"},{"location":"api/#vectara_agentic.AgentConfig","title":"<code>AgentConfig</code>  <code>dataclass</code>","text":"<p>Centralized configuration for the Vectara Agentic utilities.</p> <p>Each field can default to either a hard-coded value or an environment variable. For example, if you have environment variables you want to fall back on, you can default to them here.</p> Source code in <code>vectara_agentic/agent_config.py</code> <pre><code>@dataclass(eq=True, frozen=True)\nclass AgentConfig:\n    \"\"\"\n    Centralized configuration for the Vectara Agentic utilities.\n\n    Each field can default to either a hard-coded value or an environment\n    variable. For example, if you have environment variables you want to\n    fall back on, you can default to them here.\n    \"\"\"\n\n    # Agent type\n    agent_type: AgentType = field(\n        default_factory=lambda: AgentType(\n            os.getenv(\"VECTARA_AGENTIC_AGENT_TYPE\", AgentType.OPENAI.value)\n        )\n    )\n\n    # Main LLM provider &amp; model name\n    main_llm_provider: ModelProvider = field(\n        default_factory=lambda: ModelProvider(\n            os.getenv(\"VECTARA_AGENTIC_MAIN_LLM_PROVIDER\", ModelProvider.OPENAI.value)\n        )\n    )\n\n    main_llm_model_name: str = field(\n        default_factory=lambda: os.getenv(\"VECTARA_AGENTIC_MAIN_MODEL_NAME\", \"\")\n    )\n\n    # Tool LLM provider &amp; model name\n    tool_llm_provider: ModelProvider = field(\n        default_factory=lambda: ModelProvider(\n            os.getenv(\"VECTARA_AGENTIC_TOOL_LLM_PROVIDER\", ModelProvider.OPENAI.value)\n        )\n    )\n    tool_llm_model_name: str = field(\n        default_factory=lambda: os.getenv(\"VECTARA_AGENTIC_TOOL_MODEL_NAME\", \"\")\n    )\n\n    # Params for Private LLM endpoint if used\n    private_llm_api_base: str = field(\n        default_factory=lambda: os.getenv(\"VECTARA_AGENTIC_PRIVATE_LLM_API_BASE\",\n                                          \"http://private-endpoint.company.com:5000/v1\")\n    )\n    private_llm_api_key: str = field(\n        default_factory=lambda: os.getenv(\"VECTARA_AGENTIC_PRIVATE_LLM_API_KEY\", \"&lt;private-api-key&gt;\")\n    )\n\n    # Observer\n    observer: ObserverType = field(\n        default_factory=lambda: ObserverType(\n            os.getenv(\"VECTARA_AGENTIC_OBSERVER_TYPE\", \"NO_OBSERVER\")\n        )\n    )\n\n    # Endpoint API key\n    endpoint_api_key: str = field(\n        default_factory=lambda: os.getenv(\"VECTARA_AGENTIC_API_KEY\", \"dev-api-key\")\n    )\n\n    # max reasoning steps\n    # used for both OpenAI and React Agent types\n    max_reasoning_steps: int = field(\n        default_factory=lambda: int(os.getenv(\"VECTARA_AGENTIC_MAX_REASONING_STEPS\", \"50\"))\n    )\n\n    def __post_init__(self):\n        # Use object.__setattr__ since the dataclass is frozen\n        if isinstance(self.agent_type, str):\n            object.__setattr__(self, \"agent_type\", AgentType(self.agent_type))\n        if isinstance(self.main_llm_provider, str):\n            object.__setattr__(self, \"main_llm_provider\", ModelProvider(self.main_llm_provider))\n        if isinstance(self.tool_llm_provider, str):\n            object.__setattr__(self, \"tool_llm_provider\", ModelProvider(self.tool_llm_provider))\n        if isinstance(self.observer, str):\n            object.__setattr__(self, \"observer\", ObserverType(self.observer))\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"\n        Convert the AgentConfig to a dictionary.\n        \"\"\"\n        return {\n            \"agent_type\": self.agent_type.value,\n            \"main_llm_provider\": self.main_llm_provider.value,\n            \"main_llm_model_name\": self.main_llm_model_name,\n            \"tool_llm_provider\": self.tool_llm_provider.value,\n            \"tool_llm_model_name\": self.tool_llm_model_name,\n            \"observer\": self.observer.value,\n            \"endpoint_api_key\": self.endpoint_api_key,\n            \"max_reasoning_steps\": self.max_reasoning_steps\n        }\n\n    @classmethod\n    def from_dict(cls, config_dict: dict) -&gt; \"AgentConfig\":\n        \"\"\"\n        Create an AgentConfig from a dictionary.\n        \"\"\"\n        return cls(\n            agent_type=AgentType(config_dict[\"agent_type\"]),\n            main_llm_provider=ModelProvider(config_dict[\"main_llm_provider\"]),\n            main_llm_model_name=config_dict[\"main_llm_model_name\"],\n            tool_llm_provider=ModelProvider(config_dict[\"tool_llm_provider\"]),\n            tool_llm_model_name=config_dict[\"tool_llm_model_name\"],\n            observer=ObserverType(config_dict[\"observer\"]),\n            endpoint_api_key=config_dict[\"endpoint_api_key\"],\n            max_reasoning_steps=config_dict[\"max_reasoning_steps\"]\n        )\n</code></pre>"},{"location":"api/#vectara_agentic.AgentConfig.from_dict","title":"<code>from_dict(config_dict)</code>  <code>classmethod</code>","text":"<p>Create an AgentConfig from a dictionary.</p> Source code in <code>vectara_agentic/agent_config.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: dict) -&gt; \"AgentConfig\":\n    \"\"\"\n    Create an AgentConfig from a dictionary.\n    \"\"\"\n    return cls(\n        agent_type=AgentType(config_dict[\"agent_type\"]),\n        main_llm_provider=ModelProvider(config_dict[\"main_llm_provider\"]),\n        main_llm_model_name=config_dict[\"main_llm_model_name\"],\n        tool_llm_provider=ModelProvider(config_dict[\"tool_llm_provider\"]),\n        tool_llm_model_name=config_dict[\"tool_llm_model_name\"],\n        observer=ObserverType(config_dict[\"observer\"]),\n        endpoint_api_key=config_dict[\"endpoint_api_key\"],\n        max_reasoning_steps=config_dict[\"max_reasoning_steps\"]\n    )\n</code></pre>"},{"location":"api/#vectara_agentic.AgentConfig.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert the AgentConfig to a dictionary.</p> Source code in <code>vectara_agentic/agent_config.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"\n    Convert the AgentConfig to a dictionary.\n    \"\"\"\n    return {\n        \"agent_type\": self.agent_type.value,\n        \"main_llm_provider\": self.main_llm_provider.value,\n        \"main_llm_model_name\": self.main_llm_model_name,\n        \"tool_llm_provider\": self.tool_llm_provider.value,\n        \"tool_llm_model_name\": self.tool_llm_model_name,\n        \"observer\": self.observer.value,\n        \"endpoint_api_key\": self.endpoint_api_key,\n        \"max_reasoning_steps\": self.max_reasoning_steps\n    }\n</code></pre>"},{"location":"api/#vectara_agentic.AgentStatusType","title":"<code>AgentStatusType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration for different types of agent statuses.</p> Source code in <code>vectara_agentic/types.py</code> <pre><code>class AgentStatusType(Enum):\n    \"\"\"Enumeration for different types of agent statuses.\"\"\"\n\n    AGENT_UPDATE = \"agent_update\"\n    TOOL_CALL = \"tool_call\"\n    TOOL_OUTPUT = \"tool_output\"\n    AGENT_STEP = \"agent_step\"\n</code></pre>"},{"location":"api/#vectara_agentic.AgentType","title":"<code>AgentType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration for different types of agents.</p> Source code in <code>vectara_agentic/types.py</code> <pre><code>class AgentType(Enum):\n    \"\"\"Enumeration for different types of agents.\"\"\"\n\n    REACT = \"REACT\"\n    OPENAI = \"OPENAI\"\n    FUNCTION_CALLING = \"FUNCTION_CALLING\"\n    LLMCOMPILER = \"LLMCOMPILER\"\n    LATS = \"LATS\"\n</code></pre>"},{"location":"api/#vectara_agentic.LLMRole","title":"<code>LLMRole</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration for different types of LLM roles.</p> Source code in <code>vectara_agentic/types.py</code> <pre><code>class LLMRole(Enum):\n    \"\"\"Enumeration for different types of LLM roles.\"\"\"\n\n    MAIN = \"MAIN\"\n    TOOL = \"TOOL\"\n</code></pre>"},{"location":"api/#vectara_agentic.ModelProvider","title":"<code>ModelProvider</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration for different types of model providers.</p> Source code in <code>vectara_agentic/types.py</code> <pre><code>class ModelProvider(Enum):\n    \"\"\"Enumeration for different types of model providers.\"\"\"\n\n    OPENAI = \"OPENAI\"\n    ANTHROPIC = \"ANTHROPIC\"\n    TOGETHER = \"TOGETHER\"\n    GROQ = \"GROQ\"\n    FIREWORKS = \"FIREWORKS\"\n    COHERE = \"COHERE\"\n    GEMINI = \"GEMINI\"\n    BEDROCK = \"BEDROCK\"\n    PRIVATE = \"PRIVATE\"\n</code></pre>"},{"location":"api/#vectara_agentic.ObserverType","title":"<code>ObserverType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration for different types of observability integrations.</p> Source code in <code>vectara_agentic/types.py</code> <pre><code>class ObserverType(Enum):\n    \"\"\"Enumeration for different types of observability integrations.\"\"\"\n\n    NO_OBSERVER = \"NO_OBSERVER\"\n    ARIZE_PHOENIX = \"ARIZE_PHOENIX\"\n</code></pre>"},{"location":"api/#vectara_agentic.ToolType","title":"<code>ToolType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration for different types of tools.</p> Source code in <code>vectara_agentic/types.py</code> <pre><code>class ToolType(Enum):\n    \"\"\"Enumeration for different types of tools.\"\"\"\n    QUERY = \"query\"\n    ACTION = \"action\"\n</code></pre>"},{"location":"api/#vectara_agentic.ToolsCatalog","title":"<code>ToolsCatalog</code>","text":"<p>A curated set of tools for vectara-agentic</p> Source code in <code>vectara_agentic/tools_catalog.py</code> <pre><code>class ToolsCatalog:\n    \"\"\"\n    A curated set of tools for vectara-agentic\n    \"\"\"\n    def __init__(self, agent_config: AgentConfig):\n        self.agent_config = agent_config\n\n    @remove_self_from_signature\n    def summarize_text(\n        self,\n        text: str = Field(description=\"the original text.\"),\n        expertise: str = Field(\n            description=\"the expertise to apply to the summarization.\",\n        ),\n    ) -&gt; str:\n        \"\"\"\n        This is a helper tool.\n        Use this tool to summarize text using a given expertise\n        with no more than summary_max_length characters.\n\n        Args:\n            text (str): The original text.\n            expertise (str): The expertise to apply to the summarization.\n\n        Returns:\n            str: The summarized text.\n        \"\"\"\n        if not isinstance(expertise, str):\n            return \"Please provide a valid string for expertise.\"\n        if not isinstance(text, str):\n            return \"Please provide a valid string for text.\"\n        expertise = \"general\" if len(expertise) &lt; 3 else expertise.lower()\n        prompt = (\n            f\"As an expert in {expertise}, summarize the provided text \"\n            \"into a concise summary.\\n\"\n            f\"Original text: {text}\\nSummary:\"\n        )\n        llm = get_llm(LLMRole.TOOL, config=self.agent_config)\n        response = llm.complete(prompt)\n        return response.text\n\n    @remove_self_from_signature\n    def rephrase_text(\n        self,\n        text: str = Field(description=\"the original text.\"),\n        instructions: str = Field(description=\"the specific instructions for how to rephrase the text.\"),\n    ) -&gt; str:\n        \"\"\"\n        This is a helper tool.\n        Use this tool to rephrase the text according to the provided instructions.\n        For example, instructions could be \"as a 5 year old would say it.\"\n\n        Args:\n            text (str): The original text.\n            instructions (str): The specific instructions for how to rephrase the text.\n\n        Returns:\n            str: The rephrased text.\n        \"\"\"\n        prompt = (\n            f\"Rephrase the provided text according to the following instructions: {instructions}.\\n\"\n            \"If the input is Markdown, keep the output in Markdown as well.\\n\"\n            f\"Original text: {text}\\nRephrased text:\"\n        )\n        llm = get_llm(LLMRole.TOOL, config=self.agent_config)\n        response = llm.complete(prompt)\n        return response.text\n\n    @remove_self_from_signature\n    def critique_text(\n        self,\n        text: str = Field(description=\"the original text.\"),\n        role: str = Field(default=None, description=\"the role of the person providing critique.\"),\n        point_of_view: str = Field(default=None, description=\"the point of view with which to provide critique.\"),\n    ) -&gt; str:\n        \"\"\"\n        This is a helper tool.\n        Critique the text from the specified point of view.\n\n        Args:\n            text (str): The original text.\n            role (str): The role of the person providing critique.\n            point_of_view (str): The point of view with which to provide critique.\n\n        Returns:\n            str: The critique of the text.\n        \"\"\"\n        if role:\n            prompt = f\"As a {role}, critique the provided text from the point of view of {point_of_view}.\"\n        else:\n            prompt = f\"Critique the provided text from the point of view of {point_of_view}.\"\n        prompt += \"\\nStructure the critique as bullet points.\\n\"\n        prompt += f\"Original text: {text}\\nCritique:\"\n        llm = get_llm(LLMRole.TOOL, config=self.agent_config)\n        response = llm.complete(prompt)\n        return response.text\n</code></pre>"},{"location":"api/#vectara_agentic.ToolsCatalog.critique_text","title":"<code>critique_text(text=Field(description='the original text.'), role=Field(default=None, description='the role of the person providing critique.'), point_of_view=Field(default=None, description='the point of view with which to provide critique.'))</code>","text":"<p>This is a helper tool. Critique the text from the specified point of view.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The original text.</p> <code>Field(description='the original text.')</code> <code>role</code> <code>str</code> <p>The role of the person providing critique.</p> <code>Field(default=None, description='the role of the person providing critique.')</code> <code>point_of_view</code> <code>str</code> <p>The point of view with which to provide critique.</p> <code>Field(default=None, description='the point of view with which to provide critique.')</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The critique of the text.</p> Source code in <code>vectara_agentic/tools_catalog.py</code> <pre><code>@remove_self_from_signature\ndef critique_text(\n    self,\n    text: str = Field(description=\"the original text.\"),\n    role: str = Field(default=None, description=\"the role of the person providing critique.\"),\n    point_of_view: str = Field(default=None, description=\"the point of view with which to provide critique.\"),\n) -&gt; str:\n    \"\"\"\n    This is a helper tool.\n    Critique the text from the specified point of view.\n\n    Args:\n        text (str): The original text.\n        role (str): The role of the person providing critique.\n        point_of_view (str): The point of view with which to provide critique.\n\n    Returns:\n        str: The critique of the text.\n    \"\"\"\n    if role:\n        prompt = f\"As a {role}, critique the provided text from the point of view of {point_of_view}.\"\n    else:\n        prompt = f\"Critique the provided text from the point of view of {point_of_view}.\"\n    prompt += \"\\nStructure the critique as bullet points.\\n\"\n    prompt += f\"Original text: {text}\\nCritique:\"\n    llm = get_llm(LLMRole.TOOL, config=self.agent_config)\n    response = llm.complete(prompt)\n    return response.text\n</code></pre>"},{"location":"api/#vectara_agentic.ToolsCatalog.rephrase_text","title":"<code>rephrase_text(text=Field(description='the original text.'), instructions=Field(description='the specific instructions for how to rephrase the text.'))</code>","text":"<p>This is a helper tool. Use this tool to rephrase the text according to the provided instructions. For example, instructions could be \"as a 5 year old would say it.\"</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The original text.</p> <code>Field(description='the original text.')</code> <code>instructions</code> <code>str</code> <p>The specific instructions for how to rephrase the text.</p> <code>Field(description='the specific instructions for how to rephrase the text.')</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The rephrased text.</p> Source code in <code>vectara_agentic/tools_catalog.py</code> <pre><code>@remove_self_from_signature\ndef rephrase_text(\n    self,\n    text: str = Field(description=\"the original text.\"),\n    instructions: str = Field(description=\"the specific instructions for how to rephrase the text.\"),\n) -&gt; str:\n    \"\"\"\n    This is a helper tool.\n    Use this tool to rephrase the text according to the provided instructions.\n    For example, instructions could be \"as a 5 year old would say it.\"\n\n    Args:\n        text (str): The original text.\n        instructions (str): The specific instructions for how to rephrase the text.\n\n    Returns:\n        str: The rephrased text.\n    \"\"\"\n    prompt = (\n        f\"Rephrase the provided text according to the following instructions: {instructions}.\\n\"\n        \"If the input is Markdown, keep the output in Markdown as well.\\n\"\n        f\"Original text: {text}\\nRephrased text:\"\n    )\n    llm = get_llm(LLMRole.TOOL, config=self.agent_config)\n    response = llm.complete(prompt)\n    return response.text\n</code></pre>"},{"location":"api/#vectara_agentic.ToolsCatalog.summarize_text","title":"<code>summarize_text(text=Field(description='the original text.'), expertise=Field(description='the expertise to apply to the summarization.'))</code>","text":"<p>This is a helper tool. Use this tool to summarize text using a given expertise with no more than summary_max_length characters.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The original text.</p> <code>Field(description='the original text.')</code> <code>expertise</code> <code>str</code> <p>The expertise to apply to the summarization.</p> <code>Field(description='the expertise to apply to the summarization.')</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The summarized text.</p> Source code in <code>vectara_agentic/tools_catalog.py</code> <pre><code>@remove_self_from_signature\ndef summarize_text(\n    self,\n    text: str = Field(description=\"the original text.\"),\n    expertise: str = Field(\n        description=\"the expertise to apply to the summarization.\",\n    ),\n) -&gt; str:\n    \"\"\"\n    This is a helper tool.\n    Use this tool to summarize text using a given expertise\n    with no more than summary_max_length characters.\n\n    Args:\n        text (str): The original text.\n        expertise (str): The expertise to apply to the summarization.\n\n    Returns:\n        str: The summarized text.\n    \"\"\"\n    if not isinstance(expertise, str):\n        return \"Please provide a valid string for expertise.\"\n    if not isinstance(text, str):\n        return \"Please provide a valid string for text.\"\n    expertise = \"general\" if len(expertise) &lt; 3 else expertise.lower()\n    prompt = (\n        f\"As an expert in {expertise}, summarize the provided text \"\n        \"into a concise summary.\\n\"\n        f\"Original text: {text}\\nSummary:\"\n    )\n    llm = get_llm(LLMRole.TOOL, config=self.agent_config)\n    response = llm.complete(prompt)\n    return response.text\n</code></pre>"},{"location":"api/#vectara_agentic.ToolsFactory","title":"<code>ToolsFactory</code>","text":"<p>A factory class for creating agent tools.</p> Source code in <code>vectara_agentic/tools.py</code> <pre><code>class ToolsFactory:\n    \"\"\"\n    A factory class for creating agent tools.\n    \"\"\"\n\n    def __init__(self, agent_config: AgentConfig = None) -&gt; None:\n        self.agent_config = agent_config\n\n    def create_tool(\n        self, function: Callable, tool_type: ToolType = ToolType.QUERY\n    ) -&gt; VectaraTool:\n        \"\"\"\n        Create a tool from a function.\n\n        Args:\n            function (Callable): a function to convert into a tool.\n            tool_type (ToolType): the type of tool.\n\n        Returns:\n            VectaraTool: A VectaraTool object.\n        \"\"\"\n        return VectaraTool.from_defaults(tool_type=tool_type, fn=function)\n\n    def get_llama_index_tools(\n        self,\n        tool_package_name: str,\n        tool_spec_name: str,\n        tool_name_prefix: str = \"\",\n        **kwargs: dict,\n    ) -&gt; List[VectaraTool]:\n        \"\"\"\n        Get a tool from the llama_index hub.\n\n        Args:\n            tool_package_name (str): The name of the tool package.\n            tool_spec_name (str): The name of the tool spec.\n            tool_name_prefix (str, optional): The prefix to add to the tool names (added to every tool in the spec).\n            kwargs (dict): The keyword arguments to pass to the tool constructor (see Hub for tool specific details).\n\n        Returns:\n            List[VectaraTool]: A list of VectaraTool objects.\n        \"\"\"\n        # Dynamically install and import the module\n        if tool_package_name not in LI_packages:\n            raise ValueError(\n                f\"Tool package {tool_package_name} from LlamaIndex not supported by Vectara-agentic.\"\n            )\n\n        module_name = f\"llama_index.tools.{tool_package_name}\"\n        module = importlib.import_module(module_name)\n\n        # Get the tool spec class or function from the module\n        tool_spec = getattr(module, tool_spec_name)\n        func_type = LI_packages[tool_package_name]\n        tools = tool_spec(**kwargs).to_tool_list()\n        vtools = []\n        for tool in tools:\n            if len(tool_name_prefix) &gt; 0:\n                tool.metadata.name = tool_name_prefix + \"_\" + tool.metadata.name\n            if isinstance(func_type, dict):\n                if tool_spec_name not in func_type.keys():\n                    raise ValueError(\n                        f\"Tool spec {tool_spec_name} not found in package {tool_package_name}.\"\n                    )\n                tool_type = func_type[tool_spec_name]\n            else:\n                tool_type = func_type\n            vtool = VectaraTool(\n                tool_type=tool_type,\n                fn=tool.fn,\n                metadata=tool.metadata,\n                async_fn=tool.async_fn,\n            )\n            vtools.append(vtool)\n        return vtools\n\n    def standard_tools(self) -&gt; List[FunctionTool]:\n        \"\"\"\n        Create a list of standard tools.\n        \"\"\"\n        tc = ToolsCatalog(self.agent_config)\n        return [\n            self.create_tool(tool)\n            for tool in [tc.summarize_text, tc.rephrase_text, tc.critique_text]\n        ]\n\n    def guardrail_tools(self) -&gt; List[FunctionTool]:\n        \"\"\"\n        Create a list of guardrail tools to avoid controversial topics.\n        \"\"\"\n        return [self.create_tool(get_bad_topics)]\n\n    def financial_tools(self):\n        \"\"\"\n        Create a list of financial tools.\n        \"\"\"\n        return self.get_llama_index_tools(\n            tool_package_name=\"yahoo_finance\", tool_spec_name=\"YahooFinanceToolSpec\"\n        )\n\n    def legal_tools(self) -&gt; List[FunctionTool]:\n        \"\"\"\n        Create a list of legal tools.\n        \"\"\"\n\n        def summarize_legal_text(\n            text: str = Field(description=\"the original text.\"),\n        ) -&gt; str:\n            \"\"\"\n            Use this tool to summarize legal text with no more than summary_max_length characters.\n            \"\"\"\n            tc = ToolsCatalog(self.agent_config)\n            return tc.summarize_text(text, expertise=\"law\")\n\n        def critique_as_judge(\n            text: str = Field(description=\"the original text.\"),\n        ) -&gt; str:\n            \"\"\"\n            Critique the legal document.\n            \"\"\"\n            tc = ToolsCatalog(self.agent_config)\n            return tc.critique_text(\n                text,\n                role=\"judge\",\n                point_of_view=\"\"\"\n                an experienced judge evaluating a legal document to provide areas of concern\n                or that may require further legal scrutiny or legal argument.\n                \"\"\",\n            )\n\n        return [\n            self.create_tool(tool) for tool in [summarize_legal_text, critique_as_judge]\n        ]\n\n    def database_tools(\n        self,\n        tool_name_prefix: str = \"\",\n        content_description: Optional[str] = None,\n        sql_database: Optional[SQLDatabase] = None,\n        scheme: Optional[str] = None,\n        host: str = \"localhost\",\n        port: str = \"5432\",\n        user: str = \"postgres\",\n        password: str = \"Password\",\n        dbname: str = \"postgres\",\n        max_rows: int = 1000,\n    ) -&gt; List[VectaraTool]:\n        \"\"\"\n        Returns a list of database tools.\n\n        Args:\n\n            tool_name_prefix (str, optional): The prefix to add to the tool names. Defaults to \"\".\n            content_description (str, optional): The content description for the database. Defaults to None.\n            sql_database (SQLDatabase, optional): The SQLDatabase object. Defaults to None.\n            scheme (str, optional): The database scheme. Defaults to None.\n            host (str, optional): The database host. Defaults to \"localhost\".\n            port (str, optional): The database port. Defaults to \"5432\".\n            user (str, optional): The database user. Defaults to \"postgres\".\n            password (str, optional): The database password. Defaults to \"Password\".\n            dbname (str, optional): The database name. Defaults to \"postgres\".\n               You must specify either the sql_database object or the scheme, host, port, user, password, and dbname.\n            max_rows (int, optional): if specified, instructs the load_data tool to never return more than max_rows\n               rows. Defaults to 1000.\n\n        Returns:\n            List[VectaraTool]: A list of VectaraTool objects.\n        \"\"\"\n        if sql_database:\n            dbt = DatabaseTools(\n                tool_name_prefix=tool_name_prefix,\n                sql_database=sql_database,\n                max_rows=max_rows,\n            )\n        else:\n            if scheme in [\"postgresql\", \"mysql\", \"sqlite\", \"mssql\", \"oracle\"]:\n                dbt = DatabaseTools(\n                    tool_name_prefix=tool_name_prefix,\n                    scheme=scheme,\n                    host=host,\n                    port=port,\n                    user=user,\n                    password=password,\n                    dbname=dbname,\n                    max_rows=max_rows,\n                )\n            else:\n                raise ValueError(\n                    \"Please provide a SqlDatabase option or a valid DB scheme type \"\n                    \" (postgresql, mysql, sqlite, mssql, oracle).\"\n                )\n\n        # Update tools with description\n        tools = dbt.to_tool_list()\n        vtools = []\n        for tool in tools:\n            if content_description:\n                tool.metadata.description = (\n                    tool.metadata.description\n                    + f\"The database tables include data about {content_description}.\"\n                )\n            vtool = VectaraTool(\n                tool_type=ToolType.QUERY,\n                fn=tool.fn,\n                async_fn=tool.async_fn,\n                metadata=tool.metadata,\n            )\n            vtools.append(vtool)\n        return vtools\n</code></pre>"},{"location":"api/#vectara_agentic.ToolsFactory.create_tool","title":"<code>create_tool(function, tool_type=ToolType.QUERY)</code>","text":"<p>Create a tool from a function.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>a function to convert into a tool.</p> required <code>tool_type</code> <code>ToolType</code> <p>the type of tool.</p> <code>QUERY</code> <p>Returns:</p> Name Type Description <code>VectaraTool</code> <code>VectaraTool</code> <p>A VectaraTool object.</p> Source code in <code>vectara_agentic/tools.py</code> <pre><code>def create_tool(\n    self, function: Callable, tool_type: ToolType = ToolType.QUERY\n) -&gt; VectaraTool:\n    \"\"\"\n    Create a tool from a function.\n\n    Args:\n        function (Callable): a function to convert into a tool.\n        tool_type (ToolType): the type of tool.\n\n    Returns:\n        VectaraTool: A VectaraTool object.\n    \"\"\"\n    return VectaraTool.from_defaults(tool_type=tool_type, fn=function)\n</code></pre>"},{"location":"api/#vectara_agentic.ToolsFactory.database_tools","title":"<code>database_tools(tool_name_prefix='', content_description=None, sql_database=None, scheme=None, host='localhost', port='5432', user='postgres', password='Password', dbname='postgres', max_rows=1000)</code>","text":"<p>Returns a list of database tools.</p> <p>Args:</p> <pre><code>tool_name_prefix (str, optional): The prefix to add to the tool names. Defaults to \"\".\ncontent_description (str, optional): The content description for the database. Defaults to None.\nsql_database (SQLDatabase, optional): The SQLDatabase object. Defaults to None.\nscheme (str, optional): The database scheme. Defaults to None.\nhost (str, optional): The database host. Defaults to \"localhost\".\nport (str, optional): The database port. Defaults to \"5432\".\nuser (str, optional): The database user. Defaults to \"postgres\".\npassword (str, optional): The database password. Defaults to \"Password\".\ndbname (str, optional): The database name. Defaults to \"postgres\".\n   You must specify either the sql_database object or the scheme, host, port, user, password, and dbname.\nmax_rows (int, optional): if specified, instructs the load_data tool to never return more than max_rows\n   rows. Defaults to 1000.\n</code></pre> <p>Returns:</p> Type Description <code>List[VectaraTool]</code> <p>List[VectaraTool]: A list of VectaraTool objects.</p> Source code in <code>vectara_agentic/tools.py</code> <pre><code>def database_tools(\n    self,\n    tool_name_prefix: str = \"\",\n    content_description: Optional[str] = None,\n    sql_database: Optional[SQLDatabase] = None,\n    scheme: Optional[str] = None,\n    host: str = \"localhost\",\n    port: str = \"5432\",\n    user: str = \"postgres\",\n    password: str = \"Password\",\n    dbname: str = \"postgres\",\n    max_rows: int = 1000,\n) -&gt; List[VectaraTool]:\n    \"\"\"\n    Returns a list of database tools.\n\n    Args:\n\n        tool_name_prefix (str, optional): The prefix to add to the tool names. Defaults to \"\".\n        content_description (str, optional): The content description for the database. Defaults to None.\n        sql_database (SQLDatabase, optional): The SQLDatabase object. Defaults to None.\n        scheme (str, optional): The database scheme. Defaults to None.\n        host (str, optional): The database host. Defaults to \"localhost\".\n        port (str, optional): The database port. Defaults to \"5432\".\n        user (str, optional): The database user. Defaults to \"postgres\".\n        password (str, optional): The database password. Defaults to \"Password\".\n        dbname (str, optional): The database name. Defaults to \"postgres\".\n           You must specify either the sql_database object or the scheme, host, port, user, password, and dbname.\n        max_rows (int, optional): if specified, instructs the load_data tool to never return more than max_rows\n           rows. Defaults to 1000.\n\n    Returns:\n        List[VectaraTool]: A list of VectaraTool objects.\n    \"\"\"\n    if sql_database:\n        dbt = DatabaseTools(\n            tool_name_prefix=tool_name_prefix,\n            sql_database=sql_database,\n            max_rows=max_rows,\n        )\n    else:\n        if scheme in [\"postgresql\", \"mysql\", \"sqlite\", \"mssql\", \"oracle\"]:\n            dbt = DatabaseTools(\n                tool_name_prefix=tool_name_prefix,\n                scheme=scheme,\n                host=host,\n                port=port,\n                user=user,\n                password=password,\n                dbname=dbname,\n                max_rows=max_rows,\n            )\n        else:\n            raise ValueError(\n                \"Please provide a SqlDatabase option or a valid DB scheme type \"\n                \" (postgresql, mysql, sqlite, mssql, oracle).\"\n            )\n\n    # Update tools with description\n    tools = dbt.to_tool_list()\n    vtools = []\n    for tool in tools:\n        if content_description:\n            tool.metadata.description = (\n                tool.metadata.description\n                + f\"The database tables include data about {content_description}.\"\n            )\n        vtool = VectaraTool(\n            tool_type=ToolType.QUERY,\n            fn=tool.fn,\n            async_fn=tool.async_fn,\n            metadata=tool.metadata,\n        )\n        vtools.append(vtool)\n    return vtools\n</code></pre>"},{"location":"api/#vectara_agentic.ToolsFactory.financial_tools","title":"<code>financial_tools()</code>","text":"<p>Create a list of financial tools.</p> Source code in <code>vectara_agentic/tools.py</code> <pre><code>def financial_tools(self):\n    \"\"\"\n    Create a list of financial tools.\n    \"\"\"\n    return self.get_llama_index_tools(\n        tool_package_name=\"yahoo_finance\", tool_spec_name=\"YahooFinanceToolSpec\"\n    )\n</code></pre>"},{"location":"api/#vectara_agentic.ToolsFactory.get_llama_index_tools","title":"<code>get_llama_index_tools(tool_package_name, tool_spec_name, tool_name_prefix='', **kwargs)</code>","text":"<p>Get a tool from the llama_index hub.</p> <p>Parameters:</p> Name Type Description Default <code>tool_package_name</code> <code>str</code> <p>The name of the tool package.</p> required <code>tool_spec_name</code> <code>str</code> <p>The name of the tool spec.</p> required <code>tool_name_prefix</code> <code>str</code> <p>The prefix to add to the tool names (added to every tool in the spec).</p> <code>''</code> <code>kwargs</code> <code>dict</code> <p>The keyword arguments to pass to the tool constructor (see Hub for tool specific details).</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[VectaraTool]</code> <p>List[VectaraTool]: A list of VectaraTool objects.</p> Source code in <code>vectara_agentic/tools.py</code> <pre><code>def get_llama_index_tools(\n    self,\n    tool_package_name: str,\n    tool_spec_name: str,\n    tool_name_prefix: str = \"\",\n    **kwargs: dict,\n) -&gt; List[VectaraTool]:\n    \"\"\"\n    Get a tool from the llama_index hub.\n\n    Args:\n        tool_package_name (str): The name of the tool package.\n        tool_spec_name (str): The name of the tool spec.\n        tool_name_prefix (str, optional): The prefix to add to the tool names (added to every tool in the spec).\n        kwargs (dict): The keyword arguments to pass to the tool constructor (see Hub for tool specific details).\n\n    Returns:\n        List[VectaraTool]: A list of VectaraTool objects.\n    \"\"\"\n    # Dynamically install and import the module\n    if tool_package_name not in LI_packages:\n        raise ValueError(\n            f\"Tool package {tool_package_name} from LlamaIndex not supported by Vectara-agentic.\"\n        )\n\n    module_name = f\"llama_index.tools.{tool_package_name}\"\n    module = importlib.import_module(module_name)\n\n    # Get the tool spec class or function from the module\n    tool_spec = getattr(module, tool_spec_name)\n    func_type = LI_packages[tool_package_name]\n    tools = tool_spec(**kwargs).to_tool_list()\n    vtools = []\n    for tool in tools:\n        if len(tool_name_prefix) &gt; 0:\n            tool.metadata.name = tool_name_prefix + \"_\" + tool.metadata.name\n        if isinstance(func_type, dict):\n            if tool_spec_name not in func_type.keys():\n                raise ValueError(\n                    f\"Tool spec {tool_spec_name} not found in package {tool_package_name}.\"\n                )\n            tool_type = func_type[tool_spec_name]\n        else:\n            tool_type = func_type\n        vtool = VectaraTool(\n            tool_type=tool_type,\n            fn=tool.fn,\n            metadata=tool.metadata,\n            async_fn=tool.async_fn,\n        )\n        vtools.append(vtool)\n    return vtools\n</code></pre>"},{"location":"api/#vectara_agentic.ToolsFactory.guardrail_tools","title":"<code>guardrail_tools()</code>","text":"<p>Create a list of guardrail tools to avoid controversial topics.</p> Source code in <code>vectara_agentic/tools.py</code> <pre><code>def guardrail_tools(self) -&gt; List[FunctionTool]:\n    \"\"\"\n    Create a list of guardrail tools to avoid controversial topics.\n    \"\"\"\n    return [self.create_tool(get_bad_topics)]\n</code></pre>"},{"location":"api/#vectara_agentic.ToolsFactory.legal_tools","title":"<code>legal_tools()</code>","text":"<p>Create a list of legal tools.</p> Source code in <code>vectara_agentic/tools.py</code> <pre><code>def legal_tools(self) -&gt; List[FunctionTool]:\n    \"\"\"\n    Create a list of legal tools.\n    \"\"\"\n\n    def summarize_legal_text(\n        text: str = Field(description=\"the original text.\"),\n    ) -&gt; str:\n        \"\"\"\n        Use this tool to summarize legal text with no more than summary_max_length characters.\n        \"\"\"\n        tc = ToolsCatalog(self.agent_config)\n        return tc.summarize_text(text, expertise=\"law\")\n\n    def critique_as_judge(\n        text: str = Field(description=\"the original text.\"),\n    ) -&gt; str:\n        \"\"\"\n        Critique the legal document.\n        \"\"\"\n        tc = ToolsCatalog(self.agent_config)\n        return tc.critique_text(\n            text,\n            role=\"judge\",\n            point_of_view=\"\"\"\n            an experienced judge evaluating a legal document to provide areas of concern\n            or that may require further legal scrutiny or legal argument.\n            \"\"\",\n        )\n\n    return [\n        self.create_tool(tool) for tool in [summarize_legal_text, critique_as_judge]\n    ]\n</code></pre>"},{"location":"api/#vectara_agentic.ToolsFactory.standard_tools","title":"<code>standard_tools()</code>","text":"<p>Create a list of standard tools.</p> Source code in <code>vectara_agentic/tools.py</code> <pre><code>def standard_tools(self) -&gt; List[FunctionTool]:\n    \"\"\"\n    Create a list of standard tools.\n    \"\"\"\n    tc = ToolsCatalog(self.agent_config)\n    return [\n        self.create_tool(tool)\n        for tool in [tc.summarize_text, tc.rephrase_text, tc.critique_text]\n    ]\n</code></pre>"},{"location":"api/#vectara_agentic.VectaraTool","title":"<code>VectaraTool</code>","text":"<p>               Bases: <code>FunctionTool</code></p> <p>A subclass of FunctionTool adding the tool_type attribute.</p> Source code in <code>vectara_agentic/tool_utils.py</code> <pre><code>class VectaraTool(FunctionTool):\n    \"\"\"\n    A subclass of FunctionTool adding the tool_type attribute.\n    \"\"\"\n\n    def __init__(\n        self,\n        tool_type: ToolType,\n        metadata: ToolMetadata,\n        fn: Optional[Callable[..., Any]] = None,\n        async_fn: Optional[AsyncCallable] = None,\n    ) -&gt; None:\n        metadata_dict = (\n            metadata.dict() if hasattr(metadata, \"dict\") else metadata.__dict__\n        )\n        vm = VectaraToolMetadata(tool_type=tool_type, **metadata_dict)\n        super().__init__(fn, vm, async_fn)\n\n    @classmethod\n    def from_defaults(\n        cls,\n        fn: Optional[Callable[..., Any]] = None,\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n        return_direct: bool = False,\n        fn_schema: Optional[Type[BaseModel]] = None,\n        async_fn: Optional[AsyncCallable] = None,\n        tool_metadata: Optional[ToolMetadata] = None,\n        callback: Optional[Callable[[Any], Any]] = None,\n        async_callback: Optional[AsyncCallable] = None,\n        partial_params: Optional[Dict[str, Any]] = None,\n        tool_type: ToolType = ToolType.QUERY,\n    ) -&gt; \"VectaraTool\":\n        tool = FunctionTool.from_defaults(\n            fn,\n            name,\n            description,\n            return_direct,\n            fn_schema,\n            async_fn,\n            tool_metadata,\n            callback,\n            async_callback,\n            partial_params\n        )\n        vectara_tool = cls(\n            tool_type=tool_type,\n            fn=tool.fn,\n            metadata=tool.metadata,\n            async_fn=tool.async_fn,\n        )\n        return vectara_tool\n\n    def __str__(self) -&gt; str:\n        return f\"Tool(name={self.metadata.name}, \" f\"Tool metadata={self.metadata})\"\n\n    def __repr__(self) -&gt; str:\n        return str(self)\n\n    def __eq__(self, other):\n        try:\n            # Try to get schema as dict if possible\n            self_schema = self.metadata.fn_schema.model_json_schema()\n            other_schema = other.metadata.fn_schema.model_json_schema()\n        except Exception:\n            return False\n\n        is_equal = (\n            isinstance(other, VectaraTool)\n            and self.metadata.tool_type == other.metadata.tool_type\n            and self.metadata.name == other.metadata.name\n            and self_schema == other_schema\n        )\n        return is_equal\n\n    def call(\n        self, *args: Any, ctx: Optional[Context] = None, **kwargs: Any\n    ) -&gt; ToolOutput:\n        try:\n            return super().call(*args, ctx=ctx, **kwargs)\n        except TypeError as e:\n            sig = inspect.signature(self.metadata.fn_schema)\n            valid_parameters = list(sig.parameters.keys())\n            params_str = \", \".join(valid_parameters)\n\n            err_output = ToolOutput(\n                tool_name=self.metadata.name,\n                content=(\n                    f\"Wrong argument used when calling {self.metadata.name}: {str(e)}.\"\n                    f\"Valid arguments: {params_str}. please call the tool again with the correct arguments.\"\n                ),\n                raw_input={\"args\": args, \"kwargs\": kwargs},\n                raw_output={\"response\": str(e)},\n            )\n            return err_output\n        except Exception as e:\n            err_output = ToolOutput(\n                tool_name=self.metadata.name,\n                content=f\"Tool {self.metadata.name} Malfunction: {str(e)}\",\n                raw_input={\"args\": args, \"kwargs\": kwargs},\n                raw_output={\"response\": str(e)},\n            )\n            return err_output\n\n    async def acall(\n        self, *args: Any, ctx: Optional[Context] = None, **kwargs: Any\n    ) -&gt; ToolOutput:\n        try:\n            return await super().acall(*args, ctx=ctx, **kwargs)\n        except TypeError as e:\n            sig = inspect.signature(self.metadata.fn_schema)\n            valid_parameters = list(sig.parameters.keys())\n            params_str = \", \".join(valid_parameters)\n\n            err_output = ToolOutput(\n                tool_name=self.metadata.name,\n                content=(\n                    f\"Wrong argument used when calling {self.metadata.name}: {str(e)}. \"\n                    f\"Valid arguments: {params_str}. please call the tool again with the correct arguments.\"\n                ),\n                raw_input={\"args\": args, \"kwargs\": kwargs},\n                raw_output={\"response\": str(e)},\n            )\n            return err_output\n        except Exception as e:\n            import traceback\n            err_output = ToolOutput(\n                tool_name=self.metadata.name,\n                content=f\"Tool {self.metadata.name} Malfunction: {str(e)}, traceback: {traceback.format_exc()}\",\n                raw_input={\"args\": args, \"kwargs\": kwargs},\n                raw_output={\"response\": str(e)},\n            )\n            return err_output\n</code></pre>"},{"location":"api/#vectara_agentic.VectaraToolFactory","title":"<code>VectaraToolFactory</code>","text":"<p>A factory class for creating Vectara RAG tools.</p> Source code in <code>vectara_agentic/tools.py</code> <pre><code>class VectaraToolFactory:\n    \"\"\"\n    A factory class for creating Vectara RAG tools.\n    \"\"\"\n\n    def __init__(\n        self,\n        vectara_corpus_key: str = str(os.environ.get(\"VECTARA_CORPUS_KEY\", \"\")),\n        vectara_api_key: str = str(os.environ.get(\"VECTARA_API_KEY\", \"\")),\n        compact_docstring: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the VectaraToolFactory\n        Args:\n            vectara_corpus_key (str): The Vectara corpus key (or comma separated list of keys).\n            vectara_api_key (str): The Vectara API key.\n            compact_docstring (bool): Whether to use a compact docstring format for tools\n              This is useful if OpenAI complains on the 1024 token limit.\n        \"\"\"\n        self.vectara_corpus_key = vectara_corpus_key\n        self.vectara_api_key = vectara_api_key\n        self.num_corpora = len(vectara_corpus_key.split(\",\"))\n        self.compact_docstring = compact_docstring\n\n    def create_search_tool(\n        self,\n        tool_name: str,\n        tool_description: str,\n        tool_args_schema: type[BaseModel] = None,\n        tool_args_type: Dict[str, str] = {},\n        summarize_docs: Optional[bool] = None,\n        summarize_llm_name: Optional[str] = None,\n        fixed_filter: str = \"\",\n        lambda_val: Union[List[float], float] = 0.005,\n        semantics: Union[List[str] | str] = \"default\",\n        custom_dimensions: Union[List[Dict], Dict] = {},\n        offset: int = 0,\n        n_sentences_before: int = 2,\n        n_sentences_after: int = 2,\n        reranker: str = \"slingshot\",\n        rerank_k: int = 50,\n        rerank_limit: Optional[int] = None,\n        rerank_cutoff: Optional[float] = None,\n        mmr_diversity_bias: float = 0.2,\n        udf_expression: str = None,\n        rerank_chain: List[Dict] = None,\n        return_direct: bool = False,\n        save_history: bool = True,\n        verbose: bool = False,\n        vectara_base_url: str = \"https://api.vectara.io\",\n        vectara_verify_ssl: bool = True,\n    ) -&gt; VectaraTool:\n        \"\"\"\n        Creates a Vectara search/retrieval tool\n\n        Args:\n            tool_name (str): The name of the tool.\n            tool_description (str): The description of the tool.\n            tool_args_schema (BaseModel, optional): The schema for the tool arguments.\n            tool_args_type (Dict[str, dict], optional): attributes for each argument where they key is the field name\n                and the value is a dictionary with the following keys:\n                - 'type': the type of each filter attribute in Vectara (doc or part).\n                - 'is_list': whether the filterable attribute is a list.\n                - 'filter_name': the name of the filterable attribute in Vectara.\n            fixed_filter (str, optional): A fixed Vectara filter condition to apply to all queries.\n            lambda_val (Union[List[float] | float], optional): Lambda value (or list of values for each corpora)\n                for the Vectara query, when using hybrid search.\n            semantics (Union[List[str], str], optional): Indicates whether the query is intended as a query or response.\n                Include list if using multiple corpora specifying the query type for each corpus.\n            custom_dimensions (Union[List[Dict] | Dict], optional): Custom dimensions for the query (for each corpora).\n            offset (int, optional): Number of results to skip.\n            n_sentences_before (int, optional): Number of sentences before the matching document part.\n            n_sentences_after (int, optional): Number of sentences after the matching document part.\n            reranker (str, optional): The reranker mode.\n            rerank_k (int, optional): Number of top-k documents for reranking.\n            rerank_limit (int, optional): Maximum number of results to return after reranking.\n            rerank_cutoff (float, optional): Minimum score threshold for results to include after reranking.\n            mmr_diversity_bias (float, optional): MMR diversity bias.\n            udf_expression (str, optional): the user defined expression for reranking results.\n            rerank_chain (List[Dict], optional): A list of rerankers to be applied sequentially.\n                Each dictionary should specify the \"type\" of reranker (mmr, slingshot, udf)\n                and any other parameters (e.g. \"limit\" or \"cutoff\" for any type,\n                \"diversity_bias\" for mmr, and \"user_function\" for udf).\n                If using slingshot/multilingual_reranker_v1, it must be first in the list.\n            save_history (bool, optional): Whether to save the query in history.\n            return_direct (bool, optional): Whether the agent should return the tool's response directly.\n            verbose (bool, optional): Whether to print verbose output.\n            vectara_base_url (str, optional): The base URL for the Vectara API.\n            vectara_verify_ssl (bool, optional): Whether to verify SSL certificates for the Vectara API.\n\n        Returns:\n            VectaraTool: A VectaraTool object.\n        \"\"\"\n\n        vectara = VectaraIndex(\n            vectara_api_key=self.vectara_api_key,\n            vectara_corpus_key=self.vectara_corpus_key,\n            x_source_str=\"vectara-agentic\",\n            vectara_base_url=vectara_base_url,\n            vectara_verify_ssl=vectara_verify_ssl,\n        )\n\n        # Dynamically generate the search function\n        def search_function(*args: Any, **kwargs: Any) -&gt; ToolOutput:\n            \"\"\"\n            Dynamically generated function for semantic search Vectara.\n            \"\"\"\n            # Convert args to kwargs using the function signature\n            sig = inspect.signature(search_function)\n            bound_args = sig.bind_partial(*args, **kwargs)\n            bound_args.apply_defaults()\n            kwargs = bound_args.arguments\n\n            query = kwargs.pop(\"query\")\n            top_k = kwargs.pop(\"top_k\", 10)\n            summarize = (\n                kwargs.pop(\"summarize\", True)\n                if summarize_docs is None\n                else summarize_docs\n            )\n            try:\n                filter_string = build_filter_string(\n                    kwargs, tool_args_type, fixed_filter\n                )\n            except ValueError as e:\n                return ToolOutput(\n                    tool_name=search_function.__name__,\n                    content=str(e),\n                    raw_input={\"args\": args, \"kwargs\": kwargs},\n                    raw_output={\"response\": str(e)},\n                )\n\n            vectara_retriever = vectara.as_retriever(\n                summary_enabled=False,\n                similarity_top_k=top_k,\n                reranker=reranker,\n                rerank_k=(\n                    rerank_k\n                    if rerank_k * self.num_corpora &lt;= 100\n                    else int(100 / self.num_corpora)\n                ),\n                rerank_limit=rerank_limit,\n                rerank_cutoff=rerank_cutoff,\n                mmr_diversity_bias=mmr_diversity_bias,\n                udf_expression=udf_expression,\n                rerank_chain=rerank_chain,\n                lambda_val=lambda_val,\n                semantics=semantics,\n                custom_dimensions=custom_dimensions,\n                offset=offset,\n                filter=filter_string,\n                n_sentences_before=n_sentences_before,\n                n_sentences_after=n_sentences_after,\n                save_history=save_history,\n                x_source_str=\"vectara-agentic\",\n                verbose=verbose,\n            )\n            response = vectara_retriever.retrieve(query)\n\n            if len(response) == 0:\n                msg = \"Vectara Tool failed to retrieve any results for the query.\"\n                return ToolOutput(\n                    tool_name=search_function.__name__,\n                    content=msg,\n                    raw_input={\"args\": args, \"kwargs\": kwargs},\n                    raw_output={\"response\": msg},\n                )\n            unique_ids = set()\n            docs = []\n            for doc in response:\n                if doc.id_ in unique_ids:\n                    continue\n                unique_ids.add(doc.id_)\n                docs.append((doc.id_, doc.metadata))\n            tool_output = \"Matching documents:\\n\"\n            if summarize:\n                summaries_dict = asyncio.run(\n                    summarize_documents(\n                        corpus_key=self.vectara_corpus_key,\n                        api_key=self.vectara_api_key,\n                        llm_name=summarize_llm_name,\n                        doc_ids=list(unique_ids),\n                    )\n                )\n                for doc_id, metadata in docs:\n                    summary = summaries_dict.get(doc_id, \"\")\n                    tool_output += f\"document_id: '{doc_id}'\\nmetadata: '{metadata}'\\nsummary: '{summary}'\\n\\n\"\n            else:\n                for doc_id, metadata in docs:\n                    tool_output += (\n                        f\"document_id: '{doc_id}'\\nmetadata: '{metadata}'\\n\\n\"\n                    )\n\n            out = ToolOutput(\n                tool_name=search_function.__name__,\n                content=tool_output,\n                raw_input={\"args\": args, \"kwargs\": kwargs},\n                raw_output=response,\n            )\n            return out\n\n        class SearchToolBaseParams(BaseModel):\n            \"\"\"Model for the base parameters of the search tool.\"\"\"\n\n            query: str = Field(\n                ...,\n                description=\"The search query to perform, in the form of a question.\",\n            )\n            top_k: int = Field(\n                default=10, description=\"The number of top documents to retrieve.\"\n            )\n            summarize: bool = Field(\n                True,\n                description=\"Whether to summarize the retrieved documents.\",\n            )\n\n        class SearchToolBaseParamsWithoutSummarize(BaseModel):\n            \"\"\"Model for the base parameters of the search tool.\"\"\"\n\n            query: str = Field(\n                ...,\n                description=\"The search query to perform, in the form of a question.\",\n            )\n            top_k: int = Field(\n                10, description=\"The number of top documents to retrieve.\"\n            )\n\n        search_tool_extra_desc = (\n            tool_description\n            + \"\\n\"\n            + \"Use this tool to search for relevant documents, not to ask questions.\"\n        )\n\n        tool = create_tool_from_dynamic_function(\n            search_function,\n            tool_name,\n            search_tool_extra_desc,\n            (\n                SearchToolBaseParams\n                if summarize_docs is None\n                else SearchToolBaseParamsWithoutSummarize\n            ),\n            tool_args_schema,\n            compact_docstring=self.compact_docstring,\n            return_direct=return_direct,\n        )\n        return tool\n\n    def create_rag_tool(\n        self,\n        tool_name: str,\n        tool_description: str,\n        tool_args_schema: type[BaseModel] = None,\n        tool_args_type: Dict[str, dict] = {},\n        fixed_filter: str = \"\",\n        vectara_summarizer: str = \"vectara-summary-ext-24-05-med-omni\",\n        vectara_prompt_text: str = None,\n        summary_num_results: int = 5,\n        summary_response_lang: str = \"eng\",\n        n_sentences_before: int = 2,\n        n_sentences_after: int = 2,\n        offset: int = 0,\n        lambda_val: Union[List[float], float] = 0.005,\n        semantics: Union[List[str] | str] = \"default\",\n        custom_dimensions: Union[List[Dict], Dict] = {},\n        reranker: str = \"slingshot\",\n        rerank_k: int = 50,\n        rerank_limit: Optional[int] = None,\n        rerank_cutoff: Optional[float] = None,\n        mmr_diversity_bias: float = 0.2,\n        udf_expression: str = None,\n        rerank_chain: List[Dict] = None,\n        max_response_chars: Optional[int] = None,\n        max_tokens: Optional[int] = None,\n        llm_name: Optional[str] = None,\n        temperature: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        include_citations: bool = True,\n        save_history: bool = False,\n        fcs_threshold: float = 0.0,\n        return_direct: bool = False,\n        verbose: bool = False,\n        vectara_base_url: str = \"https://api.vectara.io\",\n        vectara_verify_ssl: bool = True,\n    ) -&gt; VectaraTool:\n        \"\"\"\n        Creates a RAG (Retrieve and Generate) tool.\n\n        Args:\n            tool_name (str): The name of the tool.\n            tool_description (str): The description of the tool.\n            tool_args_schema (BaseModel, optional): The schema for any tool arguments for filtering.\n            tool_args_type (Dict[str, dict], optional): attributes for each argument where they key is the field name\n                and the value is a dictionary with the following keys:\n                - 'type': the type of each filter attribute in Vectara (doc or part).\n                - 'is_list': whether the filterable attribute is a list.\n                - 'filter_name': the name of the filterable attribute in Vectara.\n            fixed_filter (str, optional): A fixed Vectara filter condition to apply to all queries.\n            vectara_summarizer (str, optional): The Vectara summarizer to use.\n            vectara_prompt_text (str, optional): The prompt text for the Vectara summarizer.\n            summary_num_results (int, optional): The number of summary results.\n            summary_response_lang (str, optional): The response language for the summary.\n            n_sentences_before (int, optional): Number of sentences before the summary.\n            n_sentences_after (int, optional): Number of sentences after the summary.\n            offset (int, optional): Number of results to skip.\n            lambda_val (Union[List[float] | float], optional): Lambda value (or list of values for each corpora)\n                for the Vectara query, when using hybrid search.\n            semantics (Union[List[str], str], optional): Indicates whether the query is intended as a query or response.\n                Include list if using multiple corpora specifying the query type for each corpus.\n            custom_dimensions (Union[List[Dict] | Dict], optional): Custom dimensions for the query (for each corpora).\n            reranker (str, optional): The reranker mode.\n            rerank_k (int, optional): Number of top-k documents for reranking.\n            rerank_limit (int, optional): Maximum number of results to return after reranking.\n            rerank_cutoff (float, optional): Minimum score threshold for results to include after reranking.\n            mmr_diversity_bias (float, optional): MMR diversity bias.\n            udf_expression (str, optional): The user defined expression for reranking results.\n            rerank_chain (List[Dict], optional): A list of rerankers to be applied sequentially.\n                Each dictionary should specify the \"type\" of reranker (mmr, slingshot, udf)\n                and any other parameters (e.g. \"limit\" or \"cutoff\" for any type,\n                \"diversity_bias\" for mmr, and \"user_function\" for udf).\n                If using slingshot/multilingual_reranker_v1, it must be first in the list.\n            max_response_chars (int, optional): The desired maximum number of characters for the generated summary.\n            max_tokens (int, optional): The maximum number of tokens to be returned by the LLM.\n            llm_name (str, optional): The name of the LLM to use for generation.\n            temperature (float, optional): The sampling temperature; higher values lead to more randomness.\n            frequency_penalty (float, optional): How much to penalize repeating tokens in the response,\n                higher values reducing likelihood of repeating the same line.\n            presence_penalty (float, optional): How much to penalize repeating tokens in the response,\n                higher values increasing the diversity of topics.\n            include_citations (bool, optional): Whether to include citations in the response.\n                If True, uses markdown vectara citations that requires the Vectara scale plan.\n            save_history (bool, optional): Whether to save the query in history.\n            fcs_threshold (float, optional): A threshold for factual consistency.\n                If set above 0, the tool notifies the calling agent that it \"cannot respond\" if FCS is too low.\n            return_direct (bool, optional): Whether the agent should return the tool's response directly.\n            verbose (bool, optional): Whether to print verbose output.\n            vectara_base_url (str, optional): The base URL for the Vectara API.\n            vectara_verify_ssl (bool, optional): Whether to verify SSL certificates for the Vectara API.\n\n        Returns:\n            VectaraTool: A VectaraTool object.\n        \"\"\"\n\n        vectara = VectaraIndex(\n            vectara_api_key=self.vectara_api_key,\n            vectara_corpus_key=self.vectara_corpus_key,\n            x_source_str=\"vectara-agentic\",\n            vectara_base_url=vectara_base_url,\n            vectara_verify_ssl=vectara_verify_ssl,\n        )\n\n        # Dynamically generate the RAG function\n        def rag_function(*args: Any, **kwargs: Any) -&gt; ToolOutput:\n            \"\"\"\n            Dynamically generated function for RAG query with Vectara.\n            \"\"\"\n            # Convert args to kwargs using the function signature\n            sig = inspect.signature(rag_function)\n            bound_args = sig.bind_partial(*args, **kwargs)\n            bound_args.apply_defaults()\n            kwargs = bound_args.arguments\n\n            query = kwargs.pop(\"query\")\n            try:\n                filter_string = build_filter_string(\n                    kwargs, tool_args_type, fixed_filter\n                )\n            except ValueError as e:\n                return ToolOutput(\n                    tool_name=rag_function.__name__,\n                    content=str(e),\n                    raw_input={\"args\": args, \"kwargs\": kwargs},\n                    raw_output={\"response\": str(e)},\n                )\n\n            vectara_query_engine = vectara.as_query_engine(\n                summary_enabled=True,\n                similarity_top_k=summary_num_results,\n                summary_num_results=summary_num_results,\n                summary_response_lang=summary_response_lang,\n                summary_prompt_name=vectara_summarizer,\n                prompt_text=vectara_prompt_text,\n                reranker=reranker,\n                rerank_k=(\n                    rerank_k\n                    if rerank_k * self.num_corpora &lt;= 100\n                    else int(100 / self.num_corpora)\n                ),\n                rerank_limit=rerank_limit,\n                rerank_cutoff=rerank_cutoff,\n                mmr_diversity_bias=mmr_diversity_bias,\n                udf_expression=udf_expression,\n                rerank_chain=rerank_chain,\n                n_sentences_before=n_sentences_before,\n                n_sentences_after=n_sentences_after,\n                offset=offset,\n                lambda_val=lambda_val,\n                semantics=semantics,\n                custom_dimensions=custom_dimensions,\n                filter=filter_string,\n                max_response_chars=max_response_chars,\n                max_tokens=max_tokens,\n                llm_name=llm_name,\n                temperature=temperature,\n                frequency_penalty=frequency_penalty,\n                presence_penalty=presence_penalty,\n                citations_style=\"markdown\" if include_citations else None,\n                citations_url_pattern=\"{doc.url}\" if include_citations else None,\n                save_history=save_history,\n                x_source_str=\"vectara-agentic\",\n                verbose=verbose,\n            )\n            response = vectara_query_engine.query(query)\n\n            if len(response.source_nodes) == 0:\n                msg = (\n                    \"Tool failed to generate a response since no matches were found. \"\n                    \"Please check the arguments and try again.\"\n                )\n                return ToolOutput(\n                    tool_name=rag_function.__name__,\n                    content=msg,\n                    raw_input={\"args\": args, \"kwargs\": kwargs},\n                    raw_output={\"response\": msg},\n                )\n            if str(response) == \"None\":\n                msg = \"Tool failed to generate a response.\"\n                return ToolOutput(\n                    tool_name=rag_function.__name__,\n                    content=msg,\n                    raw_input={\"args\": args, \"kwargs\": kwargs},\n                    raw_output={\"response\": msg},\n                )\n\n            # Extract citation metadata\n            pattern = r\"\\[(\\d+)\\]\"\n            matches = re.findall(pattern, response.response)\n            citation_numbers = sorted(set(int(match) for match in matches))\n            citation_metadata = \"\"\n            keys_to_ignore = [\"lang\", \"offset\", \"len\"]\n            for citation_number in citation_numbers:\n                metadata = response.source_nodes[citation_number - 1].metadata\n                citation_metadata += (\n                    f\"[{citation_number}]: \"\n                    + \"; \".join(\n                        [\n                            f\"{k}='{v}'\"\n                            for k, v in metadata.items()\n                            if k not in keys_to_ignore\n                        ]\n                    )\n                    + \".\\n\"\n                )\n            fcs = 0.0\n            fcs_str = response.metadata[\"fcs\"] if \"fcs\" in response.metadata else \"0.0\"\n            if fcs_str and is_float(fcs_str):\n                fcs = float(fcs_str)\n                if fcs &lt; fcs_threshold:\n                    msg = f\"Could not answer the query due to suspected hallucination (fcs={fcs}).\"\n                    return ToolOutput(\n                        tool_name=rag_function.__name__,\n                        content=msg,\n                        raw_input={\"args\": args, \"kwargs\": kwargs},\n                        raw_output={\"response\": msg},\n                    )\n            res = {\n                \"response\": response.response,\n                \"references_metadata\": citation_metadata,\n                \"fcs_score\": fcs,\n            }\n            if len(citation_metadata) &gt; 0:\n                tool_output = f\"\"\"\n                    Response: '''{res['response']}'''\n                    fcs_score: {res['fcs_score']:.4f}\n                    References:\n                    {res['references_metadata']}\n                \"\"\"\n            else:\n                tool_output = f\"Response: '''{res['response']}'''\"\n            out = ToolOutput(\n                tool_name=rag_function.__name__,\n                content=tool_output,\n                raw_input={\"args\": args, \"kwargs\": kwargs},\n                raw_output=res,\n            )\n            return out\n\n        class RagToolBaseParams(BaseModel):\n            \"\"\"Model for the base parameters of the RAG tool.\"\"\"\n\n            query: str = Field(\n                ...,\n                description=\"The search query to perform, in the form of a question\",\n            )\n\n        tool = create_tool_from_dynamic_function(\n            rag_function,\n            tool_name,\n            tool_description,\n            RagToolBaseParams,\n            tool_args_schema,\n            compact_docstring=self.compact_docstring,\n            return_direct=return_direct,\n        )\n        return tool\n</code></pre>"},{"location":"api/#vectara_agentic.VectaraToolFactory.__init__","title":"<code>__init__(vectara_corpus_key=str(os.environ.get('VECTARA_CORPUS_KEY', '')), vectara_api_key=str(os.environ.get('VECTARA_API_KEY', '')), compact_docstring=False)</code>","text":"<p>Initialize the VectaraToolFactory Args:     vectara_corpus_key (str): The Vectara corpus key (or comma separated list of keys).     vectara_api_key (str): The Vectara API key.     compact_docstring (bool): Whether to use a compact docstring format for tools       This is useful if OpenAI complains on the 1024 token limit.</p> Source code in <code>vectara_agentic/tools.py</code> <pre><code>def __init__(\n    self,\n    vectara_corpus_key: str = str(os.environ.get(\"VECTARA_CORPUS_KEY\", \"\")),\n    vectara_api_key: str = str(os.environ.get(\"VECTARA_API_KEY\", \"\")),\n    compact_docstring: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initialize the VectaraToolFactory\n    Args:\n        vectara_corpus_key (str): The Vectara corpus key (or comma separated list of keys).\n        vectara_api_key (str): The Vectara API key.\n        compact_docstring (bool): Whether to use a compact docstring format for tools\n          This is useful if OpenAI complains on the 1024 token limit.\n    \"\"\"\n    self.vectara_corpus_key = vectara_corpus_key\n    self.vectara_api_key = vectara_api_key\n    self.num_corpora = len(vectara_corpus_key.split(\",\"))\n    self.compact_docstring = compact_docstring\n</code></pre>"},{"location":"api/#vectara_agentic.VectaraToolFactory.create_rag_tool","title":"<code>create_rag_tool(tool_name, tool_description, tool_args_schema=None, tool_args_type={}, fixed_filter='', vectara_summarizer='vectara-summary-ext-24-05-med-omni', vectara_prompt_text=None, summary_num_results=5, summary_response_lang='eng', n_sentences_before=2, n_sentences_after=2, offset=0, lambda_val=0.005, semantics='default', custom_dimensions={}, reranker='slingshot', rerank_k=50, rerank_limit=None, rerank_cutoff=None, mmr_diversity_bias=0.2, udf_expression=None, rerank_chain=None, max_response_chars=None, max_tokens=None, llm_name=None, temperature=None, frequency_penalty=None, presence_penalty=None, include_citations=True, save_history=False, fcs_threshold=0.0, return_direct=False, verbose=False, vectara_base_url='https://api.vectara.io', vectara_verify_ssl=True)</code>","text":"<p>Creates a RAG (Retrieve and Generate) tool.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>The name of the tool.</p> required <code>tool_description</code> <code>str</code> <p>The description of the tool.</p> required <code>tool_args_schema</code> <code>BaseModel</code> <p>The schema for any tool arguments for filtering.</p> <code>None</code> <code>tool_args_type</code> <code>Dict[str, dict]</code> <p>attributes for each argument where they key is the field name and the value is a dictionary with the following keys: - 'type': the type of each filter attribute in Vectara (doc or part). - 'is_list': whether the filterable attribute is a list. - 'filter_name': the name of the filterable attribute in Vectara.</p> <code>{}</code> <code>fixed_filter</code> <code>str</code> <p>A fixed Vectara filter condition to apply to all queries.</p> <code>''</code> <code>vectara_summarizer</code> <code>str</code> <p>The Vectara summarizer to use.</p> <code>'vectara-summary-ext-24-05-med-omni'</code> <code>vectara_prompt_text</code> <code>str</code> <p>The prompt text for the Vectara summarizer.</p> <code>None</code> <code>summary_num_results</code> <code>int</code> <p>The number of summary results.</p> <code>5</code> <code>summary_response_lang</code> <code>str</code> <p>The response language for the summary.</p> <code>'eng'</code> <code>n_sentences_before</code> <code>int</code> <p>Number of sentences before the summary.</p> <code>2</code> <code>n_sentences_after</code> <code>int</code> <p>Number of sentences after the summary.</p> <code>2</code> <code>offset</code> <code>int</code> <p>Number of results to skip.</p> <code>0</code> <code>lambda_val</code> <code>Union[List[float] | float]</code> <p>Lambda value (or list of values for each corpora) for the Vectara query, when using hybrid search.</p> <code>0.005</code> <code>semantics</code> <code>Union[List[str], str]</code> <p>Indicates whether the query is intended as a query or response. Include list if using multiple corpora specifying the query type for each corpus.</p> <code>'default'</code> <code>custom_dimensions</code> <code>Union[List[Dict] | Dict]</code> <p>Custom dimensions for the query (for each corpora).</p> <code>{}</code> <code>reranker</code> <code>str</code> <p>The reranker mode.</p> <code>'slingshot'</code> <code>rerank_k</code> <code>int</code> <p>Number of top-k documents for reranking.</p> <code>50</code> <code>rerank_limit</code> <code>int</code> <p>Maximum number of results to return after reranking.</p> <code>None</code> <code>rerank_cutoff</code> <code>float</code> <p>Minimum score threshold for results to include after reranking.</p> <code>None</code> <code>mmr_diversity_bias</code> <code>float</code> <p>MMR diversity bias.</p> <code>0.2</code> <code>udf_expression</code> <code>str</code> <p>The user defined expression for reranking results.</p> <code>None</code> <code>rerank_chain</code> <code>List[Dict]</code> <p>A list of rerankers to be applied sequentially. Each dictionary should specify the \"type\" of reranker (mmr, slingshot, udf) and any other parameters (e.g. \"limit\" or \"cutoff\" for any type, \"diversity_bias\" for mmr, and \"user_function\" for udf). If using slingshot/multilingual_reranker_v1, it must be first in the list.</p> <code>None</code> <code>max_response_chars</code> <code>int</code> <p>The desired maximum number of characters for the generated summary.</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to be returned by the LLM.</p> <code>None</code> <code>llm_name</code> <code>str</code> <p>The name of the LLM to use for generation.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>The sampling temperature; higher values lead to more randomness.</p> <code>None</code> <code>frequency_penalty</code> <code>float</code> <p>How much to penalize repeating tokens in the response, higher values reducing likelihood of repeating the same line.</p> <code>None</code> <code>presence_penalty</code> <code>float</code> <p>How much to penalize repeating tokens in the response, higher values increasing the diversity of topics.</p> <code>None</code> <code>include_citations</code> <code>bool</code> <p>Whether to include citations in the response. If True, uses markdown vectara citations that requires the Vectara scale plan.</p> <code>True</code> <code>save_history</code> <code>bool</code> <p>Whether to save the query in history.</p> <code>False</code> <code>fcs_threshold</code> <code>float</code> <p>A threshold for factual consistency. If set above 0, the tool notifies the calling agent that it \"cannot respond\" if FCS is too low.</p> <code>0.0</code> <code>return_direct</code> <code>bool</code> <p>Whether the agent should return the tool's response directly.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose output.</p> <code>False</code> <code>vectara_base_url</code> <code>str</code> <p>The base URL for the Vectara API.</p> <code>'https://api.vectara.io'</code> <code>vectara_verify_ssl</code> <code>bool</code> <p>Whether to verify SSL certificates for the Vectara API.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>VectaraTool</code> <code>VectaraTool</code> <p>A VectaraTool object.</p> Source code in <code>vectara_agentic/tools.py</code> <pre><code>def create_rag_tool(\n    self,\n    tool_name: str,\n    tool_description: str,\n    tool_args_schema: type[BaseModel] = None,\n    tool_args_type: Dict[str, dict] = {},\n    fixed_filter: str = \"\",\n    vectara_summarizer: str = \"vectara-summary-ext-24-05-med-omni\",\n    vectara_prompt_text: str = None,\n    summary_num_results: int = 5,\n    summary_response_lang: str = \"eng\",\n    n_sentences_before: int = 2,\n    n_sentences_after: int = 2,\n    offset: int = 0,\n    lambda_val: Union[List[float], float] = 0.005,\n    semantics: Union[List[str] | str] = \"default\",\n    custom_dimensions: Union[List[Dict], Dict] = {},\n    reranker: str = \"slingshot\",\n    rerank_k: int = 50,\n    rerank_limit: Optional[int] = None,\n    rerank_cutoff: Optional[float] = None,\n    mmr_diversity_bias: float = 0.2,\n    udf_expression: str = None,\n    rerank_chain: List[Dict] = None,\n    max_response_chars: Optional[int] = None,\n    max_tokens: Optional[int] = None,\n    llm_name: Optional[str] = None,\n    temperature: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    include_citations: bool = True,\n    save_history: bool = False,\n    fcs_threshold: float = 0.0,\n    return_direct: bool = False,\n    verbose: bool = False,\n    vectara_base_url: str = \"https://api.vectara.io\",\n    vectara_verify_ssl: bool = True,\n) -&gt; VectaraTool:\n    \"\"\"\n    Creates a RAG (Retrieve and Generate) tool.\n\n    Args:\n        tool_name (str): The name of the tool.\n        tool_description (str): The description of the tool.\n        tool_args_schema (BaseModel, optional): The schema for any tool arguments for filtering.\n        tool_args_type (Dict[str, dict], optional): attributes for each argument where they key is the field name\n            and the value is a dictionary with the following keys:\n            - 'type': the type of each filter attribute in Vectara (doc or part).\n            - 'is_list': whether the filterable attribute is a list.\n            - 'filter_name': the name of the filterable attribute in Vectara.\n        fixed_filter (str, optional): A fixed Vectara filter condition to apply to all queries.\n        vectara_summarizer (str, optional): The Vectara summarizer to use.\n        vectara_prompt_text (str, optional): The prompt text for the Vectara summarizer.\n        summary_num_results (int, optional): The number of summary results.\n        summary_response_lang (str, optional): The response language for the summary.\n        n_sentences_before (int, optional): Number of sentences before the summary.\n        n_sentences_after (int, optional): Number of sentences after the summary.\n        offset (int, optional): Number of results to skip.\n        lambda_val (Union[List[float] | float], optional): Lambda value (or list of values for each corpora)\n            for the Vectara query, when using hybrid search.\n        semantics (Union[List[str], str], optional): Indicates whether the query is intended as a query or response.\n            Include list if using multiple corpora specifying the query type for each corpus.\n        custom_dimensions (Union[List[Dict] | Dict], optional): Custom dimensions for the query (for each corpora).\n        reranker (str, optional): The reranker mode.\n        rerank_k (int, optional): Number of top-k documents for reranking.\n        rerank_limit (int, optional): Maximum number of results to return after reranking.\n        rerank_cutoff (float, optional): Minimum score threshold for results to include after reranking.\n        mmr_diversity_bias (float, optional): MMR diversity bias.\n        udf_expression (str, optional): The user defined expression for reranking results.\n        rerank_chain (List[Dict], optional): A list of rerankers to be applied sequentially.\n            Each dictionary should specify the \"type\" of reranker (mmr, slingshot, udf)\n            and any other parameters (e.g. \"limit\" or \"cutoff\" for any type,\n            \"diversity_bias\" for mmr, and \"user_function\" for udf).\n            If using slingshot/multilingual_reranker_v1, it must be first in the list.\n        max_response_chars (int, optional): The desired maximum number of characters for the generated summary.\n        max_tokens (int, optional): The maximum number of tokens to be returned by the LLM.\n        llm_name (str, optional): The name of the LLM to use for generation.\n        temperature (float, optional): The sampling temperature; higher values lead to more randomness.\n        frequency_penalty (float, optional): How much to penalize repeating tokens in the response,\n            higher values reducing likelihood of repeating the same line.\n        presence_penalty (float, optional): How much to penalize repeating tokens in the response,\n            higher values increasing the diversity of topics.\n        include_citations (bool, optional): Whether to include citations in the response.\n            If True, uses markdown vectara citations that requires the Vectara scale plan.\n        save_history (bool, optional): Whether to save the query in history.\n        fcs_threshold (float, optional): A threshold for factual consistency.\n            If set above 0, the tool notifies the calling agent that it \"cannot respond\" if FCS is too low.\n        return_direct (bool, optional): Whether the agent should return the tool's response directly.\n        verbose (bool, optional): Whether to print verbose output.\n        vectara_base_url (str, optional): The base URL for the Vectara API.\n        vectara_verify_ssl (bool, optional): Whether to verify SSL certificates for the Vectara API.\n\n    Returns:\n        VectaraTool: A VectaraTool object.\n    \"\"\"\n\n    vectara = VectaraIndex(\n        vectara_api_key=self.vectara_api_key,\n        vectara_corpus_key=self.vectara_corpus_key,\n        x_source_str=\"vectara-agentic\",\n        vectara_base_url=vectara_base_url,\n        vectara_verify_ssl=vectara_verify_ssl,\n    )\n\n    # Dynamically generate the RAG function\n    def rag_function(*args: Any, **kwargs: Any) -&gt; ToolOutput:\n        \"\"\"\n        Dynamically generated function for RAG query with Vectara.\n        \"\"\"\n        # Convert args to kwargs using the function signature\n        sig = inspect.signature(rag_function)\n        bound_args = sig.bind_partial(*args, **kwargs)\n        bound_args.apply_defaults()\n        kwargs = bound_args.arguments\n\n        query = kwargs.pop(\"query\")\n        try:\n            filter_string = build_filter_string(\n                kwargs, tool_args_type, fixed_filter\n            )\n        except ValueError as e:\n            return ToolOutput(\n                tool_name=rag_function.__name__,\n                content=str(e),\n                raw_input={\"args\": args, \"kwargs\": kwargs},\n                raw_output={\"response\": str(e)},\n            )\n\n        vectara_query_engine = vectara.as_query_engine(\n            summary_enabled=True,\n            similarity_top_k=summary_num_results,\n            summary_num_results=summary_num_results,\n            summary_response_lang=summary_response_lang,\n            summary_prompt_name=vectara_summarizer,\n            prompt_text=vectara_prompt_text,\n            reranker=reranker,\n            rerank_k=(\n                rerank_k\n                if rerank_k * self.num_corpora &lt;= 100\n                else int(100 / self.num_corpora)\n            ),\n            rerank_limit=rerank_limit,\n            rerank_cutoff=rerank_cutoff,\n            mmr_diversity_bias=mmr_diversity_bias,\n            udf_expression=udf_expression,\n            rerank_chain=rerank_chain,\n            n_sentences_before=n_sentences_before,\n            n_sentences_after=n_sentences_after,\n            offset=offset,\n            lambda_val=lambda_val,\n            semantics=semantics,\n            custom_dimensions=custom_dimensions,\n            filter=filter_string,\n            max_response_chars=max_response_chars,\n            max_tokens=max_tokens,\n            llm_name=llm_name,\n            temperature=temperature,\n            frequency_penalty=frequency_penalty,\n            presence_penalty=presence_penalty,\n            citations_style=\"markdown\" if include_citations else None,\n            citations_url_pattern=\"{doc.url}\" if include_citations else None,\n            save_history=save_history,\n            x_source_str=\"vectara-agentic\",\n            verbose=verbose,\n        )\n        response = vectara_query_engine.query(query)\n\n        if len(response.source_nodes) == 0:\n            msg = (\n                \"Tool failed to generate a response since no matches were found. \"\n                \"Please check the arguments and try again.\"\n            )\n            return ToolOutput(\n                tool_name=rag_function.__name__,\n                content=msg,\n                raw_input={\"args\": args, \"kwargs\": kwargs},\n                raw_output={\"response\": msg},\n            )\n        if str(response) == \"None\":\n            msg = \"Tool failed to generate a response.\"\n            return ToolOutput(\n                tool_name=rag_function.__name__,\n                content=msg,\n                raw_input={\"args\": args, \"kwargs\": kwargs},\n                raw_output={\"response\": msg},\n            )\n\n        # Extract citation metadata\n        pattern = r\"\\[(\\d+)\\]\"\n        matches = re.findall(pattern, response.response)\n        citation_numbers = sorted(set(int(match) for match in matches))\n        citation_metadata = \"\"\n        keys_to_ignore = [\"lang\", \"offset\", \"len\"]\n        for citation_number in citation_numbers:\n            metadata = response.source_nodes[citation_number - 1].metadata\n            citation_metadata += (\n                f\"[{citation_number}]: \"\n                + \"; \".join(\n                    [\n                        f\"{k}='{v}'\"\n                        for k, v in metadata.items()\n                        if k not in keys_to_ignore\n                    ]\n                )\n                + \".\\n\"\n            )\n        fcs = 0.0\n        fcs_str = response.metadata[\"fcs\"] if \"fcs\" in response.metadata else \"0.0\"\n        if fcs_str and is_float(fcs_str):\n            fcs = float(fcs_str)\n            if fcs &lt; fcs_threshold:\n                msg = f\"Could not answer the query due to suspected hallucination (fcs={fcs}).\"\n                return ToolOutput(\n                    tool_name=rag_function.__name__,\n                    content=msg,\n                    raw_input={\"args\": args, \"kwargs\": kwargs},\n                    raw_output={\"response\": msg},\n                )\n        res = {\n            \"response\": response.response,\n            \"references_metadata\": citation_metadata,\n            \"fcs_score\": fcs,\n        }\n        if len(citation_metadata) &gt; 0:\n            tool_output = f\"\"\"\n                Response: '''{res['response']}'''\n                fcs_score: {res['fcs_score']:.4f}\n                References:\n                {res['references_metadata']}\n            \"\"\"\n        else:\n            tool_output = f\"Response: '''{res['response']}'''\"\n        out = ToolOutput(\n            tool_name=rag_function.__name__,\n            content=tool_output,\n            raw_input={\"args\": args, \"kwargs\": kwargs},\n            raw_output=res,\n        )\n        return out\n\n    class RagToolBaseParams(BaseModel):\n        \"\"\"Model for the base parameters of the RAG tool.\"\"\"\n\n        query: str = Field(\n            ...,\n            description=\"The search query to perform, in the form of a question\",\n        )\n\n    tool = create_tool_from_dynamic_function(\n        rag_function,\n        tool_name,\n        tool_description,\n        RagToolBaseParams,\n        tool_args_schema,\n        compact_docstring=self.compact_docstring,\n        return_direct=return_direct,\n    )\n    return tool\n</code></pre>"},{"location":"api/#vectara_agentic.VectaraToolFactory.create_search_tool","title":"<code>create_search_tool(tool_name, tool_description, tool_args_schema=None, tool_args_type={}, summarize_docs=None, summarize_llm_name=None, fixed_filter='', lambda_val=0.005, semantics='default', custom_dimensions={}, offset=0, n_sentences_before=2, n_sentences_after=2, reranker='slingshot', rerank_k=50, rerank_limit=None, rerank_cutoff=None, mmr_diversity_bias=0.2, udf_expression=None, rerank_chain=None, return_direct=False, save_history=True, verbose=False, vectara_base_url='https://api.vectara.io', vectara_verify_ssl=True)</code>","text":"<p>Creates a Vectara search/retrieval tool</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>The name of the tool.</p> required <code>tool_description</code> <code>str</code> <p>The description of the tool.</p> required <code>tool_args_schema</code> <code>BaseModel</code> <p>The schema for the tool arguments.</p> <code>None</code> <code>tool_args_type</code> <code>Dict[str, dict]</code> <p>attributes for each argument where they key is the field name and the value is a dictionary with the following keys: - 'type': the type of each filter attribute in Vectara (doc or part). - 'is_list': whether the filterable attribute is a list. - 'filter_name': the name of the filterable attribute in Vectara.</p> <code>{}</code> <code>fixed_filter</code> <code>str</code> <p>A fixed Vectara filter condition to apply to all queries.</p> <code>''</code> <code>lambda_val</code> <code>Union[List[float] | float]</code> <p>Lambda value (or list of values for each corpora) for the Vectara query, when using hybrid search.</p> <code>0.005</code> <code>semantics</code> <code>Union[List[str], str]</code> <p>Indicates whether the query is intended as a query or response. Include list if using multiple corpora specifying the query type for each corpus.</p> <code>'default'</code> <code>custom_dimensions</code> <code>Union[List[Dict] | Dict]</code> <p>Custom dimensions for the query (for each corpora).</p> <code>{}</code> <code>offset</code> <code>int</code> <p>Number of results to skip.</p> <code>0</code> <code>n_sentences_before</code> <code>int</code> <p>Number of sentences before the matching document part.</p> <code>2</code> <code>n_sentences_after</code> <code>int</code> <p>Number of sentences after the matching document part.</p> <code>2</code> <code>reranker</code> <code>str</code> <p>The reranker mode.</p> <code>'slingshot'</code> <code>rerank_k</code> <code>int</code> <p>Number of top-k documents for reranking.</p> <code>50</code> <code>rerank_limit</code> <code>int</code> <p>Maximum number of results to return after reranking.</p> <code>None</code> <code>rerank_cutoff</code> <code>float</code> <p>Minimum score threshold for results to include after reranking.</p> <code>None</code> <code>mmr_diversity_bias</code> <code>float</code> <p>MMR diversity bias.</p> <code>0.2</code> <code>udf_expression</code> <code>str</code> <p>the user defined expression for reranking results.</p> <code>None</code> <code>rerank_chain</code> <code>List[Dict]</code> <p>A list of rerankers to be applied sequentially. Each dictionary should specify the \"type\" of reranker (mmr, slingshot, udf) and any other parameters (e.g. \"limit\" or \"cutoff\" for any type, \"diversity_bias\" for mmr, and \"user_function\" for udf). If using slingshot/multilingual_reranker_v1, it must be first in the list.</p> <code>None</code> <code>save_history</code> <code>bool</code> <p>Whether to save the query in history.</p> <code>True</code> <code>return_direct</code> <code>bool</code> <p>Whether the agent should return the tool's response directly.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose output.</p> <code>False</code> <code>vectara_base_url</code> <code>str</code> <p>The base URL for the Vectara API.</p> <code>'https://api.vectara.io'</code> <code>vectara_verify_ssl</code> <code>bool</code> <p>Whether to verify SSL certificates for the Vectara API.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>VectaraTool</code> <code>VectaraTool</code> <p>A VectaraTool object.</p> Source code in <code>vectara_agentic/tools.py</code> <pre><code>def create_search_tool(\n    self,\n    tool_name: str,\n    tool_description: str,\n    tool_args_schema: type[BaseModel] = None,\n    tool_args_type: Dict[str, str] = {},\n    summarize_docs: Optional[bool] = None,\n    summarize_llm_name: Optional[str] = None,\n    fixed_filter: str = \"\",\n    lambda_val: Union[List[float], float] = 0.005,\n    semantics: Union[List[str] | str] = \"default\",\n    custom_dimensions: Union[List[Dict], Dict] = {},\n    offset: int = 0,\n    n_sentences_before: int = 2,\n    n_sentences_after: int = 2,\n    reranker: str = \"slingshot\",\n    rerank_k: int = 50,\n    rerank_limit: Optional[int] = None,\n    rerank_cutoff: Optional[float] = None,\n    mmr_diversity_bias: float = 0.2,\n    udf_expression: str = None,\n    rerank_chain: List[Dict] = None,\n    return_direct: bool = False,\n    save_history: bool = True,\n    verbose: bool = False,\n    vectara_base_url: str = \"https://api.vectara.io\",\n    vectara_verify_ssl: bool = True,\n) -&gt; VectaraTool:\n    \"\"\"\n    Creates a Vectara search/retrieval tool\n\n    Args:\n        tool_name (str): The name of the tool.\n        tool_description (str): The description of the tool.\n        tool_args_schema (BaseModel, optional): The schema for the tool arguments.\n        tool_args_type (Dict[str, dict], optional): attributes for each argument where they key is the field name\n            and the value is a dictionary with the following keys:\n            - 'type': the type of each filter attribute in Vectara (doc or part).\n            - 'is_list': whether the filterable attribute is a list.\n            - 'filter_name': the name of the filterable attribute in Vectara.\n        fixed_filter (str, optional): A fixed Vectara filter condition to apply to all queries.\n        lambda_val (Union[List[float] | float], optional): Lambda value (or list of values for each corpora)\n            for the Vectara query, when using hybrid search.\n        semantics (Union[List[str], str], optional): Indicates whether the query is intended as a query or response.\n            Include list if using multiple corpora specifying the query type for each corpus.\n        custom_dimensions (Union[List[Dict] | Dict], optional): Custom dimensions for the query (for each corpora).\n        offset (int, optional): Number of results to skip.\n        n_sentences_before (int, optional): Number of sentences before the matching document part.\n        n_sentences_after (int, optional): Number of sentences after the matching document part.\n        reranker (str, optional): The reranker mode.\n        rerank_k (int, optional): Number of top-k documents for reranking.\n        rerank_limit (int, optional): Maximum number of results to return after reranking.\n        rerank_cutoff (float, optional): Minimum score threshold for results to include after reranking.\n        mmr_diversity_bias (float, optional): MMR diversity bias.\n        udf_expression (str, optional): the user defined expression for reranking results.\n        rerank_chain (List[Dict], optional): A list of rerankers to be applied sequentially.\n            Each dictionary should specify the \"type\" of reranker (mmr, slingshot, udf)\n            and any other parameters (e.g. \"limit\" or \"cutoff\" for any type,\n            \"diversity_bias\" for mmr, and \"user_function\" for udf).\n            If using slingshot/multilingual_reranker_v1, it must be first in the list.\n        save_history (bool, optional): Whether to save the query in history.\n        return_direct (bool, optional): Whether the agent should return the tool's response directly.\n        verbose (bool, optional): Whether to print verbose output.\n        vectara_base_url (str, optional): The base URL for the Vectara API.\n        vectara_verify_ssl (bool, optional): Whether to verify SSL certificates for the Vectara API.\n\n    Returns:\n        VectaraTool: A VectaraTool object.\n    \"\"\"\n\n    vectara = VectaraIndex(\n        vectara_api_key=self.vectara_api_key,\n        vectara_corpus_key=self.vectara_corpus_key,\n        x_source_str=\"vectara-agentic\",\n        vectara_base_url=vectara_base_url,\n        vectara_verify_ssl=vectara_verify_ssl,\n    )\n\n    # Dynamically generate the search function\n    def search_function(*args: Any, **kwargs: Any) -&gt; ToolOutput:\n        \"\"\"\n        Dynamically generated function for semantic search Vectara.\n        \"\"\"\n        # Convert args to kwargs using the function signature\n        sig = inspect.signature(search_function)\n        bound_args = sig.bind_partial(*args, **kwargs)\n        bound_args.apply_defaults()\n        kwargs = bound_args.arguments\n\n        query = kwargs.pop(\"query\")\n        top_k = kwargs.pop(\"top_k\", 10)\n        summarize = (\n            kwargs.pop(\"summarize\", True)\n            if summarize_docs is None\n            else summarize_docs\n        )\n        try:\n            filter_string = build_filter_string(\n                kwargs, tool_args_type, fixed_filter\n            )\n        except ValueError as e:\n            return ToolOutput(\n                tool_name=search_function.__name__,\n                content=str(e),\n                raw_input={\"args\": args, \"kwargs\": kwargs},\n                raw_output={\"response\": str(e)},\n            )\n\n        vectara_retriever = vectara.as_retriever(\n            summary_enabled=False,\n            similarity_top_k=top_k,\n            reranker=reranker,\n            rerank_k=(\n                rerank_k\n                if rerank_k * self.num_corpora &lt;= 100\n                else int(100 / self.num_corpora)\n            ),\n            rerank_limit=rerank_limit,\n            rerank_cutoff=rerank_cutoff,\n            mmr_diversity_bias=mmr_diversity_bias,\n            udf_expression=udf_expression,\n            rerank_chain=rerank_chain,\n            lambda_val=lambda_val,\n            semantics=semantics,\n            custom_dimensions=custom_dimensions,\n            offset=offset,\n            filter=filter_string,\n            n_sentences_before=n_sentences_before,\n            n_sentences_after=n_sentences_after,\n            save_history=save_history,\n            x_source_str=\"vectara-agentic\",\n            verbose=verbose,\n        )\n        response = vectara_retriever.retrieve(query)\n\n        if len(response) == 0:\n            msg = \"Vectara Tool failed to retrieve any results for the query.\"\n            return ToolOutput(\n                tool_name=search_function.__name__,\n                content=msg,\n                raw_input={\"args\": args, \"kwargs\": kwargs},\n                raw_output={\"response\": msg},\n            )\n        unique_ids = set()\n        docs = []\n        for doc in response:\n            if doc.id_ in unique_ids:\n                continue\n            unique_ids.add(doc.id_)\n            docs.append((doc.id_, doc.metadata))\n        tool_output = \"Matching documents:\\n\"\n        if summarize:\n            summaries_dict = asyncio.run(\n                summarize_documents(\n                    corpus_key=self.vectara_corpus_key,\n                    api_key=self.vectara_api_key,\n                    llm_name=summarize_llm_name,\n                    doc_ids=list(unique_ids),\n                )\n            )\n            for doc_id, metadata in docs:\n                summary = summaries_dict.get(doc_id, \"\")\n                tool_output += f\"document_id: '{doc_id}'\\nmetadata: '{metadata}'\\nsummary: '{summary}'\\n\\n\"\n        else:\n            for doc_id, metadata in docs:\n                tool_output += (\n                    f\"document_id: '{doc_id}'\\nmetadata: '{metadata}'\\n\\n\"\n                )\n\n        out = ToolOutput(\n            tool_name=search_function.__name__,\n            content=tool_output,\n            raw_input={\"args\": args, \"kwargs\": kwargs},\n            raw_output=response,\n        )\n        return out\n\n    class SearchToolBaseParams(BaseModel):\n        \"\"\"Model for the base parameters of the search tool.\"\"\"\n\n        query: str = Field(\n            ...,\n            description=\"The search query to perform, in the form of a question.\",\n        )\n        top_k: int = Field(\n            default=10, description=\"The number of top documents to retrieve.\"\n        )\n        summarize: bool = Field(\n            True,\n            description=\"Whether to summarize the retrieved documents.\",\n        )\n\n    class SearchToolBaseParamsWithoutSummarize(BaseModel):\n        \"\"\"Model for the base parameters of the search tool.\"\"\"\n\n        query: str = Field(\n            ...,\n            description=\"The search query to perform, in the form of a question.\",\n        )\n        top_k: int = Field(\n            10, description=\"The number of top documents to retrieve.\"\n        )\n\n    search_tool_extra_desc = (\n        tool_description\n        + \"\\n\"\n        + \"Use this tool to search for relevant documents, not to ask questions.\"\n    )\n\n    tool = create_tool_from_dynamic_function(\n        search_function,\n        tool_name,\n        search_tool_extra_desc,\n        (\n            SearchToolBaseParams\n            if summarize_docs is None\n            else SearchToolBaseParamsWithoutSummarize\n        ),\n        tool_args_schema,\n        compact_docstring=self.compact_docstring,\n        return_direct=return_direct,\n    )\n    return tool\n</code></pre>"},{"location":"api/#vectara_agentic.create_app","title":"<code>create_app(agent, config)</code>","text":"<p>Create and configure the FastAPI app.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>The agent instance to handle chat/completion.</p> required <code>config</code> <code>AgentConfig</code> <p>Configuration containing the API key.</p> required <p>Returns:</p> Name Type Description <code>FastAPI</code> <code>FastAPI</code> <p>Configured FastAPI application.</p> Source code in <code>vectara_agentic/agent_endpoint.py</code> <pre><code>def create_app(agent: Agent, config: AgentConfig) -&gt; FastAPI:\n    \"\"\"\n    Create and configure the FastAPI app.\n\n    Args:\n        agent (Agent): The agent instance to handle chat/completion.\n        config (AgentConfig): Configuration containing the API key.\n\n    Returns:\n        FastAPI: Configured FastAPI application.\n    \"\"\"\n    app = FastAPI()\n    logger = logging.getLogger(\"uvicorn.error\")\n    logging.basicConfig(level=logging.INFO)\n\n    api_key_header = APIKeyHeader(name=\"X-API-Key\")\n\n    async def _verify_api_key(api_key: str = Depends(api_key_header)):\n        \"\"\"\n        Dependency that verifies the X-API-Key header.\n\n        Raises:\n            HTTPException(403): If the provided key does not match.\n\n        Returns:\n            bool: True if key is valid.\n        \"\"\"\n        if api_key != config.endpoint_api_key:\n            raise HTTPException(status_code=403, detail=\"Unauthorized\")\n        return True\n\n    @app.get(\n        \"/chat\", summary=\"Chat with the agent\", dependencies=[Depends(_verify_api_key)]\n    )\n    async def chat(message: str):\n        \"\"\"\n        Handle GET /chat requests.\n\n        Args:\n            message (str): The user's message to the agent.\n\n        Returns:\n            dict: Contains the agent's response under 'response'.\n\n        Raises:\n            HTTPException(400): If message is empty.\n            HTTPException(500): On internal errors.\n        \"\"\"\n        if not message:\n            raise HTTPException(status_code=400, detail=\"No message provided\")\n        try:\n            res = agent.chat(message)\n            return {\"response\": res}\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=\"Internal server error\") from e\n\n    @app.post(\n        \"/v1/completions\",\n        response_model=CompletionResponse,\n        dependencies=[Depends(_verify_api_key)],\n    )\n    async def completions(req: CompletionRequest):\n        \"\"\"\n        Handle POST /v1/completions requests.\n\n        Args:\n            req (CompletionRequest): The completion request payload.\n\n        Returns:\n            CompletionResponse: The generated completion and usage stats.\n\n        Raises:\n            HTTPException(400): If prompt is missing.\n            HTTPException(500): On internal errors.\n        \"\"\"\n        if not req.prompt:\n            raise HTTPException(status_code=400, detail=\"`prompt` is required\")\n        raw = req.prompt if isinstance(req.prompt, str) else req.prompt[0]\n        try:\n            start = time.time()\n            text = agent.chat(raw)\n            logger.info(f\"Agent returned in {time.time()-start:.2f}s\")\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=\"Internal server error\") from e\n\n        p_tokens = len(raw.split())\n        c_tokens = len(text.split())\n\n        return CompletionResponse(\n            id=f\"cmpl-{uuid.uuid4()}\",\n            object=\"text_completion\",\n            created=int(time.time()),\n            model=req.model,\n            choices=[Choice(text=text, index=0, logprobs=None, finish_reason=\"stop\")],\n            usage=CompletionUsage(\n                prompt_tokens=p_tokens,\n                completion_tokens=c_tokens,\n                total_tokens=p_tokens + c_tokens,\n            ),\n        )\n\n    @app.post(\n        \"/v1/chat\",\n        response_model=ChatCompletionResponse,\n        dependencies=[Depends(_verify_api_key)],\n    )\n    async def chat_completion(req: ChatCompletionRequest):\n        if not req.messages:\n            raise HTTPException(status_code=400, detail=\"`messages` is required\")\n\n        # concatenate all user messages into a single prompt\n        raw = \" \".join(m.content for m in req.messages if m.role == \"user\")\n\n        try:\n            start = time.time()\n            text = agent.chat(raw)\n            logger.info(f\"Agent returned in {time.time()-start:.2f}s\")\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=\"Internal server error\") from e\n\n        p_tokens = len(raw.split())\n        c_tokens = len(text.split())\n\n        return ChatCompletionResponse(\n            id=f\"chatcmpl-{uuid.uuid4()}\",\n            object=\"chat.completion\",\n            created=int(time.time()),\n            model=req.model,\n            choices=[\n                ChatCompletionChoice(\n                    index=0,\n                    message=ChatMessage(role=\"assistant\", content=text),\n                    finish_reason=\"stop\",\n                )\n            ],\n            usage=CompletionUsage(\n                prompt_tokens=p_tokens,\n                completion_tokens=c_tokens,\n                total_tokens=p_tokens + c_tokens,\n            ),\n        )\n\n    return app\n</code></pre>"},{"location":"api/#vectara_agentic.start_app","title":"<code>start_app(agent, host='0.0.0.0', port=8000)</code>","text":"<p>Launch the FastAPI application using Uvicorn.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>The agent instance for request handling.</p> required <code>host</code> <code>str</code> <p>Host interface. Defaults to \"0.0.0.0\".</p> <code>'0.0.0.0'</code> <code>port</code> <code>int</code> <p>Port number. Defaults to 8000.</p> <code>8000</code> Source code in <code>vectara_agentic/agent_endpoint.py</code> <pre><code>def start_app(agent: Agent, host=\"0.0.0.0\", port=8000):\n    \"\"\"\n    Launch the FastAPI application using Uvicorn.\n\n    Args:\n        agent (Agent): The agent instance for request handling.\n        host (str, optional): Host interface. Defaults to \"0.0.0.0\".\n        port (int, optional): Port number. Defaults to 8000.\n    \"\"\"\n    app = create_app(agent, config=AgentConfig())\n    uvicorn.run(app, host=host, port=port)\n</code></pre>"},{"location":"endpoint/","title":"API Endpoint","text":"<p>It\\'s super easy to host your vectara-agentic assistant or agent behind an API endpoint:</p> <p><code>vectara-agentic</code> can be easily hosted locally or on a remote machine behind an API endpoint, by following theses steps:</p> <p>1. Setup your API key Ensure that you have your API key set up as an environment variable:</p> <pre><code>export VECTARA_AGENTIC_API_KEY=&lt;YOUR-ENDPOINT-API-KEY&gt;\n</code></pre> <p>2. Start the API Server Initialize the agent and start the FastAPI server by following this example:</p> <pre><code>from agent import Agent\nfrom agent_endpoint import start_app\nagent = Agent(...)      # Initialize your agent with appropriate parameters\nstart_app(agent)\n</code></pre> <p>You can customize the host and port by passing them as arguments to start_app().</p> <p>For example:</p> <pre><code>start_app(agent, host=\"0.0.0.0\", port=8000)\n</code></pre> <p>3. Access the API Endpoint Once the server is running, you can interact with it using curl or any HTTP client. For example:</p> <pre><code>curl -G \"http://&lt;remote-server-ip&gt;:8000/chat\" \\\n--data-urlencode \"message=What is Vectara?\" \\\n-H \"X-API-Key: &lt;YOUR-API-KEY&gt;\"\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>You can install vectara-agentic using pip as follows:</p> <pre><code>pip install vectara-agentic\n</code></pre>"},{"location":"tools/","title":"Tools","text":"<p><code>vectara-agentic</code> provides a set of pre-built tools that you can use out-of-the-box for various purposes.</p>"},{"location":"tools/#standard-tools","title":"Standard Tools","text":"<p>Basic tools for general purposes:</p> <ul> <li>summarize_text: Summarizes text from a specific perspective or expertise level</li> <li>rephrase_text: Rephrases text according to specified instructions (e.g., for a 5-year-old or in formal tone)</li> </ul>"},{"location":"tools/#finance-tools","title":"Finance Tools","text":"<p><code>vectara-agentic</code> includes a few financial tools you can use right away in your agent, based on the LlamaIndex YahooFinanceToolSpec:</p> <ul> <li>balance_sheet: Returns a company's balance sheet</li> <li>income_statement: Returns a company's income statement</li> <li>cash_flow: Returns a company's cash flow statement</li> <li>stock_news: Returns latest news about a company</li> <li>stock_basic_info: Returns basic company information including price</li> <li>stock_analyst_recommendations: Returns analyst recommendations for a company</li> </ul>"},{"location":"tools/#legal-tools","title":"Legal Tools","text":"<p>vectara-agentic includes a few tools for the legal space:</p> <ul> <li>summarize_legal_text: Summarizes legal documents</li> <li>critique_as_judge: Critiques legal text from an expert judge's perspective</li> </ul>"},{"location":"tools/#guardrail-tools","title":"Guardrail Tools","text":"<p>The guardrail tools help you AI assistant or agent to avoid certain topics or responses that are prohibited by your organization or by law.</p> <p>The <code>get_bad_topics</code> tool returns a list of topics that are prohibited (politics, religion, violence, hate speech, adult content, illegal activities). The agent prompt has special instructions to call this tool if it exists, and avoid these topics.</p> <p>If you want to create your own set of topics, you can define a new tool by the same name (<code>get_bad_topics</code>) that returns a list of different topics, and the agent will use that list to avoid these topics.</p>"},{"location":"tools/#database-tools","title":"Database Tools","text":"<p>Database tools are quite useful if your agent requires access to a combination of RAG tools along with analytics capabilities. For example, consider the EV-assistant demo, providing answers about electric vehicles.</p> <p>We have provided this assistant with the following tools:</p> <ol> <li><code>ask_vehicles</code>: A Vectara RAG tool that answers general questions     about electric vehicles.</li> <li><code>ask_policies</code>: A Vectara RAG tool that answers questions about     electric vehicle policies.</li> <li>The <code>database_tools</code> that can help the agent answer analytics     queries based on three datasets: EV population data, EV population     size history by county, and EV title and registration activity.</li> </ol> <p>With the <code>ask_vehicles</code> and <code>ask_policies</code> tools, the ev-assistant can answer questions based on text, and it will use the database tools to answer analytical questions, based on the data.</p> <p>Here is an example for instantiating the database tools:</p> <pre><code># For a single database\ndatabase_tools = ToolsFactory().database_tools(\n    sql_database=your_database_object,\n    tool_name_prefix=\"ev\"\n)\n</code></pre> <p>This creates five tools:</p> <ol> <li><code>ev_list_tables</code>: A tool that lists the tables in the database.</li> <li><code>ev_describe_tables</code>: A tool that describes the schema of a table.</li> <li><code>ev_load_data</code>: A tool that loads data from a table.</li> <li><code>ev_load_sample_data</code> tool which provides a sample of the data from a table.</li> <li><code>ev_load_unique_values</code> tool which provides unique values for a set of columns in a table.</li> </ol> <p>Together, these 5 tools provide a comprehensive set of capabilities for an agent to interact with a database. </p> <p>For example, an agent can use the <code>ev_list_tables</code> tool to get a list of tables in the database, and then use the <code>ev_describe_tables</code> tool to get the schema of a specific table. It will use the <code>ev_load_sample_data</code> to get a sample of the data in the table, or the <code>ev_load_unique_values</code> to explore the type of values valid for a column. Finally, the agent can use the <code>ev_load_data</code> tool to load the data into the agent\\'s memory.</p> <p>Multiple databases</p> <p>In the case of EV-assistant, we use only a single database with 4 tables, and <code>tool_name_prefix=\"ev\"</code></p> <p>If your use-case includes multiple databases, you can define multiple database tools: each with a different database connection and a different <code>tool_name_prefix</code>.</p>"},{"location":"tools/#other-tools","title":"Other Tools","text":"<p>In addition to the tools above, vectara-agentic also supports these additional tools from the LlamaIndex Tools hub:</p> <ol> <li><code>arxiv</code>: A tool that queries the arXiv respository of papers.</li> <li><code>tavily_research</code>: A tool that queries the web using Tavily.</li> <li><code>kuzu</code>: A tool that queries the Kuzu graph database.</li> <li><code>waii</code>: A tool for querying databases with natural language.</li> <li><code>exa.ai</code>: A tool that uses EXA.AI search.</li> <li><code>brave</code>: A tool that uses Brave Search.</li> <li><code>neo4j</code>: A tool that queries a Neo4J graph database.</li> <li><code>google</code>: A set of tools that interact with Google services,     including Gmail, Google Calendar, and Google Search.</li> <li><code>slack</code>: A tool that interacts with Slack.</li> <li><code>salesforce</code>: A tool that queries Salesforce.</li> </ol>"},{"location":"usage/","title":"Usage","text":"<p>Let's walk through a complete example of creating an AI assistant using vectara-agentic. We will build a finance assistant that can answer questions about the annual financial reports for Apple Computer, Google, Amazon, Snowflake, Atlassian, Tesla, Nvidia, Microsoft, Advanced Micro Devices, Intel, and Netflix between the years 2020 and 2024.</p>"},{"location":"usage/#import-dependencies","title":"Import Dependencies","text":"<p>First, we must import some libraries and define some constants for our demo.</p> <pre><code>import os\nfrom dotenv import load_dotenv\nimport streamlit as st\nimport pandas as pd\nimport requests\nfrom pydantic import Field\n\nload_dotenv(override=True)\n</code></pre> <p>We then use the <code>load_dotenv</code> function to load our environment variables from a <code>.env</code> file.</p>"},{"location":"usage/#create-tools","title":"Create Tools","text":"<p>Next, we will create the tools for our agent.</p> <p>There are three categories of tools you can use with vectara-agentic:</p> <ol> <li>A query tool that connects to Vectara to ask a question about data     in a Vectara corpus.</li> <li>Pre-built tools that are available out of the box, or ready to use     tools from the LlamaIndex Tools     Hub.</li> <li>Any other tool that you want to make for your agent, based on custom     code in Python.</li> </ol> <p>Vectara RAG Query Tool Let's see how to create a Vectara query tool. In order to use this tool, you need to create a corpus and API key with a Vectara account. In this example, we will create the <code>ask_transcripts</code> tool, which can be used to perform RAG queries on analyst call transcripts. You can see this tool in use with our Finance Assistant demo.</p> <pre><code>from pydantic import BaseModel\n\n# define the arguments schema for the tool\nclass QueryTranscriptsArgs(BaseModel):\n    year: int = Field(..., description=f\"The year. An integer between {min(years)} and {max(years)}.\")\n    ticker: str = Field(..., description=f\"The company ticker. Must be a valid ticket symbol from the list {tickers.keys()}.\")\n</code></pre> <p>Note that:</p> <ul> <li>The arguments for this tool are defined using Python's <code>pydantic</code> package with the <code>Field</code> class. By defining the tool in this   way, we provide a good description for each argument so that the agent LLM can easily understand the tool's functionality    and how to use it properly.</li> <li>The <code>query</code> argument is added automatically to the RAG tool, and you don't need to specify it here</li> </ul> <p>You can also define an argument to support optional conditional arguments, for example:</p> <pre><code>from pydantic import BaseModel\n\n# define the arguments schema for the tool\nclass QueryTranscriptsArgs(BaseModel):\n    year: int | str = Field(\n        default=None,\n        description=f\"The year this query relates to. An integer between {min(years)} and {max(years)} or a string specifying a condition on the year\",\n        examples=[2020, '&gt;2021', '&lt;2023', '&gt;=2021', '&lt;=2023', '[2021, 2023]', '[2021, 2023)']\n    )\n    ticker: str = Field(..., description=f\"The company ticker. Must be a valid ticket symbol from the list {tickers.keys()}.\")\n</code></pre> <p>With this change for the <code>year</code> argument, we are telling the agent that both an int value (e.g. 2022) or a string value (e.g. '&gt;2022' or '&lt;2022') are valid inputs for this argument. You can also use range filters (e.g. '[2021, 2023]') to specify a range of years. If a string value is provided, <code>vectara-agentic</code> knows how to parse it properly in the backend and set a metadata filter with the right condition for Vectara.</p> <p>Now to create the actual tool, we use the <code>create_rag_tool()</code> method from the <code>VectaraToolFactory</code> class as follows:</p> <pre><code>from vectara_agentic.tools import VectaraToolFactory\n\nvec_factory = VectaraToolFactory(vectara_api_key=vectara_api_key,\n                                 vectara_corpus_key=vectara_corpus_key)\n\nask_transcripts = vec_factory.create_rag_tool(\n    tool_name = \"ask_transcripts\",\n    tool_description = \"\"\"\n    Given a company name and year,\n    returns a response (str) to a user question about a company, based on analyst call transcripts about the company's financial reports for that year.\n    You can ask this tool any question about the company including risks, opportunities, financial performance, competitors and more.\n    Make sure to provide the a valid company ticker and year.\n    \"\"\",\n    tool_args_schema = QueryTranscriptsArgs,\n    tool_args_type = {\n      \"year\": \"doc\",\n      \"ticker\": \"doc\"\n    },\n    reranker = \"chain\", rerank_k = 100,\n    rerank_chain = [\n      {\n        \"type\": \"slingshot\"\n      },\n      {\n        \"type\": \"userfn\",\n        \"user_function\": \"knee()\"\n      }\n      {\n        \"type\": \"mmr\",\n        \"diversity_bias\": 0.1\n      }\n    ],\n    n_sentences_before = 2, n_sentences_after = 2, lambda_val = 0.005,\n    summary_num_results = 10,\n    vectara_summarizer = 'vectara-summary-ext-24-05-med-omni',\n    include_citations = False,\n    fcs_threshold = 0.2\n)\n</code></pre> <p>In the code above, we did the following:</p> <ul> <li>First, we initialized the <code>VectaraToolFactory</code> with the Vectara     corpus key and API key. If you don't want to explicitly pass in     these arguments, you can specify them in your environment as     <code>VECTARA_CORPUS_KEY</code> and <code>VECTARA_API_KEY</code>. Additionally, you can     also create a single <code>VectaraToolFactory</code> that queries multiple     corpora. This may be helpful if you have related information across     multiple corpora in Vectara. To do this, create a query API key on     the     Authorization     page and give it to access to all the corpora you want for this     query tool. When specifying your environment variables, set     <code>VECTARA_CORPUS_KEY</code> to a list of corpus IDs separated by commas     (e.g. <code>5,6,19</code>).</li> <li>Then we called <code>create_rag_tool()</code>, specifying the tool name,     description and schema for the tool, followed by various optional     parameters to control the Vectara RAG query tool. Notice that we     also specified the type of each additional argument in the schema.     The type of each argument can be <code>\"doc\"</code> or <code>\"part\"</code>, corresponding     to whether the metadata argument is document metadata or part     metadata in the Vectara corpus. See this     page     on metadata for more information.</li> </ul> <p>One important parameter to point out is <code>fcs_threshold</code>. This allows you to specify a minimum factual consistency score (between 0 and 1) for the response to be considered a \"good\" response. If the generated response has an <code>FCS</code> below this threshold, the agent will not use the generated summary (considering it a hallucination). You can think of this as a hallucination guardrail. The higher you set <code>fcs_threshold</code>, the stricter your guardrail will be.</p> <p>If your agent continuously rejects all of the generated responses, consider lowering the threshold.</p> <p>Another important parameter is <code>reranker</code>. In this example, we are using a chain reranker, which chains together multiple reranking methods to achieve better control over the reranking and combines the strengths of various reranking methods. In the example above, we use the multilingual (or slingshot) reranker followed by a user-defined function (the knee reranker), and finally the MMR reranker with a diversity bias of 0.1. You can also supply other parameters to each reranker, such as a <code>cutoff</code> parameter, which removes documents that have scores below this threshold value after applying the given reranker. Lastly, you can add another user defined function reranker as the last reranker in the chain to specify a customized expression to rerank results in a way that is relevant to your specific application. If you want to learn more about reranking tips and best practices, check out our blog posts on user defined functions and knee reranking as well as this example notebook on user defined functions for some guidance and inspiration.</p> <p>That's it: now the <code>ask_transcripts</code> tool is ready to be added to the agent.</p> <p>Notes:</p> <ul> <li>You can use the <code>VectaraToolFactory</code> to generate more than one RAG tool with different parameters, depending on your needs.</li> <li><code>create_rag_tool</code> and <code>create_search_tool</code> both support the <code>vectara_base_url</code>    argument. If specified, it allows you to specify a different base URL for Vectara,   for example when you have an on-premise installation.</li> <li>If you want to specify a Certificate Authority for a local installation,   you can set \"export REQUESTS_CA_BUNDLE=/path/to/custom_ca_bundle.pem\".</li> </ul> <p>Vectara Search Tool In most cases, you will likely want to use the Vectara RAG query tool, which generates a summary to return to the agent along with the source text and documents used to generate that summary.</p> <p>In some applications, you may want the tool to only retrieve the actual text/documents that best match the query rather than summarizing all of the results. For example, you may ask your agent \"How many documents mention information about tax laws and regulations?\". The agent will be able to get a list of documents from your Vectara corpus and analyze the results to answer your question.</p> <p>Metadata Filtering In most cases, you will want to use the <code>tool_args_schema</code> to define the metadata fields used in your Vectara RAG or Search tool. Defining your parameters in this way allows the agent to interpret the user query and determine if any of these filters should be applied on that particular query.</p> <p>In some instances you may want to have a metadata filter that applies in every call to a Vectara RAG or search tool. For example, you may want to enforce that the oldest possible search results are from 2022. In this case, you can use the <code>fixed_filter</code> parameter to the <code>create_rag_tool()</code> or <code>create_search_tool()</code> functions.</p> <p>In our example where we want all results to be from 2022 and later, we would specify <code>fixed_filter = \"doc.year &gt;= 2022\"</code>.</p> <p>Additional Tools To generate non-RAG tools, you can use the <code>ToolsFactory</code> class, which provides some out-of-the-box tools that you might find helpful when building your agents, as well as an easy way to create custom tools.</p> <p>Currently, we have a few tool groups you may want to consider using:</p> <ul> <li><code>standard_tools()</code>: These are basic tools that can be helpful, and     include the <code>summarize_text</code> tool and <code>rephrase_text</code> tool.</li> <li><code>finance_tools()</code>: includes a set of financial query tools based on     Yahoo! finance.</li> <li><code>legal_tools()</code>: These tools are designed to help with legal     queries, and include <code>critique_as_judge</code> and <code>summarize_legal_text</code>.</li> <li><code>database_tools()</code>: tools to explore SQL databases and make queries     based on user prompts.</li> <li><code>guardrail_tools()</code>: These tools are designed to help the agent     avoid certain topics from its response.</li> </ul> <p>For example, to get access to all the legal tools, you can use the following:</p> <pre><code>from vectara_agentic.tools import ToolsFactory\n\nlegal_tools = ToolsFactory().legal_tools()\n</code></pre> <p>For more details about the tools see <code>Tools &lt;tools&gt;</code>{.interpreted-text role=\"doc\"}.</p> <p>Create your own tool You can also create your own tool directly by defining a Python function:</p> <pre><code>import numpy as np\n\ndef earnings_per_share(\n  net_income: float = Field(description=\"the net income for the company\"),\n  number_of_shares: float = Field(description=\"the number of oustanding shares\"),\n) -&gt; float:\n    \"\"\"\n    This tool returns the EPS (earnings per share).\n    \"\"\"\n    return np.round(net_income / number_of_shares,4)\n\nmy_tool = tools_factory.create_tool(earnings_per_share)\n</code></pre> <p>A few important things to note:</p> <ol> <li>A tool may accept any type of argument (e.g. float, int) and return     any type of value (e.g. float). The <code>create_tool()</code> method will     handle the conversion of the arguments and response into strings     (which is type the agent expects).</li> <li>It is important to define a clear and concise docstring for your     tool. This will help the agent understand what the tool does and how     to use it.</li> </ol> <p>Here are some functions we will define for our finance assistant example:</p> <pre><code>tickers = {\n  \"AAPL\": \"Apple Computer\", \n  \"GOOG\": \"Google\", \n  \"AMZN\": \"Amazon\",\n  \"SNOW\": \"Snowflake\",\n  \"TEAM\": \"Atlassian\",\n  \"TSLA\": \"Tesla\",\n  \"NVDA\": \"Nvidia\",\n  \"MSFT\": \"Microsoft\",\n  \"AMD\": \"Advanced Micro Devices\",\n  \"INTC\": \"Intel\",\n  \"NFLX\": \"Netflix\",\n}\nyears = [2020, 2021, 2022, 2023, 2024]\n\ndef get_company_info() -&gt; list[str]:\n\"\"\"\nReturns a dictionary of companies you can query about. Always check this before using any other tool.\nThe output is a dictionary of valid ticker symbols mapped to company names.\nYou can use this to identify the companies you can query about, and their ticker information.\n\"\"\"\nreturn tickers\n\ndef get_valid_years() -&gt; list[str]:\n\"\"\"\nReturns a list of the years for which financial reports are available.\nAlways check this before using any other tool.\n\"\"\"\nreturn years\n\n# Tool to get the income statement for a given company and year using the FMP API\ndef get_income_statement(\nticker=Field(description=\"the ticker symbol of the company.\"),\nyear=Field(description=\"the year for which to get the income statement.\"),\n) -&gt; str:\n\"\"\"\nGet the income statement for a given company and year using the FMP (https://financialmodelingprep.com) API.\nReturns a dictionary with the income statement data. All data is in USD, but you can convert it to more compact form like K, M, B.\n\"\"\"\nfmp_api_key = os.environ.get(\"FMP_API_KEY\", None)\nif fmp_api_key is None:\n   return \"FMP_API_KEY environment variable not set. This tool does not work.\"\nurl = f\"https://financialmodelingprep.com/api/v3/income-statement/{ticker}?apikey={fmp_api_key}\"\nresponse = requests.get(url)\nif response.status_code == 200:\n   data = response.json()\n   income_statement = pd.DataFrame(data)\n   income_statement[\"date\"] = pd.to_datetime(income_statement[\"date\"])\n   income_statement_specific_year = income_statement[\n     income_statement[\"date\"].dt.year == int(year)\n   ]\n   values_dict = income_statement_specific_year.to_dict(orient=\"records\")[0]\n   return f\"Financial results: {', '.join([f'{key}: {value}' for key, value in values_dict.items() if key not in ['date', 'cik', 'link', 'finalLink']])}\"\nelse:\n   return \"FMP API returned error. This tool does not work.\"\n</code></pre> <p>The <code>get_income_statement()</code> tool utilizes the FMP API to get the income statement for a given company and year. Notice how the tool description is structured. We describe each of the expected arguments to the function using pydantic's <code>Field</code> class. The function description only describes to the agent what the function does and how the agent should use the tool. This function definition follows best practices for defining tools. You should make this description detailed enough so that your agent knows when to use each of your tools.</p> <p>You can define your tool as an individual python function (as shown above) or as a method in a Python class. It may be helpful to define all of your tools (Vectara tools, other pre-built tools, and your custom tools) in a single AgentTools class. Please note that you cannot define a tool as a function within another tool. Each tool must be a separate Python function.</p> <p>Your tools should also handle any exceptions gracefully by returning an <code>Exception</code> or a string describing the failure. The agent can interpret that string and then decide how to deal with the failure (either calling another tool to accomplish the task or telling the user that their request was unable to be processed).</p> <p>Finally, notice that we have used snake_case for all of our function names. While this is not required, it's a best practice that we recommend for you to follow.</p>"},{"location":"usage/#initialize-the-agent","title":"Initialize The Agent","text":"<p>Now that we have our tools, let's create the agent, using the following arguments:</p> <ol> <li><code>tools: list[FunctionTool]</code>: A list of tools that the agent will use     to interact with information and apply actions. For any tools you     create yourself, make sure to pass them to the <code>create_tool()</code>     method of your <code>ToolsFactory</code> object.</li> <li><code>topic: str = \"general\"</code>: This is simply a string (should be a noun)     that is used to identify the agent's area of expertise. For our     example we set this to <code>financial analyst</code>.</li> <li><code>custom_instructions: str = \"\"</code>: This is a set of instructions that     the agent will follow. These instructions should not tell the agent     what your tools do (that's what the tool descriptions are for) but     rather any particular behavior you want your LLM to have, such as     how to present the information it receives from the tools to the     user.</li> <li><code>agent_config: Optional[AgentConfig] = None</code>: the agent configuration     See below for more details. If unspecified, defaults are used.</li> <li><code>fallback_agent_config: Optional[AgentConfig] = None</code>: configuration     for a fallback_agent. If specified, this will get activated if the     main agent API is not responding (e.g. when inference enpoint is down).     If unspecified, no fallback agent is assumed.</li> <li><code>agent_progress_callback: Optional[Callable[[AgentStatusType, str], None]] = None</code>:     This is an optional callback function that will be called on every     agent step (see below)</li> <li><code>query_logging_callback: Optional[Callable[[str, str], None]] = None</code>:     This is an optional callback function that will be called at the end     of response generation, with the query and response strings.</li> <li><code>validate_tools: bool = False</code>: whether to validate tool inconsistency      with instructions.</li> </ol> <p>Every agent has its own default set of instructions that it follows to interpret users' messages and use the necessary tools to complete its task. However, we can (and often should) define custom instructions (via the <code>custom_instructions</code> argument) for our AI assistant. Here are some guidelines to follow when creating your instructions:</p> <ul> <li>Write precise and clear instructions without overcomplicating the     agent.</li> <li>Consider edge cases and unusual or atypical scenarios.</li> <li>Be cautious to not over-specify behavior based on your primary use     case as this may limit the agent's ability to behave properly in     other situations.</li> </ul> <p>Here are the instructions we are using for our financial AI assistant:</p> <pre><code>financial_assistant_instructions = \"\"\"\n  - You are a helpful financial assistant, with expertise in financial reporting, in conversation with a user.\n  - Never discuss politics, and always respond politely.\n  - Respond in a compact format by using appropriate units of measure (e.g., K for thousands, M for millions, B for billions).\n  - Do not report the same number twice (e.g. $100K and 100,000 USD).\n  - Always check the get_company_info and get_valid_years tools to validate company and year are valid.\n  - When querying a tool for a numeric value or KPI, use a concise and non-ambiguous description of what you are looking for.\n  - If you calculate a metric, make sure you have all the necessary information to complete the calculation. Don't guess.\n\"\"\"\n</code></pre> <p>Notice how these instructions are different from the tool function descriptions. These instructions are general rules that the agent should follow. At times, these instructions may refer to specific tools, but in general, the agent should be able to decide for itself what tools it should call. This is what makes agents very powerful and makes our job as coders much simpler.</p> <p>agent_progress_callback callback The <code>agent_progress_callback</code> is an optional <code>Callable</code> function that can serve a variety of purposes for your assistant. It is a callback function that is managed by the agent, and it will be called anytime the agent is updated, such as when calling a tool, or when receiving a response from a tool.</p> <p>In our example, we will use it to log the actions of our agent so users can see the steps the agent is taking as it answers their questions. Since our assistant is using streamlit to display the results, we will append the log messages to the session state.</p> <pre><code>from vectara_agentic.agent import AgentStatusType\n\ndef agent_progress_callback(status_type: AgentStatusType, msg: str):\n  output = f\"{status_type.value} - {msg}\"\n  st.session_state.log_messages.append(output)\n</code></pre> <p>agent_config The <code>agent_config</code> argument is an optional object that you can use to explicitly specify the configuration of your agent, including the following:</p> <ul> <li><code>agent_type</code>: the agent type. Valid values are <code>REACT</code>, <code>LLMCOMPILER</code>, <code>LATS</code>, <code>FUNCTION_CALLING</code> or <code>OPENAI</code> (default: <code>OPENAI</code>).</li> <li><code>main_llm_provider</code> and <code>tool_llm_provider</code>: the LLM provider for main agent and for the tools. Valid values are <code>OPENAI</code>, <code>ANTHROPIC</code>, <code>TOGETHER</code>, <code>GROQ</code>, <code>COHERE</code>, <code>BEDROCK</code>, <code>GEMINI</code> or <code>FIREWORKS</code> (default: <code>OPENAI</code>).</li> <li><code>main_llm_model_name</code> and <code>tool_llm_model_name</code>: agent model name for agent and tools (default depends on provider).</li> <li><code>observer</code>: the observer type; should be <code>ARIZE_PHOENIX</code> or if undefined no observation framework will be used.</li> <li><code>endpoint_api_key</code>: a secret key if using the API endpoint option (defaults to <code>dev-api-key</code>)</li> <li><code>max_reasoning_steps</code>: the maximum number of reasoning steps (iterations for React and function calls for OpenAI agent, respectively). defaults to 50.</li> </ul> <p>By default, each of these parameters will be read from your environment, but you can also explicitly define them with the <code>AgentConfig</code> class.</p> <p>For example, here is how we can define an <code>AgentConfig</code> object to create a <code>ReAct</code> agent using <code>OPENAI</code> as the LLM for the agent and <code>Cohere</code> as the LLM for the agent's tools:</p> <pre><code>from vectara_agentic.agent_config import AgentConfig\n\nconfig = AgentConfig(\n  agent_type=\"REACT\",\n  main_llm_provider=\"OPENAI\",\n  tool_llm_provider=\"COHERE\"\n)\n</code></pre> <p>Creating the agent Here is how we will instantiate our finance assistant:</p> <pre><code>from vectara_agentic import Agent\n\nagent = Agent(\n     tools=[tools_factory.create_tool(tool, tool_type=\"query\") for tool in\n               [\n                   get_company_info,\n                   get_valid_years,\n                   get_income_statement\n               ]\n           ] +\n           tools_factory.standard_tools() +\n           tools_factory.financial_tools() +\n           tools_factory.guardrail_tools() +\n           [ask_transcripts],\n     topic=\"10-K annual financial reports\",\n     custom_instructions=financial_assistant_instructions,\n     agent_progress_callback=agent_progress_callback\n)\n</code></pre> <p>Notice that when we call the <code>create_tool()</code> method, we specified a <code>tool_type</code>. This can either be <code>\"query\"</code> (default) or <code>\"action\"</code>. For our example, all of the tools are query tools, so we can easily add all of them to our agent with a list comprehension, as shown above.</p>"},{"location":"usage/#chat-with-your-assistant","title":"Chat with your Assistant","text":"<p>Once you have created your agent, using it is quite simple. All you have to do is call its <code>chat()</code> method, which prompts your agent to answer the user's query using its available set of tools. It's that easy.</p> <pre><code>query = \"Which 3 companies had the highest revenue in 2022, and how did they do in 2021?\"\nprint(str(agent.chat(query)))\n</code></pre> <p>The agent returns the response:</p> <p>The three companies with the highest revenue in 2022 were:</p> <ol> <li>Amazon (AMZN): $513.98B</li> <li>Apple (AAPL): $394.33B</li> <li>Google (GOOG): $282.84B</li> </ol> <p>Their revenues in 2021 were:</p> <ol> <li>Amazon (AMZN): $469.82B</li> <li>Apple (AAPL): $365.82B</li> <li>Google (GOOG): $257.64B</li> </ol> <p>The <code>chat()</code> function returns an <code>AgentResponse</code> object, which includes the agent's generated response text and a list of <code>ToolOutput</code> objects. The agent's response text can easily be retrieved <code>response</code> member (or simply by using <code>str()</code>). The tool information can be extracted with the <code>sources</code> member of the <code>AgentResponse</code> class and will return a list of tool outputs, including the name of each tool that was called and the output from that tool that was given to the agent.</p> <p>To make a full Streamlit app, there is some extra code that is necessary to configure the demo layout. You can check out the full code and demo for this app on Hugging Face.</p>"},{"location":"usage/#other-chat-options","title":"Other Chat Options","text":"<p>The standard <code>chat()</code> method will run synchronously, so your application will wait until the agent finishes generating its response before making any other function calls. If you would prefer to run your queries asynchronously with your application, you can use the <code>achat()</code> method.</p> <p>The <code>chat()</code> function also returns the response as a single string, which could be a lengthy text. If you would prefer to stream the agent's response by chunks, you can use the <code>stream_chat()</code> method (or <code>astream_chat()</code> method to run asynchronously). This will return an <code>AgentStreamingResponse</code> object. If you want to directly print out the response, you can use the <code>print_response_stream()</code> method. If you need to yield the chunks in some other way for your application, you can obtain the generator object by accessing the <code>chat_stream</code> member.</p>"},{"location":"usage/#using-workflows","title":"Using Workflows","text":"<p>vectara-agentic now supports custom workflows via the <code>run()</code> method, enabling you to define multi-step interactions with validated inputs and outputs. To learn more about workflows read the documentation</p>"},{"location":"usage/#defining-a-custom-workflow","title":"Defining a Custom Workflow","text":"<p>To create a workflow, subclass the Workflow class from <code>llama_index.core.workflow</code> and define two Pydantic models: <code>InputsModel</code> and <code>`OutputsModel</code>.  For example:</p> <pre><code>from pydantic import BaseModel\nfrom llama_index.core.workflow import (\n    StartEvent,StopEvent, Workflow, step,\n)\n\nclass MyWorkflow(Workflow):\n    class InputsModel(BaseModel):\n        query: str\n\n    class OutputsModel(BaseModel):\n        answer: str\n\n    @step\n    async def my_step(self, ev: StartEvent) -&gt; StopEvent:\n        # do something here\n        return StopEvent(result=\"Hello, world!\")\n</code></pre> <p>When the <code>run()</code> method in vectara-agentic is invoked, it calls the workflow with the following variables in the <code>StartEvent</code>:</p> <ul> <li><code>agent</code>: the agent object used to call <code>run()</code> (self)</li> <li><code>tools</code>: the tools provided to the agent. Those can be used as needed in the flow.</li> <li><code>llm</code>: a pointer to a LlamaIndex llm, so it can be used in the workflow. For example, one of the steps may call <code>llm.acomplete(prompt)</code></li> <li><code>verbose</code>: controls whether extra debug information is displayed</li> <li><code>inputs</code>: this is the actual inputs to the workflow provided by the call to <code>run()</code> and must be of type <code>InputsModel</code></li> </ul> <p>If you want to use <code>agent</code>, <code>tools</code>, <code>llm</code> or <code>verbose</code> in other events (that are not <code>StartEvent</code>), you can store them in the <code>Context</code> of the Workflow as follows:</p> <pre><code>await ctx.set(\"agent\", ev.agent)\n</code></pre> <p>and then in any other event you can pull that agent object with</p> <pre><code>agent = await ctx.get(\"agent\")\n</code></pre> <p>Similarly you can reuse the <code>llm</code>, <code>tools</code> or <code>verbose</code> arguments within other nodes in the workflow.</p>"},{"location":"usage/#integrating-the-workflow-with-your-agent","title":"Integrating the Workflow with Your Agent","text":"<p>When instantiating your agent, pass your workflow class to the <code>workflow_cls</code> parameter (and optionally set a workflow timeout):</p> <pre><code>agent = Agent(\n    tools=[...],  # your list of tools\n    topic=\"10-K annual financial reports\",\n    custom_instructions=financial_assistant_instructions,\n    agent_progress_callback=agent_progress_callback,\n    workflow_cls=FinanceWorkflow,   # Provide your workflow class here\n    workflow_timeout=120            # Optional timeout in seconds\n)\n</code></pre>"},{"location":"usage/#running-the-workflow","title":"Running the Workflow","text":"<p>To run the workflow, create an instance of your workflow's <code>InputsModel</code> with the required parameters and call the agent's <code>run()</code> method. For example:</p> <pre><code># Create input for the workflow\nworkflow_inputs = FinanceWorkflow.InputsModel(query=\"What were the revenue trends for Apple?\", analysis_depth=3)\n\n# Execute the workflow asynchronously (ensure you're in an async context or use asyncio.run)\nworkflow_output = asyncio.run(agent.run(workflow_inputs))\n\n# Access the final answer from the output model\nprint(workflow_output.answer)\n</code></pre> <p>The <code>run()</code> method executes your workflow\u2019s logic, validates the output against the <code>OutputsModel</code>, and returns a structured result.</p>"},{"location":"usage/#using-subquestionqueryworkflow","title":"Using SubQuestionQueryWorkflow","text":"<p>vectara-agentic already includes one useful workflow you can use right away (it is also useful as an advanced example) This workflow is called <code>SubQuestionQueryWorkflow</code> and it works by breaking a complex query into sub-queries and then executing each sub-query with the agent until it reaches a good response.</p>"},{"location":"usage/#additional-information","title":"Additional Information","text":"<p>Agent Information The <code>Agent</code> class defines a few helpful methods to help you understand the internals of your application.</p> <ol> <li>The <code>report()</code> method prints out the agent object's type (REACT,     OPENAI, or LLMCOMPILER), the tools, and the LLMs used for the main     agent and tool calling.</li> <li>The <code>token_counts()</code> method tells you how many tokens you have used     in the current session for both the main agent and tool calling     LLMs. This can be helpful for users who want to track how many     tokens have been used, which translates to how much money they are     spending.</li> </ol> <p>If you have any other information that you would like to be accessible to users, feel free to make a suggestion on our community server.</p> <p>Observability You can also setup full observability for your vectara-agentic assistant or agent using Arize Phoenix. This allows you to view LLM prompt inputs and outputs, the latency of each task and subtask, and many of the individual function calls performed by the LLM, as well as FCS scores for each response.</p> <p>To set up observability for your app, follow these steps:</p> <ol> <li>Set <code>os[\"VECTARA_AGENTIC_OBSERVER_TYPE\"] = \"ARIZE_PHOENIX\"</code> or     specify <code>observer = \"ARIZE_PHOENIX\"</code> in your <code>AgentConfig</code>.</li> <li>Connect to a local phoenix server:<ol> <li>If you have a local phoenix server that you've run using e.g.     <code>python -m phoenix.server.main serve</code>, vectara-agentic will send     all traces to it automatically.</li> <li>If not, vectara-agentic will run a local instance during the     agent's lifecycle, and will close it when finished.</li> <li>In both cases, traces will be sent to the local instance, and     you can see the dashboard at http://localhost:6006.</li> </ol> </li> <li>Alternatively, you can connect to a Phoenix instance hosted on     Arize.<ol> <li>Go to https://app.phoenix.arize.com, and set up an account if     you don't have one.</li> <li>Create an API key and put it in the <code>PHOENIX_API_KEY</code> variable.     This variable indicates you want to use the hosted version.</li> <li>To view the traces go to https://app.phoenix.arize.com.</li> </ol> </li> </ol> <p>In addition to the raw traces, vectara-agentic also records <code>FCS</code> values into Arize for every Vectara RAG call. You can see those results in the <code>Feedback</code> column of the arize UI.</p> <p>Query Callback You can define a callback function to log query/response pairs in your agent. This function should be specified in the <code>query_logging_callback</code> argument when you create your agent and should take in two string arguments. The first argument passed to this function will be the user query and the second will be the agent's response.</p> <p>If defined, this function is called every time the agent receives a query and generates a response.</p>"},{"location":"usage/#using-a-private-llm","title":"Using a Private LLM","text":"<p>vectara-agentic offers a wide variety of LLM options from several providers to use for the main agent and for tool calling. However, in some instances, you may be interested in using your own LLM hosted locally at your company.</p> <p>If you would like the main agent LLM to be a custom LLM, specify <code>VECTARA_AGENTIC_MAIN_LLM_PROVIDER=\"PRIVATE\"</code> in your environment or <code>main_llm_provider=\"PRIVATE\"</code> in your <code>AgentConfig</code> object and <code>VECTARA_AGENTIC_MAIN_MODEL_NAME</code> (or <code>main_llm_model_name</code> in <code>AgentConfig</code>) as the model name of your LLM.</p> <p>If you would like the tool calling LLM to be a custom LLM, specify <code>VECTARA_AGENTIC_TOOL_LLM_PROVIDER=\"PRIVATE\"</code> in your environment or <code>tool_llm_provider=\"PRIVATE\"</code> in your <code>AgentConfig</code> object and <code>VECTARA_AGENTIC_TOOL_MODEL_NAME</code> (or <code>tool_llm_model_name</code> in <code>AgentConfig</code>) as the model name of your LLM.</p> <p>Additionally, you should specify <code>VECTARA_AGENTIC_PRIVATE_LLM_API_BASE</code> in your environment (or <code>private_llm_api_base</code> in the <code>AgentConfig</code>) as the API endpoint url for your private LLM and <code>VECTARA_AGENTIC_PRIVATE_API_KEY</code> (or <code>private_llm_api_key</code>) as the API key to your LLM.</p>"}]}